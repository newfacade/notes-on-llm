{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f202cf0-b1c2-498f-97de-b3a5472fb64a",
   "metadata": {},
   "source": [
    "# CodeAct\n",
    "\n",
    "```{note}\n",
    "LLM agents are typically prompted to produce actions\n",
    "by generating JSON or text in a pre-defined\n",
    "format, which is usually limited by constrained\n",
    "action space and restricted flexibility. This work proposes\n",
    "to use executable Python code to consolidate\n",
    "LLM agents’ actions into a unified action space\n",
    "([CodeAct](https://arxiv.org/abs/2402.01030)).\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a404517c-cf5a-4c21-b409-8122f35a03a7",
   "metadata": {},
   "source": [
    "## What is CodeAct?\n",
    "\n",
    "In Fig. 2, we first introduce a general multi-turn interaction\n",
    "framework for LLM agents’ real-world usage that considers\n",
    "three roles: agent, user, and environment. CodeAct employs\n",
    "Python code to consolidate all actions for agent-environment\n",
    "interaction. In CodeAct, each emitted action to the environment\n",
    "is a piece of Python code, and the agent will\n",
    "receive outputs of code execution (e.g., results, errors) as\n",
    "observation.\n",
    "\n",
    "```{figure} ../images/codeact1.png\n",
    "```\n",
    "\n",
    "## CodeAct Shows the Promise as a Strong Tool Use Framework\n",
    "\n",
    "**Setup.** We re-purpose API-Bank and test\n",
    "LLMs’ API-calling performance. For each evaluation instance, we\n",
    "instruct LLM to generate one atomic tool call in the format\n",
    "of a Python function call, JSON object, or text expression\n",
    "in a pre-defined format.\n",
    "\n",
    "```{figure} ../images/codeact2.png\n",
    "```\n",
    "\n",
    "**Results.** For most LLMs,\n",
    "CodeAct achieves comparable or better performance even\n",
    "in atomic actions (the simplistic tool use scenario).\n",
    "\n",
    "## CodeAct Gets More Done with Fewer Interactions\n",
    "\n",
    "**M3ToolEval.** To the best of our\n",
    "knowledge, no existing tool-use benchmarks contain complex\n",
    "tasks requiring the composition of multiple tools while\n",
    "supporting evaluating different action formats. Hence, we\n",
    "curate a benchmark M3ToolEval to fill this gap.\n",
    "\n",
    "```{figure} ../images/codeact3.png\n",
    "```\n",
    "\n",
    "**Setup.** We allow the model to generate fully functional\n",
    "Python code that enables control and data flow (e.g., ifstatement,\n",
    "for-loop). Within each turn, the model\n",
    "can either emit an action or propose an answer to be verified\n",
    "by an exact match with the ground-truth solution. The\n",
    "interaction will terminate when a maximum of 10 interaction\n",
    "turns are reached or a correct solution has been submitted.\n",
    "\n",
    "**Results.** CodeAct generally has a higher task success rate. Moreover, using CodeAct requires a lower average number\n",
    "of turns.\n",
    "\n",
    "```{figure} ../images/codeact4.png\n",
    "```\n",
    "\n",
    "## CodeAct Benefits from Multi-turn Interactions and Existing Software Packages\n",
    "\n",
    "Thanks to its extensive knowledge of Python\n",
    "learned during pre-training, the LLM agent can automatically\n",
    "import the correct Python libraries to solve tasks\n",
    "without requiring user-provided tools or demonstrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98da58c-5f87-4121-842c-905c314dc203",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
