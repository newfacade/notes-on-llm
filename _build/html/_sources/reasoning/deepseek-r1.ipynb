{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d8ef11c-8a2c-406b-8d1b-2429720f82da",
   "metadata": {},
   "source": [
    "# DeepSeek-R1\n",
    "\n",
    "```{note}\n",
    "We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1.\n",
    "DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised\n",
    "fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities.\n",
    "```\n",
    "\n",
    "![](../images/r1-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fec6e5-7031-4f55-a763-08067cb25b04",
   "metadata": {},
   "source": [
    "## DeepSeek-R1-Zero: Reinforcement Learning on the Base Model\n",
    "\n",
    "In this section, we\n",
    "explore the potential of LLMs to develop reasoning capabilities `without any supervised data`,\n",
    "focusing on their self-evolution through a pure reinforcement learning process.\n",
    "\n",
    "### Reinforcement Learning Algorithm\n",
    "\n",
    "[](grpo)\n",
    "\n",
    "### Reward Modeling\n",
    "\n",
    "The reward is the source of the training signal, which decides the optimization direction of RL.\n",
    "To train DeepSeek-R1-Zero, we adopt a rule-based reward system that mainly consists of two\n",
    "types of rewards:\n",
    "\n",
    "* **Accuracy rewards**: The accuracy reward model evaluates whether the response is correct.\n",
    "For example, in the case of math problems with deterministic results, the model is required\n",
    "to provide the final answer in a specified format (e.g., within a box), enabling reliable\n",
    "rule-based verification of correctness. Similarly, for LeetCode problems, a compiler can be\n",
    "used to generate feedback based on predefined test cases.\n",
    "\n",
    "* **Format rewards:** In addition to the accuracy reward model, we employ a format reward\n",
    "model that enforces the model to put its thinking process between \\<think> and \\</think\\>\n",
    "tags.\n",
    "\n",
    "We do not apply the outcome or process neural reward model in developing DeepSeek-R1-Zero,\n",
    "because we find that the neural reward model may suffer from reward hacking in the large-scale\n",
    "reinforcement learning process.\n",
    "\n",
    "### Training Template\n",
    "\n",
    "To train DeepSeek-R1-Zero, we begin by designing a straightforward template that guides\n",
    "the base model to adhere to our specified instructions.\n",
    "\n",
    "![](../images/r1-2.png)\n",
    "\n",
    "### Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero\n",
    "\n",
    "![](../images/r1-3.png)\n",
    "\n",
    "**Self-evolution Process of DeepSeek-R1-Zero** As depicted in Figure 3, the thinking time of DeepSeek-R1-Zero shows consistent improvement throughout the training process. This improvement is not the result of external adjustments\n",
    "but rather an intrinsic development within the model. DeepSeek-R1-Zero naturally acquires the\n",
    "ability to solve increasingly complex reasoning tasks by leveraging extended test-time computation.\n",
    "\n",
    "![](../images/r1-4.png)\n",
    "\n",
    "**Aha Moment of DeepSeek-R1-Zero**\n",
    "\n",
    "A particularly intriguing phenomenon observed during\n",
    "the training of DeepSeek-R1-Zero is the occurrence of an “aha moment”. This moment, as\n",
    "illustrated occurs in an intermediate version of the model. During this phase,\n",
    "DeepSeek-R1-Zero learns to allocate more thinking time to a problem by reevaluating its initial\n",
    "approach.\n",
    "\n",
    "**Drawback of DeepSeek-R1-Zero** Although DeepSeek-R1-Zero exhibits strong reasoning\n",
    "capabilities and autonomously develops unexpected and powerful reasoning behaviors, it faces\n",
    "several issues. For instance, DeepSeek-R1-Zero struggles with challenges like poor readability,\n",
    "and language mixing. To make reasoning processes more readable and share them with the\n",
    "open community, we explore DeepSeek-R1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd612f9d-f721-4b18-8081-75f878106d41",
   "metadata": {},
   "source": [
    "## DeepSeek-R1: Reinforcement Learning with Cold Start\n",
    "\n",
    "To make reasoning processes more readable, we design a pipeline to train DeepSeek-R1. The\n",
    "pipeline consists of four stages, outlined as follows.\n",
    "\n",
    "### Cold Start\n",
    "\n",
    "For DeepSeek-R1, we construct and collect a small amount of long CoT data\n",
    "to fine-tune the DeepSeek-V3-Base as\n",
    "the starting point for RL.\n",
    "\n",
    "### Reasoning-oriented Reinforcement Learning\n",
    "\n",
    "After fine-tuning DeepSeek-V3-Base on the cold start data, we apply the same large-scale\n",
    "reinforcement learning training process as employed in DeepSeek-R1-Zero.\n",
    "\n",
    "### Rejection Sampling and Supervised Fine-Tuning\n",
    "\n",
    "**Reasoning data** We curate reasoning prompts and generate reasoning trajectories by performing\n",
    "rejection sampling from the checkpoint from the above RL training.\n",
    "\n",
    "**Non-Reasoning data** For non-reasoning data, such as writing, factual QA, self-cognition,\n",
    "and translation, we adopt the DeepSeek-V3 pipeline and reuse portions of the SFT dataset of\n",
    "DeepSeek-V3.\n",
    "\n",
    "We fine-tune DeepSeek-V3-Base for two epochs.\n",
    "\n",
    "```{caution}\n",
    "How to perform rejection sampling? using a reward model or a verifier?\n",
    "```\n",
    "\n",
    "### Reinforcement Learning for all Scenarios\n",
    "\n",
    "We train the model using a combination\n",
    "of reward signals and diverse prompt distributions. \n",
    "\n",
    "For reasoning data, we adhere to the\n",
    "methodology outlined in DeepSeek-R1-Zero, which utilizes rule-based rewards to guide the\n",
    "learning process in math, code, and logical reasoning domains.\n",
    "\n",
    "For general data, we resort to\n",
    "reward models to capture human preferences in complex and nuanced scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b65dd0a-b890-4760-9a8f-22181011c9ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
