{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81f00d29-0477-4718-b8fe-b7ad2263d925",
   "metadata": {},
   "source": [
    "# Training Language Models to Self-Correct via Reinforcement Learning\n",
    "\n",
    "```{note}\n",
    "Training self-correction via SFT either suffers from a distribution\n",
    "mismatch between the training data and the model’s own responses (Pair-SFT), or implicitly prefers only making minor edits (STaR).<br>\n",
    "SCoRe is a multi-turn reinforcement learning approach, that directly train on self-generated data, it has two stages:\n",
    "\n",
    "* stage 1: produce high-reward revisions at the second attempt, while forcing the model to not change its first-attempt response.\n",
    "* stage 2: trains responses at both attempts towards optimizing reward, and reward shaping to incentivize self-correction.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955626e5-34bd-4509-aba7-93f20a123961",
   "metadata": {},
   "source": [
    "## Preliminaries and Problem Setup\n",
    "\n",
    "Our goal is to develop an approach for training LLMs to improve their own predictions by entirely\n",
    "training on self-generated data.\n",
    "\n",
    "Concretely, given a dataset $\\mathcal{D} = \\{(x_{i}, y_{i}^{\\ast})_{i=1}^{N}\\}$ of problems $x_{i}$ and oracle responses $y_{i}^{\\ast}$, we will train an LLM policy $\\pi_{\\theta}(\\cdot|[x,\\hat{y}_{1:l},p_{1:l}])$ that, given the problem $x$, previous $l$ model attempts $\\hat{y}_{1:l}$ at the\n",
    "problem, and auxiliary instructions $p_{1:l}$ (e.g., instruction to find a mistake and improve the response),\n",
    "solves the problem $x$ as correctly as possible. \n",
    "\n",
    "Moreover, we assume access to a reward function / verifier $\\hat{r}(y, y^{\\ast})$, such as a string-matching\n",
    "based answer checking function that evaluates correctness of response $y$ by comparing with the oracle\n",
    "response $y^{\\ast}$. Critically, we do not assume access to such a function at test-time and the model itself learns\n",
    "to deduce whether there was a mistake and corrects it.\n",
    "\n",
    "![](../images/self-correct-rl1.png)\n",
    "\n",
    "We aim to find a model $\\pi$ that maximizes the correctness reward obtained from the\n",
    "verifier at the end of $l+1$ turns:\n",
    "\n",
    "$$\n",
    "\\underset{\\pi_{\\theta}}{\\max}\\mathbb{E}_{x,y^{\\ast}\\sim\\mathcal{D},\\hat{y}_{l+1}\\sim\\pi_{\\theta}(\\cdot|[x,\\hat{y}_{1:l},p_{1:l}])}\\left[\\hat{r}(\\hat{y}_{l+1},y^{\\ast})\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bd7593-5794-4f2f-88c7-fe1fa9c71005",
   "metadata": {},
   "source": [
    "## *SCoRe*: Self-Correction via Multi-Turn Reinforcement Learning\n",
    "\n",
    "### Stage I: Training a Model Initialization to Prevent Collapse\n",
    "\n",
    "We explicitly fine-tune the base model to produce high-reward revisions at the second attempt,\n",
    "while forcing the model to not change its first-attempt response. This stage is critical in reducing the base model’s bias towards simply coupling the first and second-attempt distributions, and thus becoming trapped in a local optima when\n",
    "actual multi-turn RL is run.\n",
    "\n",
    "$$\n",
    "\\underset{\\pi_{\\theta}}{\\max}\\mathbb{E}_{x_1,y_1\\sim\\pi_{\\theta}(\\cdot|x_1),y_{2}\\sim\\pi_{\\theta}(\\cdot|[x_1,p_1])}\\left[\\hat{r}(y_{2},y^{\\ast}) - \\beta_{2}\\log(\\pi_{\\theta}(y_1|x_1)/\\pi_{\\text{ref}}(y_1|x_1))\\right]\n",
    "$$\n",
    "\n",
    "![](../images/self-correct-rl2.png)\n",
    "\n",
    "### Stage II: Multi-Turn RL with Reward Shaping\n",
    "\n",
    "Equipped with a model initialization from Stage I that exhibits a substantially smaller bias to couple the\n",
    "two responses, the second stage of SCoRe now trains responses at both attempts towards optimizing\n",
    "reward ($x_2$ denotes all the tokens from the first turn concatenated with each other):\n",
    "\n",
    "$$\n",
    "\\underset{\\pi_{\\theta}}{\\max}\\mathbb{E}_{x_1,y_1\\sim\\pi_{\\theta}(\\cdot|x_1),y_{2}\\sim\\pi_{\\theta}(\\cdot|[x_1,p_1])}\\left[\\sum_{i=1}^{2}\\hat{r}(y_{i},y^{\\ast}) - \\beta_{1}\\log(\\pi_{\\theta}(y_i|x_i)/\\pi_{\\text{ref}}(y_i|x_i))\\right]\n",
    "$$\n",
    "\n",
    "**Reward shaping to incentivize self-correction.** It is unclear if running RL for\n",
    "optimizing the above Equation prefers a strategy that incentivizes self-correction over finding the best first-attempt\n",
    "response and keeping it unchanged, to mitigate this issue, given an two-turn\n",
    "on-policy rollout $\\tau = \\{x_1, \\hat{y}_1, \\hat{r}(y_1, y^{\\ast}), x_2, \\hat{y}_{2}, \\hat{r}(y_2, y^{\\ast})\\}$, we propose to modify the reward $\\hat{r}(y_2, y^{\\ast})$ with an additional bonus given by:\n",
    "\n",
    "$$\n",
    "\\hat{b}(y_2|y_1,y^{\\ast}) = \\alpha\\cdot(\\hat{r}(y_2,y^{\\ast}) - \\hat{r}(y_1,y^{\\ast}))\n",
    "$$\n",
    "\n",
    "Adding this\n",
    "bonus to the second attempt only emphasizes traces that flip the correctness of the response and assigns\n",
    "a heavy negative penalty to transitions that change a correct response to incorrect in the second attempt.\n",
    "\n",
    "```{tip}\n",
    "SCoRe applies stages I and II in an interleaved fashion\n",
    "for multiple iterations.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4667decc-7392-44fe-91a4-639c871086f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
