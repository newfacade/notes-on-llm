{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b56cb0d3-5e07-4415-ab87-7dc63c20a4e3",
   "metadata": {},
   "source": [
    "(mla)=\n",
    "# Multi-Head Latent Attention\n",
    "\n",
    "```{note}\n",
    "Conventional Transformer models usually adopts Multi-Head Attention (MHA){cite}`vaswani2023attentionneed`, but during generation, its heavy Key-Value (KV) cache will become the bottleneck\n",
    "that limit the inference efficiency. In order to reduce the KV cache, Multi-Query Attention\n",
    "(MQA){cite}`ainslie2023gqatraininggeneralizedmultiquery`. and Grouped-Query Attention (GQA){cite}`shazeer2019fasttransformerdecodingwritehead` are\n",
    "proposed. They require a smaller magnitude of KV cache, but their performance does not match\n",
    "MHA.\n",
    "\n",
    "For DeepSeek-V2, we design an innovative attention mechanism called Multi-head Latent\n",
    "Attention (MLA){cite}`deepseekai2024deepseekv2strongeconomicalefficient`. Equipped with `low-rank key-value joint compression`, MLA achieves better\n",
    "performance than MHA, but requires a significantly smaller amount of KV cache.\n",
    "```\n",
    "\n",
    "![](../images/mla1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ede1759-af02-4926-b133-1fd80fb076c3",
   "metadata": {},
   "source": [
    "## Preliminaries: Standard Multi-Head Attention\n",
    "\n",
    "Let $d$ be the embedding dimension, $n_h$ be the number of attention heads, $d_h$ be the dimension per head, and $\\mathbf{h}_{t}\\in\\mathbb{R}^{d}$ be the attention input of the $t$-th token. Standard MHA first produces $\\mathbf{q}_{t},\\mathbf{k}_{t}, \\mathbf{v}_{t}\\in\\mathbb{R}^{d_{h}n_h}$ through three matrices $W^Q,W^K,W^V\\in\\mathbb{R}^{d_{h}n_{h}\\times{d}}$, respectively:\n",
    "\n",
    "$$\\mathbf{q}_{t} = W^{Q}\\mathbf{h}_{t}$$\n",
    "\n",
    "$$\\mathbf{k}_{t} = W^{K}\\mathbf{h}_{t}$$\n",
    "\n",
    "$$\\mathbf{v}_{t} = W^{V}\\mathbf{h}_{t}$$\n",
    "\n",
    "Then, $\\mathbf{q}_{t}, \\mathbf{k}_{t}, \\mathbf{v}_{t}$ will be sliced into $n_h$ heads for the multi-head attention computation:\n",
    "\n",
    "$$[\\mathbf{q}_{t,1};\\mathbf{q}_{t,2};\\dots;\\mathbf{q}_{t,n_h}] = \\mathbf{q}_{t}$$\n",
    "\n",
    "$$[\\mathbf{k}_{t,1};\\mathbf{k}_{t,2};\\dots;\\mathbf{k}_{t,n_h}] = \\mathbf{k}_{t}$$\n",
    "\n",
    "$$[\\mathbf{v}_{t,1};\\mathbf{v}_{t,2};\\dots;\\mathbf{v}_{t,n_h}] = \\mathbf{v}_{t}$$\n",
    "\n",
    "$$\\mathbf{o}_{t,i} = \\sum_{j=1}^{t}\\text{Softmax}_{j}\\left(\\frac{\\mathbf{q}_{t,i}^{\\intercal}\\mathbf{k}_{j,i}}{\\sqrt{d_h}}\\right)\\mathbf{v}_{j,i}$$\n",
    "\n",
    "$$\\mathbf{u}_{t} = W^{O}[\\mathbf{o}_{t,1};\\mathbf{o}_{t,2};\\dots;\\mathbf{o}_{t,n_h}]$$\n",
    "\n",
    "where $\\mathbf{q}_{t,i},\\mathbf{k}_{t,i},\\mathbf{v}_{t,i}\\in\\mathbb{R}^{d_h}$ denote the query, key, and value of the $i$-th attention head; $W^{O}\\in\\mathbb{R}^{d\\times{d_{h}n_{h}}}$ denotes the output projection matrix.\n",
    "\n",
    "```{figure} ../images/mla-step1.svg\n",
    "---\n",
    "height: 500px\n",
    "name: mha\n",
    "---\n",
    "Multi-Head Attention, the red text indicates that it needs to be cached.\n",
    "```\n",
    "\n",
    "### Why KV cache\n",
    "\n",
    "```{tip}\n",
    "During inference, all keys and values need\n",
    "to be cached to accelerate inference (the keys and values need to be computed only once), so MHA needs to cache $2n_{h}d_{h}l$ ($l$ denote layer num) elements for each token. In\n",
    "model deployment, this heavy KV cache is a large bottleneck that limits the maximum batch\n",
    "size and sequence length.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bc714f-b4ba-48c7-a648-bd96f4ae68d4",
   "metadata": {},
   "source": [
    "## Low-Rank Key-Value Joint Compression\n",
    "\n",
    "The core of MLA is the low-rank joint compression for keys and values to reduce KV cache:\n",
    "\n",
    "$$\\mathbf{c}_{t}^{KV} = W^{DKV}\\mathbf{h}_{t}$$\n",
    "\n",
    "$$\\mathbf{k}_{t}^{C} = W^{UK}\\mathbf{c}_{t}^{KV}$$\n",
    "\n",
    "$$\\mathbf{v}_{t}^{C} = W^{UV}\\mathbf{c}_{t}^{KV}$$\n",
    "\n",
    "where $\\mathbf{c}_{t}^{KV}\\in\\mathbb{R}^{d_c}$ is the compressed latent vector for keys and values, $d_c\\ll d_{h}n_{h}$ denotes the KV\n",
    "compression dimension, $W^{DKV}\\in\\mathbb{R}^{d_{c}\\times d}$ and $W^{UK},W^{UV}\\in\\mathbb{R}^{d_{h}n_{h}\\times d_c}$. During inference, MLA only\n",
    "needs to cache $\\mathbf{c}_{t}^{KV}$, so its KV cache has only $d_{c}l$ elements.\n",
    "\n",
    "\n",
    "```{figure} ../images/mla-step2.svg\n",
    "---\n",
    "height: 500px\n",
    "name: mla-1\n",
    "---\n",
    "Core of MLA: low-rank key-value joint compression.\n",
    "```\n",
    "\n",
    "In addition, during inference (omit index $i$ for brevity):\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{q}_{t}^{\\intercal}\\mathbf{k}_{j}^{C} &= (W^{Q}\\mathbf{h}_{t})^{\\intercal}W^{UK}\\mathbf{c}_{j}^{KV}\\\\\n",
    "&= \\mathbf{h}_{t}^{\\intercal}(W^{Q})^{\\intercal}W^{UK}\\mathbf{c}_{j}^{KV}\\\\\n",
    "&= \\mathbf{h}_{t}^{\\intercal}((W^{UK})^{\\intercal}W^{Q})^{\\intercal}\\mathbf{c}_{j}^{KV}\\\\\n",
    "&= ((W^{UK})^{\\intercal}W^{Q}\\mathbf{h}_{t})^{\\intercal}\\mathbf{c}_{j}^{KV}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$W^{UK}$ can be absorbed into $W^{Q}$, that is:\n",
    "\n",
    "$$W^{Q}\\leftarrow(W^{UK})^{\\intercal}W^{Q}.$$\n",
    "\n",
    "Similarily, $W^{UV}$ can be absorbed into $W^{O}$. We even do not need to compute keys and values out for attention."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac81be7-b434-446a-8d11-3ffa2beaca3b",
   "metadata": {},
   "source": [
    "### Low-Rank Compression for Queries\n",
    "\n",
    "In order to reduce the activation memory during training, we also perform low-rank compression for the queries, even if it cannot reduce the KV cache:\n",
    "\n",
    "$$\\mathbf{c}_{t}^{Q} = W^{DQ}\\mathbf{h}_{t}$$\n",
    "\n",
    "$$\\mathbf{q}_{t}^{C} = W^{UQ}\\mathbf{c}_{t}^{Q}$$\n",
    "\n",
    "where $\\mathbf{c}_{t}^{Q}\\in\\mathbb{R}^{{d_{c}}'}$ is the compressed latent vector for queries, ${d_c}'\\ll d_{h}n_{h}$ denotes the query\n",
    "compression dimension, $W^{DQ}\\in\\mathbb{R}^{{d_c}'\\times d}$ and $W^{UQ}\\in\\mathbb{R}^{d_{h}n_{h}\\times {d_c}'}$.\n",
    "\n",
    "```{figure} ../images/mla-step3.svg\n",
    "---\n",
    "height: 500px\n",
    "name: mla-2\n",
    "---\n",
    "Compression for queries.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b087d0-d5f0-4908-b2c9-8f311651fab7",
   "metadata": {},
   "source": [
    "## Decoupled Rotary Position Embedding\n",
    "\n",
    "RoPE{cite}`su2023roformerenhancedtransformerrotary` is position-sensitive for both keys and queries:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{RoPE}(\\mathbf{q}_{t,i})^{\\intercal}\\text{RoPE}(\\mathbf{k}_{j,i}^{C}) &= \\text{RoPE}(W^{Q,i}\\mathbf{h}_{t})^{\\intercal}\\text{RoPE}(W^{UK,i}\\mathbf{c}_{j}^{KV}) \\\\\n",
    "&= (\\mathcal{R}_{t}W^{Q,i}\\mathbf{h}_{t})^{\\intercal}\\mathcal{R}_{j}W^{UK,i}\\mathbf{c}_{j}^{KV}\\\\\n",
    "&= \\mathbf{h}_{t}^{\\intercal}(\\mathcal{R}_{t}W^{Q,i})^{\\intercal}\\mathcal{R}_{j}W^{UK,i}\\mathbf{c}_{j}^{KV}\\\\\n",
    "&= \\mathbf{h}_{t}^{\\intercal}(W^{Q,i})^{\\intercal}(\\mathcal{R}_{t})^{\\intercal}\\mathcal{R}_{j}W^{UK,i}\\mathbf{c}_{j}^{KV}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$W^{UK}$ cannot be absorbed into $W^{Q}$ any more during inference, since a RoPE matrix\n",
    "related to the currently generating token will lie between $W^{Q}$ and $W^{UK}$ and matrix multiplication\n",
    "does not obey a commutative law.\n",
    "\n",
    "As a solution, we propose the decoupled RoPE strategy that uses additional multi-head\n",
    "queries $\\mathbf{q}_{t,i}^{R}\\in\\mathbb{R}^{d_h^{R}}$ and a `shared` key $\\mathbf{k}_{t}^{R}\\in\\mathbb{R}^{d_h^{R}}$ to carry RoPE, where $d_{h}^{R}$ denotes the per-head\n",
    "dimension of the decoupled queries and key. Equipped with the decoupled RoPE strategy, MLA\n",
    "performs the following computation:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\left[\\mathbf{q}_{t,1}^{R},\\mathbf{q}_{t,2}^{R},\\dots,\\mathbf{q}_{t,n_h}^{R}\\right] = \\mathbf{q}_{t}^{R} &= \\text{RoPE}(W^{QR}\\mathbf{c}_{t}^{Q})\\\\\n",
    "\\mathbf{k}_{t}^{R} &= \\text{RoPE}(W^{KR}\\mathbf{h}_t)\\\\\n",
    "\\mathbf{q}_{t,i} &= [\\mathbf{q}_{t,i}^{C};\\mathbf{q}_{t,i}^{R}]\\\\\n",
    "\\mathbf{k}_{t,i} &= [\\mathbf{k}_{t,i}^{C};\\mathbf{k}_{t}^{R}]\\\\\n",
    "\\mathbf{o}_{t,i} &= \\sum_{j=1}^{t}\\text{Softmax}_{j}\\left(\\frac{\\mathbf{q}_{t,i}^{\\intercal}\\mathbf{k}_{j,i}}{\\sqrt{d_h+d_{h}^{R}}}\\right)\\mathbf{v}_{j,i}\\\\\n",
    "\\mathbf{u}_{t} &= W^{O}[\\mathbf{o}_{t,1};\\mathbf{o}_{t,2};\\dots;\\mathbf{o}_{t,n_h}]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $W^{QR}\\in\\mathbb{R}^{d_{h}^{R}n_{h}\\times {d_{c}}'}$ and $W^{KR}\\in\\mathbb{R}^{d_{h}^{R}n_{h}\\times d}$ are matrices to produce the decouples queries and key. During inference, the decoupled key should also be cached. Therefore,\n",
    "DeepSeek-V2 requires a total KV cache containing $(d_c+d_{h}^{R})l$ elements.\n",
    "\n",
    "```{figure} ../images/mla-3x.svg\n",
    "---\n",
    "height: 600px\n",
    "name: mla-3\n",
    "---\n",
    "Multi-head Latent Attention.\n",
    "```\n",
    "\n",
    "```{tip}\n",
    "MLA uses decoupled keys and queries to carry RoPE, where keys are shared across tokens to save cache.\n",
    "```\n",
    "```{caution}\n",
    "Why a shared key is enough?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d621a488-1218-4a43-b767-2a6661386b34",
   "metadata": {},
   "source": [
    "## Ablation of Attention Mechanisms\n",
    "\n",
    "### Ablation of MHA, GQA, and MQA\n",
    "\n",
    "We show the evaluation results for 7B dense models with MHA, GQA, and MQA on four hard\n",
    "benchmarks in Table 8. All of these three models are trained on 1.33T tokens, and share the same\n",
    "architecture except for the attention mechanisms. In addition, for a fair comparison, we align\n",
    "the number of parameters of them to around 7B by adjusting the number of layers. From the\n",
    "table, we can find that MHA demonstrates significant advantages over GQA and MQA on these\n",
    "benchmarks.\n",
    "\n",
    "![](../images/mla-6.png)\n",
    "\n",
    "### Comparison Between MLA and MHA\n",
    "\n",
    "In Table 9, we show the evaluation results for MoE models equipped with MLA and MHA,\n",
    "respectively, on four hard benchmarks. For a solid conclusion, we train and evaluate models\n",
    "across two scales. Also, two small MoE models and two large MoE models respectively\n",
    "share the same architecture except for the attention mechanisms. From the table, we can observe\n",
    "that MLA shows better performance than MHA. More importantly, MLA requires a significantly smaller amount of KV cache (14% for small MoE models and 4% for large MoE models) than\n",
    "MHA.\n",
    "\n",
    "![](../images/mla-7.png)\n",
    "\n",
    "```{caution}\n",
    "Comparison Between MLA and MHA for dense models?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fca9f32-a063-46a5-92c4-ae26eec51e95",
   "metadata": {},
   "source": [
    "## Comparison of Key-Value Cache\n",
    "\n",
    "```{note}\n",
    "We demonstrate a comparison of the KV cache per token among different attention mechanisms\n",
    "in Table 1. MLA requires only a small amount of KV cache, equal to GQA with only 2.25 groups,\n",
    "but can achieve stronger performance than MHA.\n",
    "```\n",
    "\n",
    "![](../images/mla-4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b346f6b8-c580-45e6-bb51-9816bd577a85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
