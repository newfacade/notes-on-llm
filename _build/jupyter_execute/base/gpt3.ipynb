{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9288366b-247a-4c60-8f66-2b92fc5483b3",
   "metadata": {},
   "source": [
    "# GPT3\n",
    "\n",
    "```{note}\n",
    "Language Models are Few-Shot Learners{cite}`brown2020languagemodelsfewshotlearners`\n",
    "```\n",
    "```{note}\n",
    "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training\n",
    "on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic\n",
    "in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of\n",
    "thousands of examples. By contrast, humans can generally perform a new language task from only\n",
    "a few examples or from simple instructions – something which current NLP systems still largely\n",
    "struggle to do. Here we show that `scaling up language models` greatly improves task-agnostic,\n",
    "few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art finetuning\n",
    "approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion\n",
    "parameters, 10x more than any previous non-sparse language model, and test its performance in\n",
    "the few-shot setting.\n",
    "```\n",
    "```{tip}\n",
    "Problems with the traditional pretrain-and-finetune paradigm:\n",
    "1. From a practical perspective, the need for a large dataset of labeled examples for every new task limits the applicability of language models.\n",
    "2. The potential to exploit spurious correlations in training data fundamentally grows with the expressiveness\n",
    "of the model and the narrowness of the training distribution.\n",
    "3. Humans do not require large supervised datasets to learn most language tasks – a brief directive in natural\n",
    "language or at most a tiny number\n",
    "of demonstrations is often sufficient to enable a human to perform a new task to at least a reasonable degree of competence.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c438d5e9-6954-42c0-ac9d-b903c86d3028",
   "metadata": {},
   "source": [
    "## Approach\n",
    "\n",
    "```{figure} ../images/gpt3-1.png\n",
    "```\n",
    "\n",
    "### Model and Architectures\n",
    "\n",
    "We use the same model and architecture as GPT-2{cite}`radford2019language`, with the exception that we use alternating dense and locally banded sparse\n",
    "attention patterns in the layers of the transformer, similar to the Sparse Transformer{cite}`child2019generatinglongsequencessparse`. To study the dependence\n",
    "of ML performance on model size, we train 8 different sizes of model, ranging over three orders of magnitude from 125\n",
    "million parameters to 175 billion parameters, with the last being the model we call GPT-3. Previous work{cite}`kaplan2020scalinglawsneurallanguage` suggests that with enough training data, scaling of validation loss should be approximately a smooth power law as a\n",
    "function of size; training models of many different sizes allows us to test this hypothesis both for validation loss and for\n",
    "downstream language tasks.\n",
    "\n",
    "```{figure} ../images/gpt3-2.png\n",
    "```\n",
    "\n",
    "### Training Dataset\n",
    "\n",
    "```{figure} ../images/gpt3-3.png\n",
    "```\n",
    "\n",
    "### Evaluation\n",
    "\n",
    "Broadly, on NLP tasks GPT-3 achieves promising results in the zero-shot and one-shot settings, and in the the few-shot\n",
    "setting is sometimes competitive with or even occasionally surpasses state-of-the-art (despite state-of-the-art being held\n",
    "by fine-tuned models).\n",
    "\n",
    "```{figure} ../images/gpt3-4.png\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8171fd73-2ceb-439a-8129-e8f598603573",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}