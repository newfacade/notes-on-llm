{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "161367e0-bc32-4935-86ad-cc66cfec6f51",
   "metadata": {},
   "source": [
    "# InstructGPT\n",
    "\n",
    "```{note}\n",
    "Training language models to follow instructions with human feedback{cite}`ouyang2022traininglanguagemodelsfollow`\n",
    "```\n",
    "```{note}\n",
    "Making language models bigger does not inherently make them better at following\n",
    "a user’s intent. For example, large language models can generate outputs that\n",
    "are untruthful, toxic, or simply not helpful to the user. In other words, these\n",
    "models are not aligned with their users. In this paper, we show an avenue for\n",
    "aligning language models with user intent on a wide range of tasks by fine-tuning\n",
    "with human feedback.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e696a0-2caa-4c3b-a54a-4d5f93f3a4e3",
   "metadata": {},
   "source": [
    "## Methods and experimental details\n",
    "\n",
    "### High-level methodology\n",
    "\n",
    "```{figure} ../images/instructgpt-1.png\n",
    "```\n",
    "\n",
    "We start with a pretrained language\n",
    "model, a distribution of prompts on which we want our model to produce aligned outputs, and a team\n",
    "of trained human labelers. We then apply the following three steps.\n",
    "\n",
    "**Step 1: Collect demonstration data, and train a supervised policy.** Our labelers provide demonstrations\n",
    "of the desired behavior on the input prompt distribution. We then fine-tune a pretrained GPT-3{cite}`brown2020languagemodelsfewshotlearners` model on this data using supervised learning.\n",
    "\n",
    "**Step 2: Collect comparison data, and train a reward model.** We collect a dataset of comparisons\n",
    "between model outputs, where labelers indicate which output they prefer for a given input. We then\n",
    "train a reward model to predict the human-preferred output.\n",
    "\n",
    "**Step 3: Optimize a policy against the reward model using PPO.** We use the output of the\n",
    "RM as a scalar reward. We fine-tune the supervised policy to optimize this reward using the PPO\n",
    "algorithm. We call the resulting models InstructGPT.\n",
    "\n",
    "Steps 2 and 3 can be iterated continuously; more comparison data is collected on the current best\n",
    "policy, which is used to train a new RM and then a new policy.\n",
    "\n",
    "### Dataset\n",
    "\n",
    "Our prompt dataset consists primarily of text prompts submitted to the OpenAI API, specifically\n",
    "those using an earlier version of the InstructGPT models (trained via supervised learning on a subset\n",
    "of our demonstration data) on the Playground interface. We heuristically deduplicate prompts by checking for prompts that share a long common\n",
    "prefix, and we limit the number of prompts to 200 per user ID. We also create our train, validation,\n",
    "and test splits based on user ID, so that the validation and test sets contain no data from users whose\n",
    "data is in the training set. To avoid the models learning potentially sensitive customer details, we\n",
    "filter all prompts in the training split for personally identifiable information (PII).\n",
    "\n",
    "To train the very first InstructGPT models, we asked labelers to write prompts themselves. We asked labelers\n",
    "to write three kinds of prompts:\n",
    "\n",
    "* **Plain:** We simply ask the labelers to come up with an arbitrary task, while ensuring the\n",
    "tasks had sufficient diversity.\n",
    "\n",
    "* **Few-shot:** We ask the labelers to come up with an instruction, and multiple query/response\n",
    "pairs for that instruction.\n",
    "\n",
    "* **User-based:** We had a number of use-cases stated in waitlist applications to the OpenAI\n",
    "API. We asked labelers to come up with prompts corresponding to these use cases.\n",
    "\n",
    "From these prompts, we produce three different datasets used in our fine-tuning procedure. The SFT dataset contains about 13k training\n",
    "prompts (from the API and labeler-written), the RM dataset has 33k training prompts (from the API\n",
    "and labeler-written), and the PPO dataset has 31k training prompts (only from the API).\n",
    "\n",
    "### Models\n",
    "\n",
    "**Supervised fine-tuning (SFT).** We fine-tune GPT-3 on our labeler demonstrations using supervised\n",
    "learning.\n",
    "\n",
    "**Reward modeling (RM).** Starting from the SFT model with the final unembedding layer removed,\n",
    "we trained a model to take in a prompt and response, and output a scalar reward.\n",
    "\n",
    "In order to speed up comparison collection, we present labelers with anywhere between $K=4$ and $K=9$ responses to rank. This produces $\\binom{K}{2}$ comparisons for each prompt shown to a labeler. We train on all $\\binom{K}{2}$ comparisons from each prompt as a single batch element.\n",
    "\n",
    "The loss function for the reward model is:\n",
    "\n",
    "$$\n",
    "\\log(\\theta) = \\frac{1}{-\\binom{K}{2}}\\mathbb{E}_{(x, y_{w}, y_{l})\\sim D}\\left[\\log(\\sigma(r_{\\theta}(x, y_{w}) - r_{\\theta}(x, y_{l})))\\right]\n",
    "$$\n",
    "\n",
    "where $r_{\\theta}(x, y)$ is the scalar output of the reward model for prompt $x$ and completion $y$ with parameters $\\theta$, $y_{w}$ is the preferred completion out of the pair $y_{w}$ and $y_{l}$, and $D$ is the dataset of human\n",
    "comparisons.\n",
    "\n",
    "```{tip}\n",
    "We employ the Bradley-Terry model for pairwise comparison of competitors, where the strength parameter for $(x, y)$ is set to $\\exp(r(x, y))$. Then:\n",
    "\n",
    "$$p(y_{1}\\succ y_{2}|x) = \\frac{\\exp(r(x, y_1))}{\\exp(r(x, y_1)) + \\exp(r(x, y_2))}=\\sigma(r(x,y_1) - r(x,y_2))$$\n",
    "```\n",
    "\n",
    "**Reinforcement learning (RL).** We fine-tuned the SFT model on our environment using PPO. Given\n",
    "the prompt and response, the reward model produces a reward and ends the episode. In addition, we add a per-token KL penalty from the SFT model at each token to mitigate over-optimization\n",
    "of the reward model. The value function is initialized from the RM. We call these\n",
    "models “PPO.”\n",
    "\n",
    "We also experiment with mixing the pretraining gradients into the PPO gradients, in order to fix the\n",
    "performance regressions on public NLP datasets. We call these models “PPO-ptx.” We maximize the\n",
    "following combined objective function in RL training:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{objective} = &\\mathbb{E}_{(x, y)\\sim D_{\\pi_{\\phi}^{\\text{RL}}}}\\left[r_{\\theta}(x, y) - \\beta\\log\\left(\\pi_{\\phi}^{\\text{RL}}(y|x) / \\pi^{\\text{SFT}}(y|x)\\right)\\right] + \\\\\n",
    "&\\gamma\\mathbb{E}_{x\\sim D_{\\text{pretrain}}}\\left[\\log(\\pi_{\\phi}^{\\text{RL}}(x))\\right]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $\\pi_{\\phi}^{\\text{RL}}$ is the learned RL policy, $\\pi^{\\text{SFT}}$ is the supervised trained model, and $D_{\\text{pretrain}}$ is the\n",
    "pretraining distribution. The KL reward coefficient, $\\beta$, and the pretraining loss coefficient, $\\gamma$, control\n",
    "the strength of the KL penalty and pretraining gradients respectively.\n",
    "\n",
    "```{tip}\n",
    "For an event $X$ with probability $p$, it's self information is\n",
    "\n",
    "$$I(X) = -\\log p(x)$$\n",
    "\n",
    "The less probable an event is, the more surprising it is and the more information it yields. The term\n",
    "\n",
    "$$\\log\\frac{p(x)}{q(x)} = -\\log q(x) - (-\\log p(x))$$\n",
    "\n",
    "can be interpreted as our relative surprise. The KL divergence between $P$ and $Q$ is\n",
    "\n",
    "$$\\mathbb{E}_{x\\sim P}\\left[\\log\\frac{p(x)}{q(x)}\\right]$$\n",
    "\n",
    "can be interpreted as the expected relative surprise from using $Q$ instead of $P$ when the actual distribution is $P$. It measures how one probability distribution $P$ is different from the reference probability distribution $Q$.\n",
    "```\n",
    "\n",
    "```{tip}\n",
    "The implementation details of PPO can be found in [this blog](https://newfacade.github.io/notes-on-reinforcement-learning/17-ppo-trl.html).\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3554b667-96bc-4235-97d2-4d2035f47788",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "```{figure} ../images/instructgpt-2.png\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bca0bd-755f-4145-bbee-4bcd10a31a1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}