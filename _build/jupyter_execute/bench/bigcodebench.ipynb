{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae3c2b6a-0be0-4704-a3d4-a840861fceb2",
   "metadata": {},
   "source": [
    "# BigCodeBench\n",
    "\n",
    "```{note}\n",
    "[BigCodeBench](https://github.com/bigcode-project/bigcodebench) is an `easy-to-use` benchmark for solving `practical` and `challenging` tasks via code. It aims to evaluate the true programming capabilities of large language models (LLMs) in a more realistic setting. The benchmark is designed for HumanEval-like function-level code generation tasks, but with much more complex instructions and diverse function calls.\n",
    "\n",
    "There are two splits in BigCodeBench:\n",
    "* Complete: Thes split is designed for code completion based on the comprehensive docstrings.\n",
    "* Instruct: The split works for the instruction-tuned and chat models only, where the models are asked to generate a code snippet based on the natural language instructions.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbb763a-d139-4a77-8609-9f28a4c85510",
   "metadata": {},
   "source": [
    "## Benchmark Construction\n",
    "\n",
    "```{figure} ../images/bcb1.png\n",
    "```\n",
    "\n",
    "### Data Synthesis\n",
    "\n",
    "Given a code snippet of API usage with a brief\n",
    "human instruction as the seed example, an LLM is instructed to enrich the programming\n",
    "intent and refine the corresponding implementation by using diverse libraries. We instruct the model with a 2-shot in-context\n",
    "demonstration.\n",
    "\n",
    "### Semi-Automatic Program Refactoring and Testing Case Generation\n",
    "\n",
    "Programs synthesized by LLMs may contain various issues, without proper verification, the implementation cannot directly serve as a ground-truth solution.\n",
    "To construct a high-quality execution-based benchmark, we need to add test cases that can rigorously\n",
    "verify the correctness of programs and identify any bugs.\n",
    "\n",
    "### Human Curation\n",
    "\n",
    "To enhance the benchmark quality, we implement a three-fold human curation process:\n",
    "\n",
    "* Examination\n",
    "* Pre-Evaluation\n",
    "* Cross-Checking\n",
    "\n",
    "### Benchmarking NL-Oriented Instructions to Code Generation\n",
    "\n",
    "```{figure} ../images/bcb2.png\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503cc4e1-60b1-4804-b42a-4da756ae9f82",
   "metadata": {},
   "source": [
    "## Benchmark Statistics\n",
    "\n",
    "```{figure} ../images/bcb3.png\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a56194-08a0-4b0b-a05d-e6adfdd5e0f6",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Our extensive evaluation of 60 LLMs shows that LLMs\n",
    "are not yet capable of following complex instructions to use function calls precisely,\n",
    "with scores up to 60%, significantly lower than the human performance of\n",
    "97%.\n",
    "\n",
    "```{figure} ../images/bcb4.png\n",
    "```\n",
    "```{figure} ../images/bcb5.png\n",
    "```\n",
    "\n",
    "````{tip}\n",
    "```python\n",
    "from titlecase import titlecase\n",
    "\n",
    "s = \"BENCHMARK CONSTRUCTION\"\n",
    "print(titlecase(s))\n",
    "```\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2bf420-a06a-4039-a2e6-128878034b7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}