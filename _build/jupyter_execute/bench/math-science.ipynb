{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eba31f57-9312-490f-a829-9d5fee7d3a04",
   "metadata": {},
   "source": [
    "# Math & Science Benchmarks\n",
    "\n",
    "## MATH\n",
    "\n",
    "The [MATH](https://github.com/hendrycks/math) dataset consists of problems from mathematics competitions including the\n",
    "AMC 10, AMC 12, AIME, and more. These competitions span decades and assess the mathematical\n",
    "problem-solving ability of the best young mathematical talent in the United States. Unlike\n",
    "most prior work, most problems in MATH cannot be solved with a straightforward application of\n",
    "standard K-12 mathematics tools.\n",
    "\n",
    "```{figure} ../images/math-0.png\n",
    "```\n",
    "\n",
    "The Mathematics Aptitude Test of Heuristics dataset, abbreviated MATH, has 12;500 problems\n",
    "(7;500 training and 5;000 test). With this many training problems, models can learn many useful\n",
    "heuristics for problem solving. Each problem has a step-by-step solution and a final boxed answer.\n",
    "\n",
    "```{figure} ../images/math-1.png\n",
    "---\n",
    "height: 500px\n",
    "---\n",
    "```\n",
    "\n",
    "**Formatting.** Problems and solutions are consistently formatted using LATEX and the Asymptote\n",
    "vector graphics language. To assess models using exact match, we force the final boxed answers to follow consistent formatting\n",
    "rules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efc504d-3aeb-411b-a07d-1ab1684f2c17",
   "metadata": {},
   "source": [
    "## MATH 500\n",
    "\n",
    "This dataset contains a subset of 500 problems from the MATH benchmark that OpenAI created in their Let's Verify Step by Step paper{cite}`lightman2023letsverifystepstep`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bafe31d-3b6c-434a-8a72-5ce6b63598c6",
   "metadata": {},
   "source": [
    "## GSM8K\n",
    "\n",
    "```{note}\n",
    "Introduced in the paper Training Verifiers to Solve Math Word Problems{cite}`cobbe2021trainingverifierssolvemath`.\n",
    "```\n",
    "\n",
    "GSM8K consists of 8.5K high quality grade school math problems created by human problem writers. We segmented these into 7.5K training problems and 1K test problems. These problems take between 2 and 8 steps to solve, and solutions primarily involve performing a sequence of elementary calculations using basic arithmetic operations (+ - / *) to reach the final answer. A bright middle school student should be able to solve every problem.\n",
    "\n",
    "```{figure} ../images/math-2.png\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8fccd4-d512-4646-9f5c-90e162e7a2d2",
   "metadata": {},
   "source": [
    "## GPQA\n",
    "\n",
    "[GPQA](https://github.com/idavidrein/gpqa) is a challenging dataset of 448 multiple-choice questions written by\n",
    "domain experts in biology, physics, and chemistry. We ensure that the questions are\n",
    "high-quality and extremely difficult: experts who have or are pursuing PhDs in the\n",
    "corresponding domains reach 65% accuracy, while highly skilled non-expert validators only\n",
    "reach 34% accuracy. The questions are also\n",
    "difficult for state-of-the-art AI systems, with our strongest GPT-4â€“based baseline\n",
    "achieving 39% accuracy. The difficulty of GPQA both for skilled non-experts and frontier\n",
    "AI systems should enable realistic `scalable oversight` experiments, which we hope\n",
    "can help devise ways for human experts to reliably get truthful information from AI\n",
    "systems that surpass human capabilities.\n",
    "\n",
    "```{tip}\n",
    "Scalable Oversight is a key research direction in the field of AI Alignment, aiming to address the challenge of how to continuously and reliably supervise the behavior of AI systems and ensure their alignment with human values when their capabilities surpass human levels.\n",
    "```\n",
    "\n",
    "**GPQA Diamond** We create GPQA Diamond (the diamond set, composed of 198 questions) is\n",
    "our highest quality subset which includes only questions where both experts answer correctly and\n",
    "the majority of non-experts answer incorrectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239aaf1d-83cf-4a68-b128-2e59a8f6df63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}