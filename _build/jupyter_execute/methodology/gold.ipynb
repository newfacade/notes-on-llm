{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dd84950-0c31-4079-9f6e-e1f268e148e0",
   "metadata": {},
   "source": [
    "# GOLD\n",
    "\n",
    "```{note}\n",
    "A dominant approach to text generation is to use autoregressive models learned by maximum\n",
    "likelihood estimation (MLE) on supervised data. However, this approach introduces two well-known\n",
    "discrepancies between training and evaluation objectives that lead to undesired generations. First,\n",
    "the training loss is negative log-likelihood, whereas the evaluation is based on human judgment of\n",
    "the output quality. Second, during training, the autoregressive model conditions on the gold history/prefix; however,\n",
    "at inference time it conditions on model-generated history.<br>\n",
    "We aim to bridge the gap between training and evaluation in this paper.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752dac4c-5b7a-4270-b2a8-015b42ca72d6",
   "metadata": {},
   "source": [
    "## From MLE to RL framework\n",
    "\n",
    "**MLE training.** Given a context $\\mathbf{x}$, we want to generate a sequence of tokens $\\mathbf{y}=(y_{0},\\dots,y_{T})$. Let $p_{\\text{human}}(\\mathbf{y}|\\mathbf{x})$ denote the data-generating distribution. Using MLE, the loss function is\n",
    "\n",
    "$$\\mathcal{L}(\\theta) = -\\mathbb{E}_{\\mathbf{y}\\sim p_{\\text{human}}}\\left[\\sum_{t=0}^{T}\\log p_{\\theta}(y_{t}|y_{0},\\dots,y_{t-1},\\mathbf{x})\\right]$$\n",
    "\n",
    "**Evaluation.** In practice, the quality of an output often relies on task-specific metrics such as fluency,\n",
    "correctness, and interestingness. Here for generality we consider perceptual quality which measures how likely a human would have generated the output given\n",
    "the context, i.e., $p_{\\text{human}}(\\mathbf{y} | \\mathbf{x})$. Thus the evaluation metric is\n",
    "\n",
    "$$-\\mathbb{E}_{\\mathbf{y}\\sim p_{\\theta}}\\left[\\sum_{t=0}^{T}\\log p_{\\text{human}}(y_{t}|y_{0},\\dots,y_{t-1},\\mathbf{x})\\right]$$\n",
    "\n",
    "We see that the training objective encourages high recall: the model must put\n",
    "probability mass on all human-generated sequences. In contrast, the evaluation metric encourages\n",
    "high precision: all outputs from the model must be of high quality.\n",
    "\n",
    "**RL formulation.** Let’s consider generation as a sequential decision-making process. At each time\n",
    "step $t$, the policy $\\pi_{\\theta}$ takes an action $a_{t}\\in\\mathcal{V}$, transits to the next state $s_{t+1} = (y_{0},\\dots,y_{t},\\mathbf{x})$, and receives a\n",
    "reward $r_{t}$. The policy corresponds to the generation model: $\\pi(a_{t}|s_{t}) = p_{\\theta}(a_{t}|y_{0},\\dots,y_{t},\\mathbf{x})$. We\n",
    "can thus represent a sequence as a trajectory $\\tau=(s_{0}, a_{0}, r_{0}, \\dots, s_{T}, a_{T}, r_{T})$. The set of trajectories\n",
    "derived from the training data is called demonstrations which show the desired behavior of a policy. The RL objective is to maximize\n",
    "\n",
    "$$J(\\theta) = \\mathbb{E}_{\\tau\\sim\\pi_{\\theta}}\\left[\\sum_{t=0}^{T}\\gamma^{t}r_{t}\\right]$$\n",
    "\n",
    "If we knew oracle rewards $r_{t}=p_{\\text{human}}(a_{t}|s_{t})$, then this objective would be exactly the evaluation metric we want to optimize."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f6b93c-cb1e-4879-9481-bfdb61387a15",
   "metadata": {},
   "source": [
    "## Off-policy policy gradient\n",
    "\n",
    "**Policy gradient.** A straightforward way to optimize $J(\\theta)$ is policy gradient:\n",
    "\n",
    "$$\\nabla_{\\theta}J(\\theta) = \\mathbb{E}_{\\tau\\sim\\pi_{\\theta}}\\left[\\sum_{t}\\nabla_{\\theta}\\log\\pi_{\\theta}(a_{t}|s_{t})\\hat{Q}(s_{t}, a_{t})\\right]$$\n",
    "\n",
    "where $\\hat{Q}(s_{t}, a_{t})$ is the estimated return from state $s_{t}$. The expectation is estimated\n",
    "by Monte Carlo samples from $\\pi_{\\theta}$. In text generation, the return $\\hat{Q}(s_{t}, a_{t})$ is often a sequence-level\n",
    "reward such as BLEU. In practice, the policy is likely to get stuck in a region of zero reward during\n",
    "training, generating gibberish without receiving any learning signal.\n",
    "\n",
    "**Offline learning.** To avoid zero-reward regions, we would like to reduce interaction with the\n",
    "environment and stay close to the demonstrated trajectories. In the extreme case, the policy is\n",
    "learned solely from the static demonstrations without additional interaction with the environment,\n",
    "which is referred to as the offline setting.\n",
    "\n",
    "In the offline setting, we cannot estimate the expected return of $\\pi_{\\theta}$ by sampling trajectories from\n",
    "it, and must use trajectories from a different behavioral policy $\\pi_{b}$. A common technique to estimate expectations under one distribution $\\pi_{\\theta}$ given samples from a\n",
    "different distribution $\\pi_{b}$ is importance sampling:\n",
    "\n",
    "$$\\mathbb{E}_{\\tau\\sim\\pi_{b}}\\left[\\sum_{t}w_{t}\\nabla_{\\theta}\\log\\pi_{\\theta}(a_{t}|s_{t})\\hat{Q}(s_{t}, a_{t})\\right]$$\n",
    "\n",
    "with importance weights $w_{t} = \\Pi_{t'=0}^{t}\\frac{\\pi_{\\theta}(a_{t'}|s_{t'})}{\\pi_{b}(a_{t'}|s_{t'})}$\n",
    "\n",
    "**Approximations.** In practice, we use the per-action approximation: $w_{t} = \\frac{\\pi_{\\theta}(a_{t}|s_{t})}{\\pi_{b}(a_{t}|s_{t})}$. Although this estimator is biased,\n",
    "empirically it has been shown to reduce variance and work reasonably well if $pi_{b}$ and $\\pi_{\\theta}$ are close. Another obstacle is that we do not know $\\pi_{b}$ which produced\n",
    "the demonstrations $\\mathcal{D}=\\{(\\mathbf{x}^{(i)}, \\mathbf{y}^{(i)})\\}_{i=1}^{N}$. One option is to estimate $\\pi_{b}$ on $\\mathcal{D}$. Here we take a simpler\n",
    "approach that uses the empirical distribution: $\\pi_{b}(\\tau)=1/N$ for $\\tau\\in\\mathcal{D}$ and 0 otherwise. As a result,\n",
    "the denominator in $w_{t}$ is a constant and can be ignored in optimization. Our final approximated\n",
    "gradient:\n",
    "\n",
    "$$\\nabla_{\\theta}J(\\theta) = \\sum_{i=1}^{N}\\sum_{t=0}^{N}\\pi_{\\theta}(a_{t}^{i}|s_{t}^{i})\\log\\pi_{\\theta}(a_{t}^{i}|s_{t}^{i})\\hat{Q}(s_{t}^{i}, a_{t}^{i})$$\n",
    "\n",
    "Compared with the MLE gradient: $\\sum_{i=1}^{N}\\sum_{t=0}^{N}\\log\\pi_{\\theta}(a_{t}^{i}|s_{t}^{i})$, our gradient upweights actions with high return and actions\n",
    "preferred by the current policy $\\pi_{\\theta}$. Intuitively, it encourages the learning algorithm to focus on “easy”\n",
    "examples (high likelihood under the model) which improves precision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cf6689-bba4-4bd7-bce1-0115b86febc5",
   "metadata": {},
   "source": [
    "## Reward\n",
    "\n",
    "Let $R$ be the reward function such that $r_{t} = R(s_{t}, a_{t})$. To optimize the perceptual quality of a\n",
    "sequence, we want $R(s, a)$ to approximate $p_{\\text{human}}(a|s)$. In the offline setting,\n",
    "we can restrict the domain of $R$ to state-action pairs on the demonstrations. Next, we propose three\n",
    "reward functions.\n",
    "\n",
    "$\\mathbf{\\delta}$**-reward.** An obvious choice is a sequence-level reward, which considers all demonstrations to be\n",
    "equally good and assigns zero reward to any other outputs. where a reward of one is received in the terminal state for any trajectory in the demonstrations.\n",
    "\n",
    "**Estimated $p_{\\textbf{human}}$.** While $p_{\\text{MLE}}$ is not a good reward function in general since it can assign\n",
    "large probability mass to low-quality outputs, however, it is a reasonable approximation to $p_{\\text{human}}$ when\n",
    "restricted to the demonstrations. Our first reward function corresponds to a product of probabilities when\n",
    "summed over the trajectory:\n",
    "\n",
    "$$R_{p}(s, a) := \\log p_{\\text{MLE}}(a|s)$$\n",
    "\n",
    "To allow for partial credits even\n",
    "if bad actions are taken at certain steps, we define another reward function corresponding to the sum\n",
    "of probabilities:\n",
    "\n",
    "$$R_{s}(s, a) := p_{\\text{MLE}}(a|s)$$\n",
    "\n",
    "Assuming $\\gamma=1$, the return $\\hat{Q}(s_{t}, a_{t}) = \\sum_{t'=t}^{T}p_{\\text{MLE}}(a|s)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f273ec-b806-478b-abee-9103c41a857b",
   "metadata": {},
   "source": [
    "## The GOLD algorithm\n",
    "\n",
    "![](../images/gold1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f3ba68-6ecd-494b-a7d4-ce8f9dc141d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}