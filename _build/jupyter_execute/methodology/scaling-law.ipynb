{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de5b1fed-2748-4684-9401-fb663b54abf6",
   "metadata": {},
   "source": [
    "# Scaling Laws for Neural Language Models\n",
    "\n",
    "```{note}\n",
    "We study empirical scaling laws for language model performance on the cross-entropy loss.\n",
    "The loss scales as a power-law with model size, dataset size, and the amount of compute\n",
    "used for training.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceee889e-f69e-46ec-be51-7c4e4b43ab9f",
   "metadata": {},
   "source": [
    "## Parameter and Compute Scaling of Transformers\n",
    "\n",
    "We parameterize the Transformer architecture using hyperparameters $n_{\\text{layer}}$(number of layers), $d_{\\text{model}}$(dimension\n",
    "of the residual stream), $d_{\\text{ff}}$(dimension of the intermediate feed-forward layer), $d_{\\text{attn}}$(dimension of\n",
    "the attention output), and $n_{\\text{heads}}$(number of attention heads per layer).\n",
    "\n",
    "$$N_{\\text{multi-head}} \\approx 3 d_{\\text{model}}\\times\\frac{d_{\\text{attn}}}{n_{\\text{heads}}}\\times n_{\\text{heads}} + d_{\\text{attn}}\\times d_{\\text{model}} = 4 d_{\\text{model}}d_{\\text{attn}}$$\n",
    "\n",
    "$$N_{\\text{feed-forward}} \\approx 2d_{\\text{model}}d_{\\text{ff}}$$\n",
    "\n",
    "We use $N$ to denote the model size, which we define as the number of non-embedding parameters\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "N &= n_{\\text{layer}}(N_{\\text{multi-head}} + N_{\\text{feed-forward}})\\\\\n",
    "&\\approx 2n_{\\text{layer}}d_{\\text{model}}(2d_{\\text{attn}} + d_{\\text{ff}})\\\\\n",
    "&= 12n_{\\text{layer}}d_{\\text{model}}^{2}\\quad\\text{with the standard}\\quad d_{\\text{attn}} = d_{\\text{ff}} / 4 = d_{\\text{model}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Evaluating a forward pass of the Transformer involves roughly\n",
    "\n",
    "$$C_{\\text{forward}} = 2N + 2n_{\\text{layer}}n_{\\text{ctx}}d_{\\text{model}}$$\n",
    "\n",
    "For contexts and models with $d_{\\text{model}} \\gg n_{\\text{ctx}}/12$, we have $C\\approx 6N$ when accounting for the backwards\n",
    "pass."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3282d4ff-b214-4f3b-8ff6-5e6f82f5a00d",
   "metadata": {},
   "source": [
    "## Empirical Results and Basic Power Laws\n",
    "\n",
    "To characterize language model scaling we train a wide variety of models, varying a number of factors\n",
    "including:\n",
    "\n",
    "* Model size\n",
    "* Dataset size\n",
    "* Shape\n",
    "* Context length\n",
    "* Batch size\n",
    "\n",
    "Transformer performance depends very weakly on the shape parameters $n_{\\text{layer}}, n_{\\text{heads}}$, and $d_{\\text{ff}}$ when we hold\n",
    "the total non-embedding parameter count $N$ fixed.\n",
    "\n",
    "### Performance with Non-Embedding Parameter Count $N$\n",
    "\n",
    "![](../images/scale1.png)\n",
    "\n",
    "We find a steady trend with non-embedding parameter count $N$:\n",
    "\n",
    "$$L(N) \\approx \\left(\\frac{N_{c}}{N}\\right)^{\\alpha_{N}}$$\n",
    "\n",
    "where $\\alpha_{N}\\sim 0.076$, $N_{c}\\sim 8.8\\times 10^{13}$(non-embedding parameters).\n",
    "\n",
    "### Performance with Dataset Size and Compute\n",
    "\n",
    "![](../images/scale2.png)\n",
    "\n",
    "For the trend with $D$ we trained a model with fixed shape on fixed subsets of the WebText2. We stopped training once the test loss ceased to decrease. We see that the resulting test losses can be\n",
    "fit with simple power-law\n",
    "\n",
    "$$L(D)\\approx\\left(\\frac{D_{c}}{D}\\right)^{\\alpha_{D}}$$\n",
    "\n",
    "The total amount of non-embedding compute used during training can be estimated as $C=6NBS$, where $B$ is the batch size, $S$ is the number of parameter updates, and the factor of 6 accounts for the forward and\n",
    "backward passes. Thus for a given value of $C$ we can scan over all models with various $N$ to find the model with the best performance on step $S=\\frac{C}{6NB}$. Note that in these results the batch size $B$ remains fixed for\n",
    "all models.\n",
    "\n",
    "The result appears as the heavy black line on the left-hand plot in Figure 1. It can be fit with\n",
    "\n",
    "$$L(C) = \\left(\\frac{C_{c}}{C}\\right)^{\\alpha_{C}}$$\n",
    "\n",
    "The figure also includes images of individual learning curves to clarify when individual models are optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a94dad-3f66-43c7-8c03-1eb94c5f94f9",
   "metadata": {},
   "source": [
    "## Charting the Infinite Data Limit and Overfitting\n",
    "\n",
    "Here we will\n",
    "study the performance of a model of size $N$ trained on a dataset with $D$ while varing $N$ and $D$ simultaneously. We have chosen the parameterization:\n",
    "\n",
    "$$L(N, D) = \\left[\\left(\\frac{N_{c}}{N}\\right)^{\\frac{\\alpha_{N}}{\\alpha_{D}}} + \\frac{D_{c}}{D}\\right]^{\\alpha_{D}}$$\n",
    "\n",
    "Fixing $D$ and sending $N\\to\\infty$, the overall loss should approach $L(D)$. Conversely, fixing $N$ and sending $D\\to\\infty$ the loss must approach $L(N)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010b338c-6c27-4ee3-ae5a-f30a2d131f48",
   "metadata": {},
   "source": [
    "## Scaling Laws with Model Size and Training Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32566c37-a3bd-4dce-a77d-4503ff7c16f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}