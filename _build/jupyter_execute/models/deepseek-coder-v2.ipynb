{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "177bfab9-cf63-45fe-9bde-78099b14a24e",
   "metadata": {},
   "source": [
    "# DeepSeek-Coder-V2\n",
    "\n",
    "```{note}\n",
    "DeepSeek-Coder-V2{cite}`deepseekai2024deepseekcoderv2breakingbarrierclosedsource` is further pre-trained from an intermediate checkpoint of DeepSeek-V2{cite}`deepseekai2024deepseekv2strongeconomicalefficient`\n",
    "with additional 6 trillion tokens.<br>\n",
    "In standard benchmark evaluations, DeepSeek-Coder-V2 achieves\n",
    "superior performance compared to closed-source models such as GPT4-Turbo, Claude 3 Opus,\n",
    "and Gemini 1.5 Pro in coding and math benchmarks.\n",
    "```\n",
    "\n",
    "```{figure} ../images/deepseek-coder-v2-1.png\n",
    "---\n",
    "height: 400px\n",
    "name: ds-coder-v2-1\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2851e622-1e4d-4cee-b6f1-3eae07bd4c40",
   "metadata": {},
   "source": [
    "## Data Collection\n",
    "\n",
    "The pre-training data for DeepSeek-Coder-V2 primarily consists of 60% source code, 10% math\n",
    "corpus, and 30% natural language corpus. Since the natural language corpus is directly sampled\n",
    "from the training dataset of DeepSeek-V2, this section focuses on the collection, cleaning, and\n",
    "filtering processes of the code and math data.\n",
    "\n",
    "We collect public repositories created before November 2023 on GitHub, We first apply the\n",
    "same filtering rules and near-deduplication as those used in the DeepSeek-Coder{cite}`guo2024deepseekcoderlargelanguagemodel`. By\n",
    "applying these filtering rules and near-deduplication, we obtain 821B code encompassing 338\n",
    "programming languages and 185B code-related text, such as markdown and issues.\n",
    "\n",
    "To collect code-related and math-related web texts from Common Crawl, we follow the\n",
    "same pipeline as DeepSeekMath{cite}`shao2024deepseekmathpushinglimitsmathematical`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288303f1-f50f-416c-93a9-1985b3c3d8f0",
   "metadata": {},
   "source": [
    "## Alignment\n",
    "\n",
    "### Supervised Fine-Tuning\n",
    "\n",
    "To build DeepSeek-Coder-V2 Chat, we construct the instruction training dataset mixed with\n",
    "code and math data. We first collect 20k code-related instruction data and 30k math related data\n",
    "from DeepSeek-Coder and DeepSeek-Math. To maintain the general ability, we also sample\n",
    "several data from the instruction data of DeepSeek-V2. Finally, we use a instruction dataset\n",
    "of 300M tokens.\n",
    "\n",
    "### Reinforcement Learning\n",
    "\n",
    "**Prompts** Considerable effort was spent collecting prompts related to code and math from\n",
    "various sources, and `each code prompt comes with corresponding test cases`. After filtering the\n",
    "prompts, there are approximately 40k data in total.\n",
    "\n",
    "**Reward Modeling** Reward models play crucial roles in the RL training. In terms of mathematical\n",
    "preference data, we obtain them using the ground-truth labels. In terms of code preference\n",
    "data, although the code compiler itself can already provide 0-1 feedback (whether the code pass\n",
    "all test cases or not), some code prompts may have a limited number of test cases, and do not\n",
    "provide full coverage, and hence directly using 0-1 feedback from the compiler may be noisy\n",
    "and sub-optimal. Therefore, we still decide to train a reward model on the data provided by the\n",
    "`compiler`, and use the reward model to provide signal during RL training, which is more robust and has better generalization ability, in comparison with raw compiler signal. As illustrated in\n",
    "Figure 2, in our in-house test sets (Leetcode and Leetcode-zh), using a reward model to provide\n",
    "RL training signal clearly outperforms using raw compiler signal.\n",
    "\n",
    "**Reinforcement Learning Algorithm** We employ Group Relative Policy Optimization[](grpo) as our RL algorithm, which is the same as what DeepSeek-V2 uses. Notably,\n",
    "GRPO is proven to be quite effective and has less cost compared with PPO, since there is no\n",
    "need to maintain an additional critic model.\n",
    "\n",
    "```{figure} ../images/deepseek-coder-v2-2.png\n",
    "---\n",
    "height: 300px\n",
    "name: ds-coder-v2-2\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db901ef2-4279-49c6-a21f-8cf0a875be5b",
   "metadata": {},
   "source": [
    "## Experimental Results\n",
    "\n",
    "```{figure} ../images/deepseek-coder-v2-bench1.png\n",
    "```\n",
    "```{figure} ../images/deepseek-coder-v2-bench2.png\n",
    "```\n",
    "```{figure} ../images/deepseek-coder-v2-bench3.png\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c09f03-b1a1-4035-940d-9247a3af78e6",
   "metadata": {},
   "source": [
    "## Takeaway\n",
    "\n",
    "```{note}\n",
    "* Considerable effort was spent collecting prompts related to code and math from various sources, and each code prompt comes with corresponding test cases.\n",
    "* train a reward model on the data provided by the compiler, and use the reward model to provide signal during RL trainin.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389da6f3-2f3e-4a6c-a5e6-eed61d77f2fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}