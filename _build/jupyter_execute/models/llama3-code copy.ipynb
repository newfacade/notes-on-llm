{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa381b5f-36bc-4746-938f-e34d773ae89a",
   "metadata": {},
   "source": [
    "# Llama3\n",
    "\n",
    "github: https://github.com/meta-llama/llama3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87177700-0831-411b-b4ea-bc8dd7b210a0",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Overview of Llama3 model:\n",
    "\n",
    "![](../images/llama3_new.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa4b6a4-a2c8-4323-958e-2209e8e0aecf",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "To improve the\n",
    "training stability, we normalize the input of each\n",
    "transformer sub-layer, instead of normalizing the\n",
    "output. We use the RMSNorm normalizing function.\n",
    "\n",
    "![](../images/llama3-pre-norm.svg)\n",
    "\n",
    "#### BatchNorm\n",
    "\n",
    "$$y = \\frac{x - \\mathbf{E}[x]}{\\sqrt{\\mathbf{Var}[x] + \\epsilon}}*\\gamma+\\beta$$\n",
    "\n",
    "The mean and standard-deviation are calculated per-dimension over the mini-batches and $\\gamma$ and $\\beta$ are learnable parameter vectors of size $C$, where $C$ is the number of features or channels of the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e78a68a3-48ad-4157-96ae-7167e0cac043",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c89bd82-3fe0-46c3-b091-712ae4536052",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (N, C)\n",
    "batch, num_features = 2, 3\n",
    "x = torch.randn(batch, num_features)\n",
    "bn = nn.BatchNorm1d(num_features)\n",
    "\n",
    "t1 = bn(x)\n",
    "t2 = (x - x.mean(axis=0)) / x.std(axis=0, unbiased=False)\n",
    "torch.allclose(t1, t2, atol=1e-3, rtol=0)  # all elments are close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12ded407-841b-4800-b306-f97a98feac87",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (N, L, C)\n",
    "batch, seq_len, num_features = 2, 3, 4\n",
    "x = torch.randn(batch, seq_len, num_features).permute(0, 2, 1)  # BatchNorm1d requires (N, C, L) input\n",
    "bn = nn.BatchNorm1d(num_features)\n",
    "\n",
    "t1 = bn(x)\n",
    "t2 = (x - x.mean(axis=[0, 2], keepdim=True)) / x.std(axis=[0, 2], unbiased=False, keepdim=True)\n",
    "torch.allclose(t1, t2, atol=1e-3, rtol=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c5f4e04-7936-432c-a8b2-19ef84db2788",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (N, C, H, W)\n",
    "batch, num_channels, height, width = 2, 3, 2, 3\n",
    "x = torch.randn(batch, num_channels, height, width)\n",
    "bn = nn.BatchNorm2d(num_channels)\n",
    "\n",
    "t1 = bn(x)\n",
    "t2 = (x - x.mean(axis=[0, 2, 3], keepdim=True)) / x.std(axis=[0, 2, 3], unbiased=False, keepdim=True)\n",
    "torch.allclose(t1, t2, atol=1e-3, rtol=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507ca8d4-1f4c-4fbc-b70b-0e58e762bffa",
   "metadata": {},
   "source": [
    "#### LayerNorm\n",
    "\n",
    "$$y = \\frac{x - \\mathbf{E}[x]}{\\sqrt{\\mathbf{Var}[x] + \\epsilon}}*\\gamma+\\beta$$\n",
    "\n",
    "The mean and standard-deviation are calculated over the last $D$ dimensions, where $D$ is the dimension of `normalized_shape`. $\\gamma$ and $\\beta$ are learnable affine transform parameters of `normalized_shape`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df53bba8-74af-4a73-9d91-90ffc069fea9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (N, L, C)\n",
    "batch, seq_len, num_features = 2, 3, 4\n",
    "x = torch.randn(batch, seq_len, num_features)\n",
    "ln = nn.LayerNorm(num_features)\n",
    "\n",
    "t1 = ln(x)\n",
    "t2 = (x - x.mean(axis=-1, keepdim=True)) / x.std(axis=-1, unbiased=False, keepdim=True)\n",
    "torch.allclose(t1, t2, atol=1e-3, rtol=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923df599-f1fb-496c-b4f0-7fb4bc422822",
   "metadata": {},
   "source": [
    "#### RMSNorm\n",
    "\n",
    "$$y = \\frac{x}{\\sqrt{\\mathbf{MeanSquare}[x] + \\epsilon}}*\\gamma$$\n",
    "\n",
    "The mean-squre are calculated over the last $D$ dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d0409d3-bb27-44da-bcef-126883c9bf43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RMSNorm(torch.nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (N, L, C)\n",
    "        # weight shape: (C,)\n",
    "        output = self._norm(x.float()).type_as(x)\n",
    "        return output * self.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfd340a-53b2-42c2-ab80-34553c2d4a1b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Attention\n",
    "\n",
    "![](../images/llama3-attention.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d73222a-ee76-403f-b703-d3a47e399b35",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### RoPE\n",
    "\n",
    "Given a position index $m\\in[0,c)$ and an embedding vector $\\mathbf{x} = [x_0,x_1,\\dots,x_{d-1}]^{\\top}$, where $d$ is the dimension of the attention head, RoPE defines a vector-valued complex function $\\mathbf{f}(\\mathbf{x}, m)$ as follows:\n",
    "\n",
    "$$\\mathbf{f}(\\mathbf{x}, m) = [(x_0 + ix_{1})e^{im\\theta_0},(x_2 + ix_{3})e^{im\\theta_1},\\dots,(x_{d-2} + ix_{d-1})e^{im\\theta_{d/2-1}}]$$\n",
    "\n",
    "where $\\theta_{j}=\\theta^{-2j/d}$ ($\\theta$ is a hyper-parameter). Using RoPE, the self-attention score\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\text{Re}\\left \\langle \\mathbf{f}(\\mathbf{q}, m), \\mathbf{f}(\\mathbf{k}, n)  \\right \\rangle &= \\text{Re}\\left[\\sum_{j=0}^{d/2-1}(q_{2j} + iq_{2j+1})(k_{2j} - ik_{2j+1})e^{i(m-n)\\theta_{j}}\\right]\\\\\n",
    "&= \\sum_{j=0}^{d/2-1}(q_{2j}k_{2j} + q_{2j+1}k_{2j+1})\\cos((m-n)\\theta_{j}) + (q_{2j}k_{2j+1} - q_{2j+1}k_{2j})\\sin((m-n)\\theta_{j})\\\\\n",
    "&= a(m-n)\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "is only dependent on relative position $m-n$ through trigonometric functions.\n",
    "\n",
    "$$\\text{RoPE}(\\mathbf{x}, m) = \\left[\\text{Re}((x_0 + ix_{1})e^{im\\theta_0}), \\text{Im}((x_0 + ix_{1})e^{im\\theta_0}), \\text{Re}((x_2 + ix_{3})e^{im\\theta_1}), \\text{Im}((x_2 + ix_{3})e^{im\\theta_1}), ...\\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96f261b8-0df4-4629-8bab-9e5ba93bbc24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Tuple, Optional\n",
    "\n",
    "\n",
    "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n",
    "    # theta_j\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "    # 0,1,...,c-1\n",
    "    t = torch.arange(end, device=freqs.device, dtype=torch.float32)\n",
    "    # m * theta_j\n",
    "    freqs = torch.outer(t, freqs)\n",
    "    # exp ** (i*m*theta_j)\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    return freqs_cis\n",
    "\n",
    "\n",
    "def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):\n",
    "    ndim = x.ndim\n",
    "    assert 1 < ndim\n",
    "    assert freqs_cis.shape == (x.shape[1], x.shape[-1])\n",
    "    # (seqlen, head_dim // 2) -> (1, seqlen, 1, head_dim // 2)\n",
    "    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n",
    "    return freqs_cis.view(*shape)\n",
    "\n",
    "\n",
    "def apply_rotary_emb(\n",
    "    xq: torch.Tensor,\n",
    "    xk: torch.Tensor,\n",
    "    freqs_cis: torch.Tensor,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    # view_as_complex: [[q_0, q_1], [q_2, q_3],...] -> [q_0 + i*q_1, q_2 + i*q_3, ...]\n",
    "    # (bsz, seqlen, n_heads, head_dim) -> (bsz, seqlen, n_heads, head_dim // 2)\n",
    "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
    "    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
    "    freqs_cis = reshape_for_broadcast(freqs_cis, xq_)\n",
    "    # (bsz, seqlen, n_heads, head_dim // 2) ->\n",
    "    # (bsz, seqlen, n_heads, head_dim // 2, 2) ->\n",
    "    # (bsz, seqlen, n_heads, head_dim)\n",
    "    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)\n",
    "    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)\n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5d6712-88a4-4201-b4ff-4715a5b3b536",
   "metadata": {},
   "source": [
    "#### Attention\n",
    "\n",
    "![](../images/llama3-qkv.svg)\n",
    "\n",
    "```{note}\n",
    "We do not need to cache previous queries.<br>\n",
    "Attention is the only place that one position interact with other positions.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "686d0c55-ba36-4a73-81db-2d0ca9b81e84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class ModelArgs:\n",
    "    dim: int = 4096\n",
    "    n_layers: int = 32\n",
    "    n_heads: int = 32\n",
    "    vocab_size: int = -1\n",
    "    multiple_of: int = 256  # make SwiGLU hidden layer size multiple of large power of 2\n",
    "    norm_eps: float = 1e-5\n",
    "    rope_theta: float = 500000\n",
    "\n",
    "    max_batch_size: int = 32\n",
    "    max_seq_len: int = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ee5a118-3de6-4bf1-b97c-bddfdf0d1910",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.n_heads = args.n_heads\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "        \n",
    "        # Simple linear transformations for query, key, value\n",
    "        self.wq = nn.Linear(args.dim, args.n_heads * self.head_dim, bias=False)\n",
    "        self.wk = nn.Linear(args.dim, args.n_heads * self.head_dim, bias=False)\n",
    "        self.wv = nn.Linear(args.dim, args.n_heads * self.head_dim, bias=False)\n",
    "        # Output linear transformation\n",
    "        self.wo = nn.Linear(args.n_heads * self.head_dim, args.dim, bias=False)\n",
    "        \n",
    "        # Cache\n",
    "        self.cache_k = torch.zeros((args.max_batch_size, args.max_seq_len, self.n_heads, self.head_dim))\n",
    "        self.cache_v = torch.zeros((args.max_batch_size, args.max_seq_len, self.n_heads, self.head_dim))\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        start_pos: int,\n",
    "        freqs_cis: torch.Tensor,\n",
    "        mask: Optional[torch.Tensor],\n",
    "    ):\n",
    "        bsz, seqlen, _ = x.shape\n",
    "        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)\n",
    "        \n",
    "        # Reshape for multi-head attention computation\n",
    "        xq = xq.view(bsz, seqlen, self.n_heads, self.head_dim)\n",
    "        xk = xk.view(bsz, seqlen, self.n_heads, self.head_dim)\n",
    "        xv = xv.view(bsz, seqlen, self.n_heads, self.head_dim)\n",
    "        \n",
    "        # RoPE\n",
    "        xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis)\n",
    "        \n",
    "        self.cache_k = self.cache_k.to(xq)\n",
    "        self.cache_v = self.cache_v.to(xq)\n",
    "        \n",
    "        self.cache_k[:bsz, start_pos : start_pos + seqlen] = xk\n",
    "        self.cache_v[:bsz, start_pos : start_pos + seqlen] = xv\n",
    "        \n",
    "        # Add previous keys and values\n",
    "        keys = self.cache_k[:bsz, : start_pos + seqlen]\n",
    "        values = self.cache_v[:bsz, : start_pos + seqlen]\n",
    "        \n",
    "        # Transpose for matrix multiplication\n",
    "        xq = xq.transpose(1, 2)          # (bs, n_heads, seqlen, head_dim)\n",
    "        keys = keys.transpose(1, 2)      # (bs, n_heads, cache_len + seqlen, head_dim)\n",
    "        values = values.transpose(1, 2)  # (bs, n_heads, cache_len + seqlen, head_dim)\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        scores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "        if mask is not None:\n",
    "            scores = scores + mask  # (bs, n_heads, seqlen, cache_len + seqlen)\n",
    "        scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
    "        \n",
    "        # Compute weighted average\n",
    "        output = torch.matmul(scores, values)  # (bs, n_heads, seqlen, head_dim)\n",
    "        output = output.transpose(1, 2).contiguous().view(bsz, seqlen, -1)  # (bs, seqlen, n_heads * head_dim)\n",
    "        \n",
    "        return self.wo(output)  # (bs, seqlen, dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e75ea2-fba8-4bda-9a8c-639fe650446f",
   "metadata": {},
   "source": [
    "### FeedForward\n",
    "\n",
    "The \"position-wise feed-forward networks\" (FFN) takes a vector $x$ (the hidden representation at a particular position in the sequence) and passes it through two learned linear transformations, (represented by the matrices $W_{1}$ and $W_{2}$ and bias vectors $b_{1}$ and $b_{2}$). A rectified-linear (ReLU) activation function applied between the two linear transformations.\n",
    "\n",
    "$$\\text{FFN}(x, W_{1}, W_{2}, b_{1}, b_{2}) = \\max(0, xW_{1}+b_{1})W_{2} + b_{2}$$\n",
    "\n",
    "If we use a version with no bias:\n",
    "\n",
    "$$\\text{FFN}_{\\text{ReLU}}(x, W_{1}, W_{2}) = \\max(0, xW_{1})W_{2}$$\n",
    "\n",
    "Subsequent work has proposed replacing the ReLU with other nonlinear activation functions such as $\\text{Swish} = x\\sigma(x)$ (also known as SiLU):\n",
    "\n",
    "$$\n",
    "\\text{FFN}_{\\text{Swish}}(x, W_{1}, W_{2}) = \\text{Swish}(xW_{1})W_{2}\n",
    "$$\n",
    "\n",
    "![](../images/swish.svg)\n",
    "\n",
    "#### SwiGLU activation function\n",
    "\n",
    "GLU is a neural network layer defined as the component-wise product of two linear transformations of the input, one of which is sigmoid-activated.\n",
    "\n",
    "$$\\text{GLU}(x, W, V, b, c) = \\sigma(xW + b)\\otimes(xV+c)$$\n",
    "\n",
    "We propose additional variations on the Transformer FFN layer which use GLU or one of\n",
    "its variants in place of the first linear transformation and the activation function:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{FFN}_{\\text{GLU}}(x, W, V, W_{2}) &= (\\sigma(xW + b)\\otimes(xV+c))W_{2}\\\\\n",
    "\\text{FFN}_{\\text{ReGLU}}(x, W, V, W_{2}) &= (\\max(0, xW + b)\\otimes(xV+c))W_{2}\\\\\n",
    "\\text{FFN}_{\\text{SwiGLU}}(x, W, V, W_{2}) &= (\\text{Swish}_{1}(xW + b)\\otimes(xV+c))W_{2}\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c3656bc-4585-4726-a60f-4a39fdbf3b95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        hidden_dim: int,\n",
    "        multiple_of: int\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # use a dimension of 2/3*4d instead of 4d as in PaLM\n",
    "        hidden_dim = int(2 * hidden_dim / 3)\n",
    "        # make SwiGLU hidden layer size multiple of large power of 2\n",
    "        hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n",
    "        \n",
    "        self.w1 = nn.Linear(dim, hidden_dim, bias=False)\n",
    "        self.w2 = nn.Linear(hidden_dim, dim, bias=False)\n",
    "        self.w3 = nn.Linear(dim, hidden_dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w2(F.silu(self.w1(x)) * self.w3(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19b6776-e4a2-41d6-a1af-4227e3e95946",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2454b726-07f7-48e3-a4ff-12b5449a059e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, layer_id: int, args: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.n_heads = args.n_heads\n",
    "        self.dim = args.dim\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "        self.attention = Attention(args)\n",
    "        self.feed_forward = FeedForward(\n",
    "            dim=args.dim,\n",
    "            hidden_dim=4 * args.dim,\n",
    "            multiple_of=args.multiple_of\n",
    "        )\n",
    "        self.layer_id = layer_id\n",
    "        self.attention_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
    "        self.ffn_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        start_pos: int,\n",
    "        freqs_cis: torch.Tensor,\n",
    "        mask: Optional[torch.Tensor],\n",
    "    ):\n",
    "        h = x + self.attention(self.attention_norm(x), start_pos, freqs_cis, mask)\n",
    "        out = h + self.feed_forward(self.ffn_norm(h))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a623db7-cae1-41a1-b147-e831e5e9ac2b",
   "metadata": {},
   "source": [
    "Mask illustration:\n",
    "\n",
    "![](../images/llama3-mask.svg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c816c92-0c8c-455b-abfb-4fd5b063ea92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, params: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.params = params\n",
    "        self.vocab_size = params.vocab_size\n",
    "        self.n_layers = params.n_layers\n",
    "        \n",
    "        self.tok_embeddings = nn.Embedding(params.vocab_size, params.dim)\n",
    "\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        for layer_id in range(params.n_layers):\n",
    "            self.layers.append(TransformerBlock(layer_id, params))\n",
    "\n",
    "        self.norm = RMSNorm(params.dim, eps=params.norm_eps)\n",
    "        self.output = nn.Linear(params.dim, params.vocab_size, bias=False)\n",
    "\n",
    "        # end = 2 * max_seq_len\n",
    "        self.freqs_cis = precompute_freqs_cis(\n",
    "            params.dim // params.n_heads,\n",
    "            params.max_seq_len * 2,\n",
    "            params.rope_theta,\n",
    "        )\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def forward(self, tokens: torch.Tensor, start_pos: int):\n",
    "        _bsz, seqlen = tokens.shape\n",
    "        h = self.tok_embeddings(tokens)\n",
    "        self.freqs_cis = self.freqs_cis.to(h.device)\n",
    "        freqs_cis = self.freqs_cis[start_pos : start_pos + seqlen]\n",
    "\n",
    "        mask = None\n",
    "        if seqlen > 1:\n",
    "            mask = torch.full((seqlen, seqlen), float(\"-inf\"), device=tokens.device)\n",
    "\n",
    "            mask = torch.triu(mask, diagonal=1)\n",
    "\n",
    "            # When performing key-value caching, we compute the attention scores\n",
    "            # only for the new sequence. Thus, the matrix of scores is of size\n",
    "            # (seqlen, cache_len + seqlen), and the only masked entries are (i, j) for\n",
    "            # j > cache_len + i, since row i corresponds to token cache_len + i.\n",
    "            mask = torch.hstack(\n",
    "                [torch.zeros((seqlen, start_pos), device=tokens.device), mask]\n",
    "            ).type_as(h)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            h = layer(h, start_pos, freqs_cis, mask)\n",
    "        h = self.norm(h)\n",
    "        output = self.output(h).float()\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26878938-04ad-4f58-b831-b4b4cab46b4b",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "\n",
    "### Tiktoken\n",
    "\n",
    "tiktoken is a fast open-source tokenizer by OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e10a6d0-9ae8-4cfe-b524-d1a0390833fa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tiktoken'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtiktoken\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtiktoken\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mload\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_tiktoken_bpe\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Loading an encoding\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tiktoken'"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "from tiktoken.load import load_tiktoken_bpe\n",
    "\n",
    "# Loading an encoding\n",
    "encoding = tiktoken.encoding_for_model('gpt-3.5-turbo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4754b6-d47b-4bca-b630-60df5bc9b1d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "encoding.encode(\"tiktoken is great!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64450a8-cb1b-4afb-8c90-4da368e7f3be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "encoding.decode([83, 1609, 5963, 374, 2294, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6503c4-d202-498b-b28e-ac3d810958f7",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c42bc4-5b06-4057-a602-6bc61de7ca78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import (\n",
    "    AbstractSet,\n",
    "    cast,\n",
    "    Collection,\n",
    "    Dict,\n",
    "    Iterator,\n",
    "    List,\n",
    "    Literal,\n",
    "    Sequence,\n",
    "    TypedDict,\n",
    "    Union,\n",
    ")\n",
    "\n",
    "Role = Literal[\"system\", \"user\", \"assistant\"]\n",
    "\n",
    "\n",
    "class Message(TypedDict):\n",
    "    role: Role\n",
    "    content: str\n",
    "    \n",
    "\n",
    "Dialog = Sequence[Message]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478bc10f-f35f-44af-ba07-0d5b19ca4472",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    \"\"\"\n",
    "    Tokenizing and encoding/decoding text using the Tiktoken tokenizer.\n",
    "    \"\"\"\n",
    "\n",
    "    special_tokens: Dict[str, int]\n",
    "\n",
    "    num_reserved_special_tokens = 256\n",
    "\n",
    "    # A regex pattern string that is used to split the input text.\n",
    "    pat_str = r\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\"  # noqa: E501\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the Tokenizer with a Tiktoken model.\n",
    "        \"\"\"\n",
    "        # Simplified\n",
    "        mergeable_ranks = tiktoken.encoding_for_model('gpt-3.5-turbo')._mergeable_ranks\n",
    "        num_base_tokens = len(mergeable_ranks)\n",
    "        special_tokens = [\n",
    "            \"<|begin_of_text|>\",\n",
    "            \"<|end_of_text|>\",\n",
    "            \"<|reserved_special_token_0|>\",\n",
    "            \"<|reserved_special_token_1|>\",\n",
    "            \"<|reserved_special_token_2|>\",\n",
    "            \"<|reserved_special_token_3|>\",\n",
    "            \"<|start_header_id|>\",\n",
    "            \"<|end_header_id|>\",\n",
    "            \"<|reserved_special_token_4|>\",\n",
    "            \"<|eot_id|>\",  # end of turn\n",
    "        ] + [\n",
    "            f\"<|reserved_special_token_{i}|>\"\n",
    "            for i in range(5, self.num_reserved_special_tokens - 5)\n",
    "        ]\n",
    "        self.special_tokens = {\n",
    "            token: num_base_tokens + i for i, token in enumerate(special_tokens)\n",
    "        }\n",
    "        self.model = tiktoken.Encoding(\n",
    "            name=\"gpt-3.5-turbo-with-llama3-special\",\n",
    "            pat_str=self.pat_str,\n",
    "            mergeable_ranks=mergeable_ranks,\n",
    "            special_tokens=self.special_tokens,\n",
    "        )\n",
    "\n",
    "        self.n_words: int = self.model.n_vocab\n",
    "        # BOS / EOS token IDs\n",
    "        self.bos_id: int = self.special_tokens[\"<|begin_of_text|>\"]\n",
    "        self.eos_id: int = self.special_tokens[\"<|end_of_text|>\"]\n",
    "        self.pad_id: int = -1\n",
    "        self.stop_tokens = {\n",
    "            self.special_tokens[\"<|end_of_text|>\"],\n",
    "            self.special_tokens[\"<|eot_id|>\"],\n",
    "        }\n",
    "\n",
    "    def encode(\n",
    "        self,\n",
    "        s: str,\n",
    "        *,\n",
    "        bos: bool,\n",
    "        eos: bool,\n",
    "        allowed_special: Union[Literal[\"all\"], AbstractSet[str]] = set(),\n",
    "        disallowed_special: Union[Literal[\"all\"], Collection[str]] = (),\n",
    "    ) -> List[int]:\n",
    "        \"\"\"\n",
    "        Encodes a string into a list of token IDs.\n",
    "\n",
    "        Args:\n",
    "            s (str): The input string to be encoded.\n",
    "            bos (bool): Whether to prepend the beginning-of-sequence token.\n",
    "            eos (bool): Whether to append the end-of-sequence token.\n",
    "            allowed_tokens (\"all\"|set[str]): allowed special tokens in string\n",
    "            disallowed_tokens (\"all\"|set[str]): special tokens that raise an error when in string\n",
    "\n",
    "        Returns:\n",
    "            list[int]: A list of token IDs.\n",
    "\n",
    "        By default, setting disallowed_special=() encodes a string by ignoring\n",
    "        special tokens. Specifically:\n",
    "        - Setting `disallowed_special` to () will cause all text corresponding\n",
    "          to special tokens to be encoded as natural text (insteading of raising\n",
    "          an error).\n",
    "        - Setting `allowed_special` to \"all\" will treat all text corresponding\n",
    "          to special tokens to be encoded as special tokens.\n",
    "        \"\"\"\n",
    "        # Simplified\n",
    "        t = self.model.encode(s, allowed_special=allowed_special, disallowed_special=disallowed_special)\n",
    "        if bos:\n",
    "            t.insert(0, self.bos_id)\n",
    "        if eos:\n",
    "            t.append(self.eos_id)\n",
    "        return t\n",
    "\n",
    "    def decode(self, t: Sequence[int]) -> str:\n",
    "        \"\"\"\n",
    "        Decodes a list of token IDs into a string.\n",
    "\n",
    "        Args:\n",
    "            t (List[int]): The list of token IDs to be decoded.\n",
    "\n",
    "        Returns:\n",
    "            str: The decoded string.\n",
    "        \"\"\"\n",
    "        return self.model.decode(cast(List[int], t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94198cd-3139-4ee3-8ba5-76adda73ddb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "print(tokenizer.encode('Decodes a list of token IDs into a string.', bos=False, eos=False))\n",
    "print(tokenizer.decode(tokenizer.encode('Decodes a list of token IDs into a string.', bos=False, eos=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb35ebe-3735-47c8-9d91-2e0815a1c323",
   "metadata": {},
   "source": [
    "### ChatFormat\n",
    "\n",
    "![](../images/llama3-tokenizer.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049984ae-29ed-44f2-8449-ee7172be0b09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ChatFormat:\n",
    "    def __init__(self, tokenizer: Tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def encode_header(self, message: Message) -> List[int]:\n",
    "        tokens = []\n",
    "        tokens.append(self.tokenizer.special_tokens[\"<|start_header_id|>\"])\n",
    "        tokens.extend(self.tokenizer.encode(message[\"role\"], bos=False, eos=False))\n",
    "        tokens.append(self.tokenizer.special_tokens[\"<|end_header_id|>\"])\n",
    "        tokens.extend(self.tokenizer.encode(\"\\n\\n\", bos=False, eos=False))\n",
    "        return tokens\n",
    "\n",
    "    def encode_message(self, message: Message) -> List[int]:\n",
    "        tokens = self.encode_header(message)\n",
    "        tokens.extend(\n",
    "            self.tokenizer.encode(message[\"content\"].strip(), bos=False, eos=False)\n",
    "        )\n",
    "        tokens.append(self.tokenizer.special_tokens[\"<|eot_id|>\"])\n",
    "        return tokens\n",
    "\n",
    "    def encode_dialog_prompt(self, dialog: Dialog) -> List[int]:\n",
    "        tokens = []\n",
    "        tokens.append(self.tokenizer.special_tokens[\"<|begin_of_text|>\"])\n",
    "        for message in dialog:\n",
    "            tokens.extend(self.encode_message(message))\n",
    "        # Add the start of an assistant message for the model to complete.\n",
    "        tokens.extend(self.encode_header({\"role\": \"assistant\", \"content\": \"\"}))\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671d9ea7-4117-4523-9b8b-3f223218f401",
   "metadata": {},
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38c7438-a6cb-4d4c-83cb-6a722f91ef9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sample_top_p(probs, p):\n",
    "    \"\"\"\n",
    "    Perform top-p (nucleus) sampling on a probability distribution.\n",
    "\n",
    "    Args:\n",
    "        probs (torch.Tensor): Probability distribution tensor.\n",
    "        p (float): Probability threshold for top-p sampling.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Sampled token indices.\n",
    "\n",
    "    Note:\n",
    "        Top-p sampling selects the smallest set of tokens whose cumulative probability mass\n",
    "        exceeds the threshold p. The distribution is renormalized based on the selected tokens.\n",
    "    \"\"\"\n",
    "    probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)\n",
    "    probs_sum = torch.cumsum(probs_sort, dim=-1)\n",
    "    mask = probs_sum - probs_sort > p\n",
    "    probs_sort[mask] = 0.0\n",
    "    probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))\n",
    "    next_token = torch.multinomial(probs_sort, num_samples=1)\n",
    "    next_token = torch.gather(probs_idx, -1, next_token)\n",
    "    return next_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b85319-173a-495b-b5ba-aae0218f8095",
   "metadata": {},
   "source": [
    "![](../images/llama3-generate.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbde38c-078a-4bde-9a84-b4162413c378",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Llama:\n",
    "    @staticmethod\n",
    "    def build(\n",
    "        ckpt_dir: str,\n",
    "        max_seq_len: int,\n",
    "        max_batch_size: int,\n",
    "    ) -> \"Llama\":\n",
    "        \"\"\"\n",
    "        Build a Llama instance by initializing and loading a model checkpoint.\n",
    "\n",
    "        Args:\n",
    "            ckpt_dir (str): Path to the directory containing checkpoint files.\n",
    "            max_seq_len (int): Maximum sequence length for input text.\n",
    "            max_batch_size (int): Maximum batch size for inference.\n",
    "\n",
    "        Returns:\n",
    "            Llama: An instance of the Llama class with the loaded model and tokenizer.\n",
    "\n",
    "        Raises:\n",
    "            AssertionError: If there are no checkpoint files in the specified directory,\n",
    "                or if the model parallel size does not match the number of checkpoint files.\n",
    "\n",
    "        Note:\n",
    "            This method initializes the distributed process group, sets the device to CUDA,\n",
    "            and loads the pre-trained model and tokenizer.\n",
    "        \"\"\"\n",
    "        ckpt_path = sorted(Path(ckpt_dir).glob(\"*.pth\"))[0]\n",
    "        checkpoint = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "        with open(Path(ckpt_dir) / \"params.json\", \"r\") as f:\n",
    "            params = json.loads(f.read())\n",
    "\n",
    "        model_args: ModelArgs = ModelArgs(\n",
    "            max_seq_len=max_seq_len,\n",
    "            max_batch_size=max_batch_size,\n",
    "            **params,\n",
    "        )\n",
    "        tokenizer = Tokenizer()\n",
    "        assert model_args.vocab_size == tokenizer.n_words\n",
    "        model = Transformer(model_args)\n",
    "        model.load_state_dict(checkpoint, strict=False)\n",
    "\n",
    "        return Llama(model, tokenizer)\n",
    "\n",
    "    def __init__(self, model: Transformer, tokenizer: Tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.formatter = ChatFormat(tokenizer)\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def generate(\n",
    "        self,\n",
    "        prompt_tokens: List[List[int]],\n",
    "        max_gen_len: int,\n",
    "        temperature: float = 0.6,\n",
    "        top_p: float = 0.9,\n",
    "        logprobs: bool = False,\n",
    "        echo: bool = False,\n",
    "    ) -> Tuple[List[List[int]], Optional[List[List[float]]]]:\n",
    "        \"\"\"\n",
    "        Generate text sequences based on provided prompts using the language generation model.\n",
    "\n",
    "        Args:\n",
    "            prompt_tokens (List[List[int]]): List of tokenized prompts, where each prompt is represented as a list of integers.\n",
    "            max_gen_len (int): Maximum length of the generated text sequence.\n",
    "            temperature (float, optional): Temperature value for controlling randomness in sampling. Defaults to 0.6.\n",
    "            top_p (float, optional): Top-p probability threshold for nucleus sampling. Defaults to 0.9.\n",
    "            logprobs (bool, optional): Flag indicating whether to compute token log probabilities. Defaults to False.\n",
    "            echo (bool, optional): Flag indicating whether to include prompt tokens in the generated output. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[List[List[int]], Optional[List[List[float]]]]: A tuple containing generated token sequences and, if logprobs is True, corresponding token log probabilities.\n",
    "\n",
    "        Note:\n",
    "            This method uses the provided prompts as a basis for generating text. It employs nucleus sampling to produce text with controlled randomness.\n",
    "            If logprobs is True, token log probabilities are computed for each generated token.\n",
    "\n",
    "        \"\"\"\n",
    "        params = self.model.params\n",
    "        bsz = len(prompt_tokens)\n",
    "        assert bsz <= params.max_batch_size, (bsz, params.max_batch_size)\n",
    "\n",
    "        min_prompt_len = min(len(t) for t in prompt_tokens)\n",
    "        max_prompt_len = max(len(t) for t in prompt_tokens)\n",
    "        assert max_prompt_len <= params.max_seq_len\n",
    "        total_len = min(params.max_seq_len, max_gen_len + max_prompt_len)\n",
    "\n",
    "        pad_id = self.tokenizer.pad_id\n",
    "        tokens = torch.full((bsz, total_len), pad_id, dtype=torch.long, device=\"cuda\")\n",
    "        for k, t in enumerate(prompt_tokens):\n",
    "            tokens[k, : len(t)] = torch.tensor(t, dtype=torch.long, device=\"cuda\")\n",
    "        if logprobs:\n",
    "            token_logprobs = torch.zeros_like(tokens, dtype=torch.float)\n",
    "\n",
    "        prev_pos = 0\n",
    "        eos_reached = torch.tensor([False] * bsz, device=\"cuda\")\n",
    "        input_text_mask = tokens != pad_id\n",
    "        if min_prompt_len == total_len:  # no need to generate\n",
    "            logits = self.model.forward(tokens, prev_pos)\n",
    "            token_logprobs = -F.cross_entropy(\n",
    "                input=logits.transpose(1, 2),\n",
    "                target=tokens,\n",
    "                reduction=\"none\",\n",
    "                ignore_index=pad_id,\n",
    "            )\n",
    "            \n",
    "        stop_tokens = torch.tensor(list(self.tokenizer.stop_tokens))\n",
    "\n",
    "        for cur_pos in range(min_prompt_len, total_len):\n",
    "            logits = self.model.forward(tokens[:, prev_pos:cur_pos], prev_pos)\n",
    "            if temperature > 0:\n",
    "                probs = torch.softmax(logits[:, -1] / temperature, dim=-1)\n",
    "                next_token = sample_top_p(probs, top_p)\n",
    "            else:\n",
    "                next_token = torch.argmax(logits[:, -1], dim=-1)  # if temperature <= 0 then greedy\n",
    "\n",
    "            next_token = next_token.reshape(-1)\n",
    "            # only replace token if prompt has already been generated\n",
    "            next_token = torch.where(\n",
    "                input_text_mask[:, cur_pos], tokens[:, cur_pos], next_token\n",
    "            )\n",
    "            tokens[:, cur_pos] = next_token\n",
    "            if logprobs:\n",
    "                token_logprobs[:, prev_pos + 1 : cur_pos + 1] = -F.cross_entropy(\n",
    "                    input=logits.transpose(1, 2),\n",
    "                    target=tokens[:, prev_pos + 1 : cur_pos + 1],\n",
    "                    reduction=\"none\",\n",
    "                    ignore_index=pad_id,\n",
    "                )\n",
    "            eos_reached |= (~input_text_mask[:, cur_pos]) & (\n",
    "                torch.isin(next_token, stop_tokens)\n",
    "            )\n",
    "            prev_pos = cur_pos\n",
    "            if all(eos_reached):\n",
    "                break\n",
    "\n",
    "        if logprobs:\n",
    "            token_logprobs = token_logprobs.tolist()\n",
    "        out_tokens, out_logprobs = [], []\n",
    "        for i, toks in enumerate(tokens.tolist()):\n",
    "            # cut to max gen len\n",
    "            start = 0 if echo else len(prompt_tokens[i])\n",
    "            toks = toks[start : len(prompt_tokens[i]) + max_gen_len]\n",
    "            probs = None\n",
    "            if logprobs:\n",
    "                probs = token_logprobs[i][start : len(prompt_tokens[i]) + max_gen_len]\n",
    "            # cut to after eos tok if any\n",
    "            for stop_token in self.tokenizer.stop_tokens:\n",
    "                try:\n",
    "                    eos_idx = toks.index(stop_token)\n",
    "                    toks = toks[:eos_idx]\n",
    "                    probs = probs[:eos_idx] if logprobs else None\n",
    "                except ValueError:\n",
    "                    pass\n",
    "            out_tokens.append(toks)\n",
    "            out_logprobs.append(probs)\n",
    "        return (out_tokens, out_logprobs if logprobs else None)\n",
    "\n",
    "    def text_completion(\n",
    "        self,\n",
    "        prompts: List[str],\n",
    "        temperature: float = 0.6,\n",
    "        top_p: float = 0.9,\n",
    "        max_gen_len: Optional[int] = None,\n",
    "        logprobs: bool = False,\n",
    "        echo: bool = False,\n",
    "    ) -> List:\n",
    "        \"\"\"\n",
    "        Perform text completion for a list of prompts using the language generation model.\n",
    "\n",
    "        Args:\n",
    "            prompts (List[str]): List of text prompts for completion.\n",
    "            temperature (float, optional): Temperature value for controlling randomness in sampling. Defaults to 0.6.\n",
    "            top_p (float, optional): Top-p probability threshold for nucleus sampling. Defaults to 0.9.\n",
    "            max_gen_len (Optional[int], optional): Maximum length of the generated completion sequence.\n",
    "                If not provided, it's set to the model's maximum sequence length minus 1.\n",
    "            logprobs (bool, optional): Flag indicating whether to compute token log probabilities. Defaults to False.\n",
    "            echo (bool, optional): Flag indicating whether to include prompt tokens in the generated output. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            List: List of completion predictions, each containing the generated text completion.\n",
    "\n",
    "        Note:\n",
    "            This method generates text completions for the provided prompts, employing nucleus sampling to introduce controlled randomness.\n",
    "            If logprobs is True, token log probabilities are computed for each generated token.\n",
    "\n",
    "        \"\"\"\n",
    "        if max_gen_len is None:\n",
    "            max_gen_len = self.model.params.max_seq_len - 1\n",
    "        prompt_tokens = [self.tokenizer.encode(x, bos=True, eos=False) for x in prompts]\n",
    "        generation_tokens, generation_logprobs = self.generate(\n",
    "            prompt_tokens=prompt_tokens,\n",
    "            max_gen_len=max_gen_len,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            logprobs=logprobs,\n",
    "            echo=echo,\n",
    "        )\n",
    "        if logprobs:\n",
    "            return [\n",
    "                {\n",
    "                    \"generation\": self.tokenizer.decode(t),\n",
    "                    \"tokens\": [self.tokenizer.decode([x]) for x in t],\n",
    "                    \"logprobs\": logprobs_i,\n",
    "                }\n",
    "                for t, logprobs_i in zip(generation_tokens, generation_logprobs)\n",
    "            ]\n",
    "        return [{\"generation\": self.tokenizer.decode(t)} for t in generation_tokens]\n",
    "\n",
    "    def chat_completion(\n",
    "        self,\n",
    "        dialogs: List[Dialog],\n",
    "        temperature: float = 0.6,\n",
    "        top_p: float = 0.9,\n",
    "        max_gen_len: Optional[int] = None,\n",
    "        logprobs: bool = False,\n",
    "    ) -> List:\n",
    "        \"\"\"\n",
    "        Generate assistant responses for a list of conversational dialogs using the language generation model.\n",
    "\n",
    "        Args:\n",
    "            dialogs (List[Dialog]): List of conversational dialogs, where each dialog is a list of messages.\n",
    "            temperature (float, optional): Temperature value for controlling randomness in sampling. Defaults to 0.6.\n",
    "            top_p (float, optional): Top-p probability threshold for nucleus sampling. Defaults to 0.9.\n",
    "            max_gen_len (Optional[int], optional): Maximum length of the generated response sequence.\n",
    "                If not provided, it's set to the model's maximum sequence length minus 1.\n",
    "            logprobs (bool, optional): Flag indicating whether to compute token log probabilities. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            List: List of chat predictions, each containing the assistant's generated response.\n",
    "\n",
    "        Note:\n",
    "            This method generates assistant responses for the provided conversational dialogs.\n",
    "            It employs nucleus sampling to introduce controlled randomness in text generation.\n",
    "            If logprobs is True, token log probabilities are computed for each generated token.\n",
    "        \"\"\"\n",
    "        if max_gen_len is None:\n",
    "            max_gen_len = self.model.params.max_seq_len - 1\n",
    "\n",
    "        prompt_tokens = [\n",
    "            self.formatter.encode_dialog_prompt(dialog) for dialog in dialogs\n",
    "        ]\n",
    "        generation_tokens, generation_logprobs = self.generate(\n",
    "            prompt_tokens=prompt_tokens,\n",
    "            max_gen_len=max_gen_len,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            logprobs=logprobs,\n",
    "        )\n",
    "        if logprobs:\n",
    "            return [\n",
    "                {\n",
    "                    \"generation\": {\n",
    "                        \"role\": \"assistant\",\n",
    "                        \"content\": self.tokenizer.decode(t),\n",
    "                    },\n",
    "                    \"tokens\": [self.tokenizer.decode([x]) for x in t],\n",
    "                    \"logprobs\": logprobs_i,\n",
    "                }\n",
    "                for t, logprobs_i in zip(generation_tokens, generation_logprobs)\n",
    "            ]\n",
    "        return [\n",
    "            {\n",
    "                \"generation\": {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": self.tokenizer.decode(t),\n",
    "                },\n",
    "            }\n",
    "            for t in generation_tokens\n",
    "        ]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}