{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa381b5f-36bc-4746-938f-e34d773ae89a",
   "metadata": {},
   "source": [
    "# Llama 3 Source Code\n",
    "\n",
    "```{note}\n",
    "See [github](https://github.com/meta-llama/llama3) for more details.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87177700-0831-411b-b4ea-bc8dd7b210a0",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Overview of Llama3 model:\n",
    "\n",
    "```{figure} ../images/llama3_new.svg\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa4b6a4-a2c8-4323-958e-2209e8e0aecf",
   "metadata": {},
   "source": [
    "### RMSNorm\n",
    "\n",
    "$$y = \\frac{x}{\\sqrt{\\mathbf{MeanSquare}[x] + \\epsilon}}*\\gamma$$\n",
    "\n",
    "$\\mathbf{MeanSquare}[x]$ is calculated over $C$ and is a vector of size $(N, L)$.\n",
    "\n",
    "```{note}\n",
    "See [](normalization) for comparisons of different normalization.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8876b6f2-a48e-49ce-9287-188206be8fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class RMSNorm(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (N, L, C)\n",
    "        # weight shape: (C,)\n",
    "        output = self._norm(x.float()).type_as(x)\n",
    "        return output * self.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfd340a-53b2-42c2-ab80-34553c2d4a1b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Attention\n",
    "\n",
    "```{figure} ../images/llama3-attention.svg\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d73222a-ee76-403f-b703-d3a47e399b35",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### RoPE\n",
    "\n",
    "Given a position index $m\\in[0,c)$ and an embedding vector $\\mathbf{x} = [x_0,x_1,\\dots,x_{d-1}]^{\\top}$, where $d$ is the dimension of the attention head, RoPE defines a vector-valued complex function $\\mathbf{f}(\\mathbf{x}, m)$ as follows:\n",
    "\n",
    "$$\\mathbf{f}(\\mathbf{x}, m) = [(x_0 + ix_{1})e^{im\\theta_0},(x_2 + ix_{3})e^{im\\theta_1},\\dots,(x_{d-2} + ix_{d-1})e^{im\\theta_{d/2-1}}]$$\n",
    "\n",
    "where $\\theta_{j}=\\theta^{-2j/d}$ ($\\theta$ is a hyper-parameter). Using RoPE, the self-attention score\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\text{Re}\\left \\langle \\mathbf{f}(\\mathbf{q}, m), \\mathbf{f}(\\mathbf{k}, n)  \\right \\rangle &= \\text{Re}\\left[\\sum_{j=0}^{d/2-1}(q_{2j} + iq_{2j+1})(k_{2j} - ik_{2j+1})e^{i(m-n)\\theta_{j}}\\right]\\\\\n",
    "&= \\sum_{j=0}^{d/2-1}(q_{2j}k_{2j} + q_{2j+1}k_{2j+1})\\cos((m-n)\\theta_{j}) + (q_{2j}k_{2j+1} - q_{2j+1}k_{2j})\\sin((m-n)\\theta_{j})\\\\\n",
    "&= a(m-n)\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "is only dependent on relative position $m-n$ through trigonometric functions.\n",
    "\n",
    "$$\\text{RoPE}(\\mathbf{x}, m) = \\left[\\text{Re}((x_0 + ix_{1})e^{im\\theta_0}), \\text{Im}((x_0 + ix_{1})e^{im\\theta_0}), \\text{Re}((x_2 + ix_{3})e^{im\\theta_1}), \\text{Im}((x_2 + ix_{3})e^{im\\theta_1}), ...\\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96f261b8-0df4-4629-8bab-9e5ba93bbc24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Tuple, Optional\n",
    "\n",
    "\n",
    "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n",
    "    # theta_j\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "    # 0,1,...,c-1\n",
    "    t = torch.arange(end, device=freqs.device, dtype=torch.float32)\n",
    "    # m * theta_j\n",
    "    freqs = torch.outer(t, freqs)\n",
    "    # exp ** (i*m*theta_j)\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    return freqs_cis\n",
    "\n",
    "\n",
    "def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):\n",
    "    ndim = x.ndim\n",
    "    assert 1 < ndim\n",
    "    assert freqs_cis.shape == (x.shape[1], x.shape[-1])\n",
    "    # (seqlen, head_dim // 2) -> (1, seqlen, 1, head_dim // 2)\n",
    "    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n",
    "    return freqs_cis.view(*shape)\n",
    "\n",
    "\n",
    "def apply_rotary_emb(\n",
    "    xq: torch.Tensor,\n",
    "    xk: torch.Tensor,\n",
    "    freqs_cis: torch.Tensor,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    # view_as_complex: [[q_0, q_1], [q_2, q_3],...] -> [q_0 + i*q_1, q_2 + i*q_3, ...]\n",
    "    # (bsz, seqlen, n_heads, head_dim) -> (bsz, seqlen, n_heads, head_dim // 2)\n",
    "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
    "    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
    "    freqs_cis = reshape_for_broadcast(freqs_cis, xq_)\n",
    "    # (bsz, seqlen, n_heads, head_dim // 2) ->\n",
    "    # (bsz, seqlen, n_heads, head_dim // 2, 2) ->\n",
    "    # (bsz, seqlen, n_heads, head_dim)\n",
    "    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)\n",
    "    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)\n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5d6712-88a4-4201-b4ff-4715a5b3b536",
   "metadata": {},
   "source": [
    "#### Attention\n",
    "\n",
    "```{figure} ../images/llama3-qkv.svg\n",
    "```\n",
    "\n",
    "```{note}\n",
    "We do not need to cache previous queries.<br>\n",
    "Attention is the only place that one position interact with other positions.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "686d0c55-ba36-4a73-81db-2d0ca9b81e84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class ModelArgs:\n",
    "    dim: int = 4096\n",
    "    n_layers: int = 32\n",
    "    n_heads: int = 32\n",
    "    vocab_size: int = -1\n",
    "    multiple_of: int = 256  # make SwiGLU hidden layer size multiple of large power of 2\n",
    "    norm_eps: float = 1e-5\n",
    "    rope_theta: float = 500000\n",
    "\n",
    "    max_batch_size: int = 32\n",
    "    max_seq_len: int = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ee5a118-3de6-4bf1-b97c-bddfdf0d1910",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.n_heads = args.n_heads\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "        \n",
    "        # Simple linear transformations for query, key, value\n",
    "        self.wq = nn.Linear(args.dim, args.n_heads * self.head_dim, bias=False)\n",
    "        self.wk = nn.Linear(args.dim, args.n_heads * self.head_dim, bias=False)\n",
    "        self.wv = nn.Linear(args.dim, args.n_heads * self.head_dim, bias=False)\n",
    "        # Output linear transformation\n",
    "        self.wo = nn.Linear(args.n_heads * self.head_dim, args.dim, bias=False)\n",
    "        \n",
    "        # Cache\n",
    "        self.cache_k = torch.zeros((args.max_batch_size, args.max_seq_len, self.n_heads, self.head_dim))\n",
    "        self.cache_v = torch.zeros((args.max_batch_size, args.max_seq_len, self.n_heads, self.head_dim))\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        start_pos: int,\n",
    "        freqs_cis: torch.Tensor,\n",
    "        mask: Optional[torch.Tensor],\n",
    "    ):\n",
    "        bsz, seqlen, _ = x.shape\n",
    "        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)\n",
    "        \n",
    "        # Reshape for multi-head attention computation\n",
    "        xq = xq.view(bsz, seqlen, self.n_heads, self.head_dim)\n",
    "        xk = xk.view(bsz, seqlen, self.n_heads, self.head_dim)\n",
    "        xv = xv.view(bsz, seqlen, self.n_heads, self.head_dim)\n",
    "        \n",
    "        # RoPE\n",
    "        xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis)\n",
    "        \n",
    "        self.cache_k = self.cache_k.to(xq)\n",
    "        self.cache_v = self.cache_v.to(xq)\n",
    "        \n",
    "        self.cache_k[:bsz, start_pos : start_pos + seqlen] = xk\n",
    "        self.cache_v[:bsz, start_pos : start_pos + seqlen] = xv\n",
    "        \n",
    "        # Add previous keys and values\n",
    "        keys = self.cache_k[:bsz, : start_pos + seqlen]\n",
    "        values = self.cache_v[:bsz, : start_pos + seqlen]\n",
    "        \n",
    "        # Transpose for matrix multiplication\n",
    "        xq = xq.transpose(1, 2)          # (bs, n_heads, seqlen, head_dim)\n",
    "        keys = keys.transpose(1, 2)      # (bs, n_heads, cache_len + seqlen, head_dim)\n",
    "        values = values.transpose(1, 2)  # (bs, n_heads, cache_len + seqlen, head_dim)\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        scores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "        if mask is not None:\n",
    "            scores = scores + mask  # (bs, n_heads, seqlen, cache_len + seqlen)\n",
    "        scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
    "        \n",
    "        # Compute weighted average\n",
    "        output = torch.matmul(scores, values)  # (bs, n_heads, seqlen, head_dim)\n",
    "        output = output.transpose(1, 2).contiguous().view(bsz, seqlen, -1)  # (bs, seqlen, n_heads * head_dim)\n",
    "        \n",
    "        return self.wo(output)  # (bs, seqlen, dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e75ea2-fba8-4bda-9a8c-639fe650446f",
   "metadata": {},
   "source": [
    "### FeedForward\n",
    "\n",
    "The \"position-wise feed-forward networks\" (FFN) takes a vector $x$ (the hidden representation at a particular position in the sequence) and passes it through two learned linear transformations, (represented by the matrices $W_{1}$ and $W_{2}$ and bias vectors $b_{1}$ and $b_{2}$). A rectified-linear (ReLU) activation function applied between the two linear transformations.\n",
    "\n",
    "$$\\text{FFN}(x, W_{1}, W_{2}, b_{1}, b_{2}) = \\max(0, xW_{1}+b_{1})W_{2} + b_{2}$$\n",
    "\n",
    "If we use a version with no bias:\n",
    "\n",
    "$$\\text{FFN}_{\\text{ReLU}}(x, W_{1}, W_{2}) = \\max(0, xW_{1})W_{2}$$\n",
    "\n",
    "Subsequent work has proposed replacing the ReLU with other nonlinear activation functions such as $\\text{Swish} = x\\sigma(x)$ (also known as SiLU):\n",
    "\n",
    "$$\n",
    "\\text{FFN}_{\\text{Swish}}(x, W_{1}, W_{2}) = \\text{Swish}(xW_{1})W_{2}\n",
    "$$\n",
    "\n",
    "```{figure} ../images/swish.svg\n",
    "```\n",
    "\n",
    "#### SwiGLU activation function\n",
    "\n",
    "GLU is a neural network layer defined as the component-wise product of two linear transformations of the input, one of which is sigmoid-activated.\n",
    "\n",
    "$$\\text{GLU}(x, W, V, b, c) = \\sigma(xW + b)\\otimes(xV+c)$$\n",
    "\n",
    "We propose additional variations on the Transformer FFN layer which use GLU or one of\n",
    "its variants in place of the first linear transformation and the activation function:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{FFN}_{\\text{GLU}}(x, W, V, W_{2}) &= (\\sigma(xW + b)\\otimes(xV+c))W_{2}\\\\\n",
    "\\text{FFN}_{\\text{ReGLU}}(x, W, V, W_{2}) &= (\\max(0, xW + b)\\otimes(xV+c))W_{2}\\\\\n",
    "\\text{FFN}_{\\text{SwiGLU}}(x, W, V, W_{2}) &= (\\text{Swish}_{1}(xW + b)\\otimes(xV+c))W_{2}\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c3656bc-4585-4726-a60f-4a39fdbf3b95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        hidden_dim: int,\n",
    "        multiple_of: int\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # use a dimension of 2/3*4d instead of 4d as in PaLM\n",
    "        hidden_dim = int(2 * hidden_dim / 3)\n",
    "        # make SwiGLU hidden layer size multiple of large power of 2\n",
    "        hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n",
    "        \n",
    "        self.w1 = nn.Linear(dim, hidden_dim, bias=False)\n",
    "        self.w2 = nn.Linear(hidden_dim, dim, bias=False)\n",
    "        self.w3 = nn.Linear(dim, hidden_dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w2(F.silu(self.w1(x)) * self.w3(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19b6776-e4a2-41d6-a1af-4227e3e95946",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2454b726-07f7-48e3-a4ff-12b5449a059e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, layer_id: int, args: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.n_heads = args.n_heads\n",
    "        self.dim = args.dim\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "        self.attention = Attention(args)\n",
    "        self.feed_forward = FeedForward(\n",
    "            dim=args.dim,\n",
    "            hidden_dim=4 * args.dim,\n",
    "            multiple_of=args.multiple_of\n",
    "        )\n",
    "        self.layer_id = layer_id\n",
    "        self.attention_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
    "        self.ffn_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        start_pos: int,\n",
    "        freqs_cis: torch.Tensor,\n",
    "        mask: Optional[torch.Tensor],\n",
    "    ):\n",
    "        h = x + self.attention(self.attention_norm(x), start_pos, freqs_cis, mask)\n",
    "        out = h + self.feed_forward(self.ffn_norm(h))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a623db7-cae1-41a1-b147-e831e5e9ac2b",
   "metadata": {},
   "source": [
    "Mask illustration:\n",
    "\n",
    "```{figure} ../images/llama3-mask.svg\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c816c92-0c8c-455b-abfb-4fd5b063ea92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, params: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.params = params\n",
    "        self.vocab_size = params.vocab_size\n",
    "        self.n_layers = params.n_layers\n",
    "        \n",
    "        self.tok_embeddings = nn.Embedding(params.vocab_size, params.dim)\n",
    "\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        for layer_id in range(params.n_layers):\n",
    "            self.layers.append(TransformerBlock(layer_id, params))\n",
    "\n",
    "        self.norm = RMSNorm(params.dim, eps=params.norm_eps)\n",
    "        self.output = nn.Linear(params.dim, params.vocab_size, bias=False)\n",
    "\n",
    "        # end = 2 * max_seq_len\n",
    "        self.freqs_cis = precompute_freqs_cis(\n",
    "            params.dim // params.n_heads,\n",
    "            params.max_seq_len * 2,\n",
    "            params.rope_theta,\n",
    "        )\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def forward(self, tokens: torch.Tensor, start_pos: int):\n",
    "        _bsz, seqlen = tokens.shape\n",
    "        h = self.tok_embeddings(tokens)\n",
    "        self.freqs_cis = self.freqs_cis.to(h.device)\n",
    "        freqs_cis = self.freqs_cis[start_pos : start_pos + seqlen]\n",
    "\n",
    "        mask = None\n",
    "        if seqlen > 1:\n",
    "            mask = torch.full((seqlen, seqlen), float(\"-inf\"), device=tokens.device)\n",
    "\n",
    "            mask = torch.triu(mask, diagonal=1)\n",
    "\n",
    "            # When performing key-value caching, we compute the attention scores\n",
    "            # only for the new sequence. Thus, the matrix of scores is of size\n",
    "            # (seqlen, cache_len + seqlen), and the only masked entries are (i, j) for\n",
    "            # j > cache_len + i, since row i corresponds to token cache_len + i.\n",
    "            mask = torch.hstack(\n",
    "                [torch.zeros((seqlen, start_pos), device=tokens.device), mask]\n",
    "            ).type_as(h)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            h = layer(h, start_pos, freqs_cis, mask)\n",
    "        h = self.norm(h)\n",
    "        output = self.output(h).float()\n",
    "        return output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}