{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2c057ba-304e-4360-afab-7be0e04a3f5b",
   "metadata": {},
   "source": [
    "# Qwen 2.5\n",
    "\n",
    "```{note}\n",
    "Compared to previous iterations, Qwen 2.5{cite}`qwen2025qwen25technicalreport` has\n",
    "been significantly improved during both the pre-training and post-training stages. In\n",
    "terms of pre-training, we have scaled the high-quality pre-training datasets from the\n",
    "previous 7 trillion tokens to 18 trillion tokens. In terms of post-training,\n",
    "we implement intricate supervised finetuning with over 1 million samples, as well as\n",
    "multistage reinforcement learning, including `offline learning DPO` and `online learning\n",
    "GRPO`.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5aecd7-d3f7-4a4b-b6d2-7c9dec8b0b00",
   "metadata": {},
   "source": [
    "## Architecture & Tokenizer\n",
    "\n",
    "Basically, the Qwen2.5 series include dense models for opensource, and MoE models for API service.\n",
    "\n",
    "For dense models, we maintain the Transformer-based decoder architecture{cite}`vaswani2023attentionneed` as Qwen2. The architecture incorporates several key components:\n",
    "Grouped Query Attention (GQA{cite}`ainslie2023gqatraininggeneralizedmultiquery`) for efficient KV cache utilization, SwiGLU activation\n",
    "function{cite}`dauphin2017languagemodelinggatedconvolutional` for non-linear activation, Rotary Positional Embeddings (RoPE{cite}`su2023roformerenhancedtransformerrotary`) for encoding position information, [QKV](https://spaces.ac.cn/archives/9577) bias in the attention mechanism and\n",
    "RMSNorm{cite}`jiang2023prermsnormprecrmsnormtransformersequivalent` with pre-normalization to ensure stable training.\n",
    "\n",
    "Building upon the dense model architectures, we extend it to MoE model architectures. This is achieved\n",
    "by replacing standard feed-forward network (FFN) layers with specialized MoE layers.\n",
    "\n",
    "For tokenization, we utilize Qwen’s tokenizer{cite}`bai2023qwentechnicalreport`, which implements byte-level byte-pair\n",
    "encoding (BBPE{cite}`wang2019neuralmachinetranslationbytelevel`) with a vocabulary of 151,643\n",
    "regular tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c750024-c765-4758-abe4-1aa6cf6dd5ce",
   "metadata": {},
   "source": [
    "## Pre-training\n",
    "\n",
    "Qwen2.5 demonstrates significant enhancements in pre-training data quality compared to its predecessor\n",
    "Qwen2. These improvements stem from several key aspects:\n",
    "\n",
    "1. Better data filtering.\n",
    "2. Better math and code data.\n",
    "3. Better synthetic data.\n",
    "4. Better data mixture.\n",
    "\n",
    "We develop scaling laws for hyper-parameter based on the pre-training data of Qwen2.5. While previous studies primarily used scaling laws to determine optimal model sizes given compute budgets, we\n",
    "leverage them to identify optimal hyperparameters across model architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a71d68e-c441-4ffb-82c2-5d007f019dbc",
   "metadata": {},
   "source": [
    "## Post-training\n",
    "\n",
    "Qwen 2.5 introduces two significant advancements in its post-training design compared to Qwen 2:\n",
    "\n",
    "1. Expanded Supervised Fine-tuning Data Coverage: The supervised fine-tuning process leverages\n",
    "a massive dataset comprising millions of high-quality examples.\n",
    "2. Two-stage Reinforcement Learning: The reinforcement learning (RL) process in Qwen 2.5 is\n",
    "divided into two distinct stages: Offline RL and Online RL.\n",
    "    * Offline RL: This stage focuses on developing capabilities that are challenging for the reward\n",
    "model to evaluate, such as reasoning, factuality, and instruction-following.\n",
    "    * Online RL: The Online RL phase leverages the reward model’s ability to detect nuances in\n",
    "output quality.\n",
    "\n",
    "### Supervised Fine-tuning\n",
    "\n",
    "We detail the key enhancements made during the SFT phase of Qwen2.5, focusing on\n",
    "several critical areas:\n",
    "\n",
    "**Coding:** To enhance coding capabilities, we incorporate the instruction tuning data of Qwen2.5-\n",
    "Coder. We expand our instruction dataset by synthesizing new examples from code-related Q&A\n",
    "websites and gathering algorithmic code snippets from GitHub. A `comprehensive multilingual\n",
    "sandbox` is used to perform static code checking and validate code snippets through automated\n",
    "unit testing, ensuring code quality and correctness.\n",
    "\n",
    "### Offline Reinforcement Learning\n",
    "\n",
    "In this study, we focus on objective query domains such as mathematics,\n",
    "coding, instruction following, and logical reasoning, where obtaining accurate evaluations can be complex.\n",
    "In the previous phase, we extensively employ strategies like `execution feedback` and answer matching to\n",
    "ensure the quality of responses. For the current phase, we reuse that pipeline, employing the SFT model\n",
    "to resample responses for a new set of queries. Responses that pass our quality checks are used as positive\n",
    "examples, while those that fail are treated as negative examples for Direct Preference Optimization (DPO{cite}`rafailov2024directpreferenceoptimizationlanguage`). To further enhance the reliability and accuracy of the training signals, we\n",
    "make use of both `human and automated review` processes.\n",
    "\n",
    "### Online Reinforcement Learning\n",
    "\n",
    "To develop a robust reward model for online RL, we adhere to a set of carefully defined labeling criteria:\n",
    "\n",
    "* Truthfulness\n",
    "* Helpfulness\n",
    "* Conciseness\n",
    "* Relevance\n",
    "* Harmlessness\n",
    "* Debiasing\n",
    "\n",
    "The queries utilized to train the reward model are drawn from two distinct datasets: publicly available\n",
    "open-source data and a proprietary query set characterized by higher complexity. Responses are generated\n",
    "from checkpoints of the Qwen models, which have been fine-tuned using different methods—SFT,\n",
    "DPO, and RL—at various stages of training. To introduce diversity, those responses are sampled at\n",
    "`different temperature settings`. Preference pairs are created through both human and automated labeling\n",
    "processes, and the training data for DPO is also integrated into this dataset.\n",
    "\n",
    "In our online reinforcement learning (RL) framework, we employ [](grpo). `The query set utilized for training the reward model is identical to the one used\n",
    "in the RL training phase`. The sequence in which queries are processed during training is determined by\n",
    "the variance of their response scores, as evaluated by the reward model. Specifically, queries with higher\n",
    "variance in response scores are prioritized to ensure more effective learning. We sample 8 responses\n",
    "for each query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a098c2-d869-4f2a-854c-9ec72a4ea741",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "### Base Models\n",
    "\n",
    "```{figure} ../images/qwen25-1.png\n",
    "```\n",
    "\n",
    "### Instruction-tuned Model\n",
    "\n",
    "```{figure} ../images/qwen25-2.png\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03656b47-2a43-4844-8c0c-b65a343cf151",
   "metadata": {},
   "source": [
    "## Takeaway\n",
    "\n",
    "```{note}\n",
    "Prompts:\n",
    "* publicly available open-source data\n",
    "* algorithmic code snippets from GitHub\n",
    "* synthesizing new examples from code-related Q&A websites\n",
    "* proprietary query set characterized by higher complexity\n",
    "\n",
    "Check correctness:\n",
    "* comprehensive multilingual sandbox\n",
    "* human and automated review\n",
    "* a set of carefully defined labeling criteria\n",
    "\n",
    "Tips:\n",
    "* sample using different models and temperature settings\n",
    "* the query set utilized for training the reward model is identical to the one used in the RL training phase\n",
    "* two stage RL: offline DPO and online GRPO\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a983facf-e947-4a5b-aae5-0e99a80995f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}