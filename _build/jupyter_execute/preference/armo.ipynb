{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed16110f-16a6-4f43-9f13-395aacf2b503",
   "metadata": {},
   "source": [
    "# ArmoRM-MoE\n",
    "\n",
    "```{note}\n",
    "Interpretable preferences via multi-objective reward modeling and mixture-of-experts.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27fe9b8-93d3-4d8a-b101-0d7ad8cdaa03",
   "metadata": {},
   "source": [
    "## The Need for Interpretable Reward Models\n",
    "\n",
    "![](../images/armo1.png)\n",
    "\n",
    "When applying RLHF for LLM alignment, the phenomenon of reward hacking is widely observed. A notable example of this is the verbosity bias, where aligned LLMs produce longer-than-necessary responses because the RM favors length, regardless of quality.\n",
    "\n",
    "![](../images/armo2.png)\n",
    "\n",
    "How can we mitigate the reward hacking issue? We believe one solution is to make the reward model more interpretable and debuggable. Let’s continue considering the verbosity bias example. Suppose the RM’s output is interpretable, explaining that it assigns a high score to a response due to two factors: 40% for its helpfulness and 60% for its length. We could adjust its decision-making process to base its scoring 100% on helpfulness, regardless of response length, thus mitigating the verbosity bias.\n",
    "\n",
    "To build RMs with interpretable preferences, we propose a two-stage approach:\n",
    "\n",
    "1. Train an Absolute-Rating Multi-Objective Reward Model (ArmoRM) with multi-dimensional absolute-rating data;\n",
    "\n",
    "2. Employ a Mixture-of-Experts (MoE) strategy with a gating network that automatically selects the most suitable reward objectives based on the context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91f3b6c-0419-4e7c-bfa5-6c2bf123626f",
   "metadata": {},
   "source": [
    "## Stage-1: Multi-Objective Reward Modeling\n",
    "\n",
    "Some recent high-quality datasets have multi-objective absolute ratings. For instance, the UltraFeedback dataset is curated with 5-objective absolute ratings: Overall Score, Instruction Following, Truthfulness, Honesty, and Helpfulness.\n",
    "\n",
    "![](../images/armo3.png)\n",
    "\n",
    "We consider each example to consist of a prompt $x$, response $y$, and a $k$-dimensional rating vector $r\\in\\mathbb{R}^{k}$, where each dimension corresponds to a reward objective such as helpfulness and truthfulness. Now, we take a pre-trained decoder-only LLM without the original output linear layer as the feature extractor $f_{\\theta}$, and pass $(x,y)$ through the decoder layers to take the hidden state of the final decoder layer on the last token as a \n",
    "$d$-dimensional feature. Also, we attach a new linear regression layer $w\\in\\mathbb{R}^{d\\times k}$ on top of $f_{\\theta}$, which outputs \n",
    "$k$-dimensional rating prediction. The model can be straightforwardly trained with regression loss:\n",
    "\n",
    "$$\n",
    "\\underset{\\theta,w}{\\min}\\mathbb{E}_{x,y,r}\\|w^{\\intercal}f_{\\theta}(x,y)-r\\|^{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c968841c-b20f-4e5b-a169-5471752d5bb7",
   "metadata": {},
   "source": [
    "## Stage-2: Mixture-of-Experts Aggregation of Reward Objectives\n",
    "\n",
    "We follow the common MoE practice to add a gating layer:\n",
    "\n",
    "$$\n",
    "g_{\\phi}:\\mathbb{R}^{d}\\to v\\in\\mathbb{R}^{k},v_{i}\\ge 0\\text{ and }\\sum{v}_{i}=1\n",
    "$$\n",
    "\n",
    "based on the feature extracted from the prompt $f_{\\theta}(x)\\in\\mathbb{R}^{d}$, i.e. the hidden state on the last token of $x$. The gating layer $g_{\\phi}$ can simply be a shallow MLP.\n",
    "\n",
    "![](../images/armo4.png)\n",
    "\n",
    "However, most reward objectives are highly correlated with `verbosity`, which indicates a strong verbosity bias. Using non-negative gating coefficients would make the final output inherit the bias. To resolve the issue, we adjust each reward objective, $r_{i}$, with a penalty using the verbosity reward objective,\n",
    "\n",
    "$$\n",
    "r_{i}'\\gets r_{i} - \\lambda_{i}r_{\\text{verbose}}\n",
    "$$\n",
    "\n",
    "where the penalty coefficient $\\lambda_{i}$ is chosen such that for a proper correction metric (e.g., Pearson or Spearman correlation coefficient) and a reference data distribution $\\mathcal{D}$:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{\\mathcal{D}}\\text{Corr}(r_{i}',r_{\\text{verbose}}) = 0.\n",
    "$$\n",
    "\n",
    "The adjusted reward vector is denoted as $r'\\in\\mathbb{R}^{k}$.\n",
    "\n",
    "Finally, we multiply the gating coefficients to the multi-objective rewards, to obtain a scalar score for the response \n",
    "$y$ given prompt $x$,\n",
    "\n",
    "$$\n",
    "R = g_{\\phi}(f_{\\theta}(x))^{\\intercal}r'\n",
    "$$\n",
    "\n",
    "To train the gating layer, we freeze the parameters of the backbone and the regression layer, and only train the gating layer with the Bradley-Terry loss,\n",
    "\n",
    "$$\n",
    "\\underset{\\phi}{\\min}\\mathbb{E}\\left[\\frac{\\exp(R_{\\text{chosen}})}{\\exp(R_{\\text{chosen}} + R_{\\text{rejected}})}\\right]\n",
    "$$\n",
    "\n",
    "where $R_{\\text{chosen}}$ and $R_{\\text{rejected}}$ are the preference scores for the chosen and rejected responses in each pairwise example, $(x,y_{\\text{chosen}},y_{\\text{rejected}})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4b3c8f-d261-49f5-9d9a-e0e5dc6d6180",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}