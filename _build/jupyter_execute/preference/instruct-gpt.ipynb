{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9766cee1-5aaf-4835-8a01-0d5c8c03f201",
   "metadata": {},
   "source": [
    "# Instruct GPT\n",
    "\n",
    "```{note}\n",
    "The language modeling objective used for many recent large LMs—predicting the next token on a webpage from the internet is different from the objective “follow the user’s instructions helpfully and safely”.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2ee164-1cbc-432a-aa6a-810c6bf83411",
   "metadata": {},
   "source": [
    "## methodology\n",
    "\n",
    "We start with a pretrained language\n",
    "model, a distribution of prompts on which we want our model to produce aligned outputs, and a team\n",
    "of trained human labelers. We then apply the following three steps.\n",
    "\n",
    "![](../images/instruct-1.png)\n",
    "\n",
    "Steps 2 and 3 can be iterated continuously; more comparison data is collected on the current best\n",
    "policy, which is used to train a new RM and then a new policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a0803b-eaf4-4144-a42a-da9941821a28",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Our prompt dataset consists primarily of text prompts submitted to the OpenAI API. To train the very first InstructGPT models, we asked labelers to write prompts themselves.\n",
    "\n",
    "From these prompts, we produce three different datasets used in our fine-tuning procedure:\n",
    "\n",
    "1. our SFT dataset, with labeler demonstrations used to train our SFT models.\n",
    "\n",
    "2. our RM dataset, with labeler rankings of model outputs used to train our RMs.\n",
    "\n",
    "3. our PPO dataset, without any human labels, which are used as inputs for RLHF fine-tuning.\n",
    "\n",
    "![](../images/instruct-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b75abf-1165-4659-9543-b281a822a1b1",
   "metadata": {},
   "source": [
    "## Supervised fine-tuning (SFT)\n",
    "\n",
    "We fine-tune GPT-3 on our labeler demonstrations using supervised\n",
    "learning trained for 16 epochs. \n",
    "\n",
    "```{caution}\n",
    "We do our final SFT model selection based on the RM score on the validation set?\n",
    "```\n",
    "\n",
    "We find that our SFT models overfit on validation loss after 1 epoch; however, we find\n",
    "that training for more epochs helps both the RM score and human preference ratings, despite this\n",
    "overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb8950f-3e56-4292-acf4-098622e280bf",
   "metadata": {},
   "source": [
    "## Reward modeling (RM)\n",
    "\n",
    "Starting from the SFT model with the final unembedding layer removed,\n",
    "we trained a model to take in a prompt and response, and output a scalar reward.\n",
    "\n",
    "In order to speed up comparison collection, we present labelers with anywhere between $K=4$ and $K=9$ responses to rank. This produces $\\binom{K}{2}$ comparisons for each prompt shown to a labeler. We train on all $\\binom{K}{2}$ comparisons from each prompt as a single batch element.\n",
    "\n",
    "The loss function for the reward model is:\n",
    "\n",
    "$$\n",
    "\\log(\\theta) = \\frac{1}{-\\binom{K}{2}}\\mathbb{E}_{(x, y_{w}, y_{l})\\sim D}\\left[\\log(\\sigma(r_{\\theta}(x, y_{w}) - r_{\\theta}(x, y_{l})))\\right]\n",
    "$$\n",
    "\n",
    "where $r_{\\theta}(x, y)$ is the scalar output of the reward model for prompt $x$ and completion $y$ with parameters $\\theta$, $y_{w}$ is the preferred completion out of the pair $y_{w}$ and $y_{l}$, and $D$ is the dataset of human\n",
    "comparisons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e049a346-9678-40db-8710-56cc1a0f50b8",
   "metadata": {},
   "source": [
    "## Reinforcement learning (RL)\n",
    "\n",
    "We fine-tuned the SFT model on our environment using PPO. Given\n",
    "the prompt and response, the reward model produces a reward and ends the episode. In addition, we add a per-token KL penalty from the SFT model at each token to mitigate over-optimization\n",
    "of the reward model. The value function is initialized from the RM. We call these\n",
    "models “PPO.”\n",
    "\n",
    "We also experiment with mixing the pretraining gradients into the PPO gradients, in order to fix the\n",
    "performance regressions on public NLP datasets. We call these models “PPO-ptx.” We maximize the\n",
    "following combined objective function in RL training:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{objective} = &\\mathbb{E}_{(x, y)\\sim D_{\\pi_{\\phi}^{\\text{RL}}}}\\left[r_{\\theta}(x, y) - \\beta\\log\\left(\\pi_{\\phi}^{\\text{RL}}(y|x) / \\pi^{\\text{SFT}}(y|x)\\right)\\right] + \\\\\n",
    "&\\gamma\\mathbb{E}_{x\\sim D_{\\text{pretrain}}}\\left[\\log(\\pi_{\\phi}^{\\text{RL}}(x))\\right]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $\\pi_{\\phi}^{\\text{RL}}$ is the learned RL policy, $\\pi^{\\text{SFT}}$ is the supervised trained model, and $D_{\\text{pretrain}}$ is the\n",
    "pretraining distribution. The KL reward coefficient, $\\beta$, and the pretraining loss coefficient, $\\gamma$, control\n",
    "the strength of the KL penalty and pretraining gradients respectively.\n",
    "\n",
    "![](../images/trl.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa345ba-14e6-427f-9321-a458e6f94471",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}