{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "79c50b51-1256-4bc8-8338-e128b457e1d3",
   "metadata": {},
   "source": [
    "# OpenRLHF\n",
    "\n",
    "| Feature | OpenRLHF | DSChat | CAIChat | TRL |\n",
    "| ------------- |:-------------:| :-------------:| :-------------:| :-------------:|\n",
    "| 70B+ Full Tuning with 16 A100-80GB      | ✅ | ❌ | ❌ | ❌ |\n",
    "| 7B Full Tuning with 4 RTX4090 | ✅      |    ❌ | ❌ | ❌ |\n",
    "| 34B DPO Full Tuning with 8 A100-80GB | ✅      |    ❌ | ❌ | ❌ |  \n",
    "| Inference Engine in PPO | ✅      |    ✅ | ❌ | ❌ |  \n",
    "| PPO Implementation Tricks | ✅      |    ❌ | ❌ | ✅ |\n",
    "| Support QLoRA | ✅      |    ❌ | ❌ | ✅ | \n",
    "| Support Mixtral 8*7b | ✅      |    ❌ | ❌ | ❌ |  \n",
    "| Support Unmerged Actor-Critic | ✅     |   ✅ | ✅ | ❌ | \n",
    "| Support Multiple Reward Models | ✅      |    ❌ | ❌ | ❌ |   \n",
    "| Support Huggingface Models | ✅      |    ✅ | ✅ | ✅ | \n",
    "| Easy-to-use | ✅      |   ❌ (HybridEngine bugs) | ✅ | ✅ | "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e14887f-23c3-41d7-9ff8-19dc19661c14",
   "metadata": {},
   "source": [
    "## PPO implementation detail\n",
    "\n",
    "`train_ppo_llama.sh`:\n",
    "\n",
    "```shell\n",
    "openrlhf.cli.train_ppo \\\n",
    "   --pretrain OpenRLHF/Llama-3-8b-sft-mixture \\\n",
    "   --reward_pretrain OpenRLHF/Llama-3-8b-rm-mixture \\\n",
    "   --save_path ./checkpoint/llama-3-8b-rlhf \\\n",
    "   --save_steps -1 \\\n",
    "   --logging_steps 1 \\\n",
    "   --eval_steps -1 \\\n",
    "   --micro_train_batch_size 2 \\\n",
    "   --train_batch_size 128 \\\n",
    "   --micro_rollout_batch_size 4 \\\n",
    "   --rollout_batch_size 1024 \\\n",
    "   --max_epochs 1 \\\n",
    "   --prompt_max_len 1024 \\\n",
    "   --generate_max_len 1024 \\\n",
    "   --zero_stage 2 \\\n",
    "   --bf16 \\\n",
    "   --actor_learning_rate 5e-7 \\\n",
    "   --critic_learning_rate 9e-6 \\\n",
    "   --init_kl_coef 0.01 \\\n",
    "   --prompt_data OpenRLHF/prompt-collection-v0.1 \\\n",
    "   --input_key context_messages \\\n",
    "   --apply_chat_template \\\n",
    "   --max_samples 100000 \\\n",
    "   --normalize_reward \\\n",
    "   --adam_offload \\\n",
    "   --flash_attn \\\n",
    "   --load_checkpoint \\\n",
    "   --gradient_checkpointing\n",
    "```\n",
    "\n",
    "`trainer.fit`:\n",
    "\n",
    "```python\n",
    "# The outermost layer\n",
    "for episode in range(args.num_episodes):\n",
    "    \n",
    "    # len(rand_prompts) == args.rollout_batch_size // strategy.world_size\n",
    "    # drop_last=True\n",
    "    for rand_prompts in self.prompts_dataloader:\n",
    "        \n",
    "        # `make_experience_list` make a list of experience with the micro_rollout_batch_size.\n",
    "        # `append` first split_experience_batch, then extend items\n",
    "        # micro_rollout_batch_size makes no real difference\n",
    "        for experience in self.experience_maker.make_experience_list(rand_prompts, **self.generate_kwargs):\n",
    "            self.replay_buffer.append(experience)\n",
    "\n",
    "        self.replay_buffer.normalize(\"advantages\", self.strategy)\n",
    "        status = self.ppo_train(steps)\n",
    "        self.replay_buffer.clear()\n",
    "```\n",
    "\n",
    "`ppo_train`:\n",
    "\n",
    "```python\n",
    "for epoch in range(self.max_epochs):\n",
    "    \n",
    "    # go over replay buffer\n",
    "    # len(experience) == micro_train_batch_size\n",
    "    # train_batch_size makes no real difference\n",
    "    for experience in pbar:\n",
    "\n",
    "        # training step\n",
    "        status = {}\n",
    "        status = self.training_step_actor(experience)\n",
    "        status.update(self.training_step_critic(experience))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffaefe2-e948-4743-b483-a946ad23a1c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}