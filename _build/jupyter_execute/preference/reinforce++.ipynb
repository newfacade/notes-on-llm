{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b331930a-7d08-4609-a721-e72654bda9e2",
   "metadata": {},
   "source": [
    "# REINFORCE++\n",
    "\n",
    "```{note}\n",
    "We present REINFORCE++, an enhanced variant of the classical REINFORCE algorithm\n",
    "that incorporates key optimization techniques from PPO while eliminating the need for a critic\n",
    "network.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b7707c-66bd-49f5-ae50-a54a7a4f3d01",
   "metadata": {},
   "source": [
    "## Background: The REINFORCE Algorithm\n",
    "\n",
    "REINFORCE is a foundational policy gradient method in reinforcement learning that directly optimizes the expected\n",
    "return of a policy through gradient ascent. The algorithm operates as follows:\n",
    "\n",
    "**Trajectory Sampling:** The agent interacts with the environment to generate trajectories consisting of states,\n",
    "actions, and rewards.\n",
    "\n",
    "**Return Calculation:** The discounted cumulative rewards for each trajectory are computed as:\n",
    "\n",
    "$$\n",
    "G_t = \\sum_{k=t+1}^{T}\\gamma^{k-t}r_{k}\n",
    "$$\n",
    "\n",
    "where $\\gamma$ is the discount factor.\n",
    "\n",
    "**Policy Gradient Estimation:** The gradient of the expected return with respect to the policy parameters is\n",
    "estimated using:\n",
    "\n",
    "$$\n",
    "\\nabla_{\\theta}J(\\theta) = \\mathbb{E}_{\\pi}[G_t\\nabla_{\\theta}\\log\\pi_{\\theta}(A_t|S_t)]\n",
    "$$\n",
    "\n",
    "```{tip}\n",
    "Don’t Let the Past Distract You: using reward to go instead of sum of all rewards.\n",
    "```\n",
    "\n",
    "**Policy Update:** The policy parameters are updated via gradient ascent:\n",
    "\n",
    "$$\n",
    "\\theta\\leftarrow\\theta + \\alpha\\nabla_{\\theta}J(\\theta)\n",
    "$$\n",
    "\n",
    "where $\\alpha$ is the learning rate.\n",
    "\n",
    "Despite its simplicity, REINFORCE suffers from high variance in gradient estimates, which can hinder its scalability to\n",
    "complex tasks such as aligning LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9db899-402a-4a33-8d79-4b75614c2d73",
   "metadata": {},
   "source": [
    "## REINFORCE++ Enhancements\n",
    "\n",
    "### Token-Level KL Penalty\n",
    "\n",
    "We implement a token-level KL divergence penalty between the RL model and the SFT model distributions. This penalty is incorporated into the reward function as follows:\n",
    "\n",
    "$$\n",
    "r(s_t, a_t) = \\mathbf{I}(s_t=[EOS])r(x,y) - \\beta\\mbox{KL}(t)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mbox{KL}(t) = \\log\\left(\\frac{\\pi_{\\theta_{\\text{old}}}^{\\text{RL}}(a_t|s_t)}{\\pi^{\\text{SFT}}(a_t|s_t)}\\right)\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "* $x$ represents the input prompt\n",
    "* $y$ denotes the generated response\n",
    "* $\\mathbf{I}(s_t=[EOS])$ indicates whether $t$ is the final token\n",
    "* $\\beta$ is the KL penalty coefficient\n",
    "\n",
    "This approach facilitates better credit assignment.\n",
    "\n",
    "### PPO-Clip Integration\n",
    "\n",
    "We incorporate PPO’s clipping mechanism to constrain policy updates:\n",
    "\n",
    "$$\n",
    "L^{CLIP}(\\theta) = \\mathbb{E}_{t}\\left[\\min\\left(r_{t}(\\theta)\\hat{A}_{t}, \\mbox{clip}(r_{t}(\\theta), 1-\\epsilon, 1 + \\epsilon)\\hat{A}_t\\right)\\right]\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "* $r_{t}(\\theta) = \\frac{\\pi_{\\theta}(a_t|s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t|s_t)}$ is the probability ratio of taking action $a_t$ in state $s_t$ under the new policy versus the old policy.\n",
    "* $\\hat{A}_{t}$ is the estimated advantage for token $t$.\n",
    "* $\\mbox{clip}(r_{t}(\\theta), 1-\\epsilon, 1 + \\epsilon)$ restricts the probability ratio to be within the range of $[1-\\epsilon,1+\\epsilon]$, where $\\epsilon$ is a small hyperparameter.\n",
    "\n",
    "This formulation effectively allows the algorithm to take advantage of positive advantages while preventing excessively\n",
    "large updates that could destabilize training. The use of the minimum function ensures that if the ratio moves too far\n",
    "from 1 (either above or below), it does not contribute `positively` to the objective, thus maintaining a form of trust region\n",
    "for policy updates.\n",
    "\n",
    "### Advantage Normalization\n",
    "\n",
    "The advantage function in REINFORCE++ is defined as:\n",
    "\n",
    "$$\n",
    "A_{t}(s_t, a_t) = r(x,y) - \\beta\\sum_{i=t}^{T}\\mbox{KL}(i)\n",
    "$$\n",
    "\n",
    "We normalize these advantages using z-score normalization:\n",
    "\n",
    "$$\n",
    "A_{\\text{normalized}} = \\frac{A - \\mu_{A}}{\\sigma_{A}}\n",
    "$$\n",
    "\n",
    "where $\\mu_A$ and $sigma_{A}$ represent the batch mean and standard deviation respectively. Normalization ensures stable gradients\n",
    "and prevents divergence during training.\n",
    "\n",
    "```{tip}\n",
    "PPO compute advantages using GAE:\n",
    "* TD error $\\delta_t = r(s_t, a_t) + \\gamma V(s_{t+1}) - V(s_t)$\n",
    "* The Generalized Advantage Estimator $\\hat{A}(s_t, a_t) = \\sum(\\gamma\\lambda)^{l}\\delta_{t+l}^{V}$\n",
    "* $\\hat{R}_{t} = \\hat{A}(s_t, a_t) + V(s_t)$ is the estimated reward to go\n",
    "\n",
    "Reinforce++ compute advantages using z-score normalization:\n",
    "* $A_{t}(s_t, a_t) = r(x,y) - \\beta\\sum_{i=t}^{T}\\mbox{KL}(i)$ is exactly the reward to go when $\\gamma=1$\n",
    "* $A_{\\text{normalized}} = \\frac{A - \\mu_{A}}{\\sigma_{A}}$ ensures stable gradients\n",
    "```\n",
    "\n",
    "### Reward Normalization and Clipping\n",
    "\n",
    "We implement comprehensive reward processing to stabilize training:\n",
    "\n",
    "* **Normalization:** Standardizes rewards using z-score normalization to mitigate outliers.\n",
    "* **Clipping:** Constrains reward values within predefined bounds to avoid instability.\n",
    "* **Scaling:** Applies appropriate scaling factors for numerical stability during updates.\n",
    "\n",
    "```{caution}\n",
    "Detail of scailing?\n",
    "```\n",
    "\n",
    "### Mini-Batch Updates\n",
    "\n",
    "To enhance training efficiency, we implement mini-batch updates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0586d21e-44ae-40c0-bf4a-72bac8b59e0f",
   "metadata": {},
   "source": [
    "## Experimental Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36660ea6-5c54-4a8b-9d70-f9b7ef7886df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}