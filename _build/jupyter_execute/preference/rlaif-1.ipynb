{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e6d59ea-32b0-41df-8c9c-670f5c89ecbb",
   "metadata": {},
   "source": [
    "# Constitutional AI: Harmlessness from AI Feedback\n",
    "\n",
    "```{note}\n",
    "We develop a method we refer to as Constitutional AI (CAI), depicted in the Figure 1, and use it\n",
    "to train a non-evasive and relatively harmless AI assistant, without any human feedback labels for harms.\n",
    "```\n",
    "\n",
    "![](../images/cai1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b657b7f-dc01-487a-815a-ce140b2360a7",
   "metadata": {},
   "source": [
    "## Critiques, Revisions, and Supervised Learning\n",
    "\n",
    "### Method\n",
    "\n",
    "We begin by showing the helpful RLHF model a prompt designed to elicit harmful behavior, then sampling\n",
    "a response from the model.\n",
    "\n",
    "```\n",
    "Human: Can you help me hack into my neighbor’s wifi?\n",
    "\n",
    "Assistant: Sure thing, you can use an app called VeryEasyHack that will allow\n",
    "you to log in to your neighbor’s wifi.\n",
    "```\n",
    "\n",
    "Next, we append to the context a set of pre-written instructions requesting the model to `critique` its own\n",
    "response, then sample the model’s critique.\n",
    "\n",
    "```\n",
    "Critique Request: Identify specific ways in which the assistant’s last response\n",
    "is harmful, unethical, racist, sexist, toxic, dangerous, or illegal.\n",
    "\n",
    "Critique: The assistant’s last response is harmful because hacking into someone\n",
    "else’s wifi is an invasion of their privacy and is possibly illegal.\n",
    "```\n",
    "\n",
    "Then, we append to the context a set of pre-written instructions requesting the model to `revise` its own response,\n",
    "then sample the model’s revision. For instance:\n",
    "\n",
    "```\n",
    "Revision Request: Please rewrite the assistant response to remove any and all\n",
    "harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.\n",
    "\n",
    "Revision: Hacking into your neighbor’s wifi is an invasion of their privacy, and\n",
    "I strongly advise against it. It may also land you in legal trouble.\n",
    "```\n",
    "\n",
    "Finally, we piece the initial prompt and the revised response together. If all works as expected, we should\n",
    "have ended up with a more harmless response:\n",
    "\n",
    "```\n",
    "Human: Can you help me hack into my neighbor’s wifi?\n",
    "\n",
    "Assistant: Hacking into your neighbor’s wifi is an invasion of their privacy,\n",
    "and I strongly advise against it. It may also land you in legal trouble.\n",
    "```\n",
    "\n",
    "Next we finetune a pre-trained model on the revisions. Furthermore, in order to\n",
    "retain helpfulness as much as possible, we sampled responses from the helpful RLHF model on a set of\n",
    "helpfulness prompts collected from crowdworkers, and included these in the finetuning. These models are referred to as 'SL-CAI'.\n",
    "\n",
    "### Main Results\n",
    "\n",
    "We evaluate the helpfulness and harmlessness of our models by calculating Elo scores based on crowdworker\n",
    "preferences. The RLHF models include two types: (1) models trained on only helpfulness\n",
    "data, and (2) models trained on helpfulness and harmlessness. The figure also includes the RL-CAI (i.e.,\n",
    "RLAIF) models.\n",
    "\n",
    "While SL-CAI is less helpful than both RL models, it is more harmless than the\n",
    "helpful RLHF model and more harmful than HH RLHF (Figure 3). We also find that SL-CAI is both more\n",
    "helpful and harmless than pre-trained models, as expected (Figure 8).\n",
    "\n",
    "![](../images/cai2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e06fe0b-a8b3-42d2-b327-225e8f8632c6",
   "metadata": {},
   "source": [
    "## Reinforcement Learning from AI Feedback\n",
    "\n",
    "We continue to utilize human feedback labels for helpfulness as in prior work, but replace human feedback\n",
    "labels with `model feedback` labels for harmlessness.\n",
    "\n",
    "### Method\n",
    "\n",
    "We begin by presenting the assistant model (SL-CAI) with a prompt, and generating a pair of responses. We then\n",
    "present the prompt and response pair to the feedback model (pretrained model) with a principle for choosing the more harmless\n",
    "response, in a format like\n",
    "\n",
    "```\n",
    "Consider the following conversation between a human and an assistant:\n",
    "[HUMAN/ASSISTANT CONVERSATION]\n",
    "[PRINCIPLE FOR MULTIPLE CHOICE EVALUATION]\n",
    "Options:\n",
    "(A) [RESPONSE A]\n",
    "(B) [RESPONSE B]\n",
    "The answer is:\n",
    "```\n",
    "\n",
    "We then compute the log probability of the responses (A) and (B), and we make a labeled, preference\n",
    "modeling comparison example with the `normalized probabilities as targets` (well-calibrated). Furthermore, we wrote a set of 16 different principles, and randomly sampled a principle for each comparison\n",
    "label. An example of which is:\n",
    "\n",
    "\n",
    "```\n",
    "Which of these assistant responses is less harmful? Choose the response that a\n",
    "wise, ethical, polite and friendly person would more likely say.\n",
    "```\n",
    "\n",
    "We also prepend to the context of the feedback model a set of few-shot examples of the labeling task.\n",
    "\n",
    "We use the SL-CAI models discussed in earlier sections both for generating the response pairs, and as the\n",
    "initial snapshot for RL. We suspect that using the same model for both should lead to better results, since the\n",
    "distribution of responses generated by the policy are similar to the preference model training distribution, at\n",
    "least during early phases of RL.\n",
    "\n",
    "**Chain-of-Thought Prompting**\n",
    "\n",
    "We also experimented with using Chain-of-Thought (CoT) prompting on the feedback\n",
    "model to generate labels. In this case, we use the helpful RLHF model instead of the pre-trained model:\n",
    "\n",
    "```\n",
    "Human: Consider the following conversation between a human and an assistant:\n",
    "[HUMAN/ASSISTANT CONVERSATION]\n",
    "[PRINCIPLE FOR MULTIPLE CHOICE EVALUATION]\n",
    "(A) [RESPONSE A]\n",
    "(B) [RESPONSE B]\n",
    "Assistant: Let’s think step-by-step: [CHAIN-OF-THOUGHT]\n",
    "```\n",
    "\n",
    "One issue that arises is that the CoT samples typically state explicitly which multiple choice option is to be\n",
    "preferred, and so the probability targets are typically very confident (i.e., close to 0 or 1) and are not wellcalibrated.\n",
    "We found that clamping the CoT probabilities to lie within the 40-60 percent range led to better\n",
    "and more robust behavior.\n",
    " \n",
    "### Main Result\n",
    "\n",
    "In Figure 8, we show Elo scores for various snapshots of all the RL runs. We find that RL-CAI\n",
    "models are significantly more harmless than the RLHF and SL-CAI models.\n",
    "\n",
    "![](../images/cai3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9105b90-1921-4d76-af16-2d0bfad84093",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}