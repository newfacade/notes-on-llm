{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe5e0002-fd7f-4e13-90df-89858680083c",
   "metadata": {},
   "source": [
    "# RLCD\n",
    "\n",
    "```{note}\n",
    "RLCD (Reinforcement Learning from Contrastive Distillation) creates\n",
    "preference pairs from two contrasting model outputs, one using a positive prompt\n",
    "designed to encourage following the given principles, and one using a negative\n",
    "prompt designed to encourage violating them.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8340aff-b488-4a77-84b2-22644880550c",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "RLAIF approaches simulate human pairwise preferences by scoring $o_1$ and\n",
    "$o_2$ with an LLM. However, by using the same prompt $p$ to generate both $o_1$ and $o_2$, causing $o_1$ and $o_2$ to often be of very similar quality and thus hard to differentiate.\n",
    "\n",
    "Context distillation methods create more training signal by\n",
    "modifying the initial prompt $p$. The modified prompt $p+$ typically contains additional context\n",
    "encouraging a directional attribute change in the output $o+$. However, context\n",
    "distillation methods only generate a single output $o+$ per prompt $p+$, which is then used for supervised\n",
    "fine-tuning.\n",
    "\n",
    "![](../images/rlcd1.png)\n",
    "\n",
    "Rather than producing\n",
    "two i.i.d. $(o_1, o_2)$ from the same prompt p as in RLAIF, RLCD creates two variations of $p$: a positive\n",
    "prompt $p+$ similar to context distillation which encourages directional change toward a desired\n",
    "attribute, and a negative prompt $p−$ which encourages directional change against it. We then generate model outputs $(o+, o−)$ respectively, and automatically label $o+$ as preferred. We then follow the\n",
    "standard RL pipeline of training a preference model followed by PPO."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c8835b-c3c1-416c-8722-a083c2328fb0",
   "metadata": {},
   "source": [
    "## RLCD\n",
    "\n",
    "How to construct RLCD’s positive and negative prompts\n",
    "$p+$, $p−$ for preference pair generation. We identify two major criteria for prompt construction:\n",
    "\n",
    "1. $p+$ should be more likely than $p−$ to produce outputs exemplifying the desired attribute (e.g.,\n",
    "harmlessness, helpfulness). Equivalently, $p−$ may explicitly encourage directional change\n",
    "toward the opposite attribute.\n",
    "2. The surface forms of $p+$ and $p−$ should be as similar as possible, for example, $p+$ and $p−$ differ only in the words “harmless” vs. “harmful.”\n",
    "\n",
    "![](../images/rlcd2.png)\n",
    "\n",
    "For RLCD, for each example when simulating data, we randomly sample a pair of descriptions from\n",
    "Table 9 to use when building $p+$ and $p−$, $p+$ and $p−$ are then constructed by placing a description in parentheses before the final colon\n",
    "in the ending “Assistant:” indicator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2363d3f-f641-4a7b-b71b-42e5656d5fef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}