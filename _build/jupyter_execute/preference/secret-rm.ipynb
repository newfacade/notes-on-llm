{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "333b703e-d823-4bcd-ae14-5bb529cbdd82",
   "metadata": {},
   "source": [
    "# Secrets of RLHF in Large Language Models: Reward Modeling\n",
    "\n",
    "```{note}\n",
    "While reward models are often considered central to achieving high\n",
    "performance, they face the following challenges in practice:<br>\n",
    "1. Incorrect and ambiguous preference pairs in the dataset may hinder the reward\n",
    "model from accurately capturing human intent.<br>\n",
    "2. Reward models trained on data\n",
    "from a specific distribution often struggle to generalize to examples outside that\n",
    "distribution and are not suitable for iterative RLHF training.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a87814a-b40f-41ce-a9cf-7c7db6716014",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "\n",
    "In the reward modeling stage, the SFT model $\\pi^{\\text{SFT}}$ is prompted\n",
    "with a user query denoted as $x$ to produce two distinct outputs $(y_1,y_2)\\sim\\pi^{\\text{SFT}}(y|x)$. Human labelers\n",
    "are instructed to choose their preferred output, resulting in $y_c\\succ y_r$, where $y_c$ and $y_r$ represent the\n",
    "chosen and rejected outputs, respectively, from the pair $(y_1,y_2)$. By following the Bradley-Terry\n",
    "model, we formulate a preference distribution by employing the reward function $r_{\\psi}(x,y)$ as\n",
    "outlined below:\n",
    "\n",
    "$$\n",
    "p_{\\psi}(y_c\\succ y_r|x) = \\sigma(r_{\\psi}(x, y_c) - r_{\\psi}(x, y_r))\n",
    "$$\n",
    "\n",
    "where $\\sigma$ is the logistic function. Treating the problem as a binary classification task yields the negative\n",
    "log-likelihood loss function:\n",
    "\n",
    "$$\n",
    "L(r_{\\psi}) = -\\mathbb{E}_{(x,y)\\sim\\mathcal{D}}[\\log \\sigma(r_{\\psi}(x, y_c) - r_{\\psi}(x, y_r))]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581b1c71-31c9-4547-87b7-81fbda405a57",
   "metadata": {},
   "source": [
    "## Measuring the Strength of Preferences\n",
    "\n",
    "The **preference strength (difference)** between chosen and rejected responses can be quantified using\n",
    "\n",
    "$$\n",
    "d_{i,\\psi} = r_{\\psi}(x^{(i)}, y_{\\text{c}}^{(i)}) - r_{\\psi}(x^{(i)}, y_{\\text{r}}^{(i)})\n",
    "$$\n",
    "\n",
    "We train $M$ reward models using the same preference data, with the training order randomized. By utilizing the ensemble of reward scores from these $M$ reward\n",
    "models, we can calculate the mean and standard deviation (std) of preference strength for each\n",
    "comparison pair:\n",
    "\n",
    "$$\n",
    "\\hat{u}_{i} = \\frac{1}{M}\\sum_{m=1}^{M}d_{i,\\psi_{m}},\\quad\\hat{\\sigma}_{i} = \\sqrt{\\frac{\\sum_{m=1}^{M}(d_{i,\\psi_{m}} - \\hat{u}_{i})^{2}}{M}}\n",
    "$$\n",
    "\n",
    "We observe that the mean of preference differences for approximately 25% of the data is less than\n",
    "0.\n",
    "\n",
    "![](../images/secret1.png)\n",
    "\n",
    "![](../images/secret2x.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1b0c78-8ba6-496d-9490-ff57f1774dd5",
   "metadata": {},
   "source": [
    "## Impacts of Different Data on RM Performance\n",
    "\n",
    "We can use preference strength to partition the training data into different\n",
    "groups. We are curious about the contributions that different groups of training sets have made to\n",
    "modeling preferences. We train a reward model from scratch for each group, where each group’s\n",
    "data size is 10% of the original training data size, and then evaluate its performance on the validation\n",
    "set.\n",
    "\n",
    "![](../images/secret3.png)\n",
    "\n",
    "According to the results, we can observe that:\n",
    "\n",
    "1. For the top 20% of data with the lowest preference\n",
    "strength, they have a negative impact on the model’s performance on the validation set.\n",
    "\n",
    "2. For data ranked between 20% and 40%, after\n",
    "training, the model’s prediction accuracy on the validation set is approximately 0.5.\n",
    "\n",
    "3. The remaining data significantly improves the model’s\n",
    "performance. However, the top 10% of data with the highest preference strength does not achieve\n",
    "the best performance when trained alone.\n",
    "\n",
    "Based on the above results, we can roughly categorize\n",
    "preference data into three types: incorrect data, ambiguous data (almost no difference), and normal\n",
    "data (clear differences)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f71fbd6-f494-4449-99fa-d4bcf7f0b807",
   "metadata": {},
   "source": [
    "## Analyze and Leverage Diverse Data to its Fullest Potential\n",
    "\n",
    "### Flipping the Labels\n",
    "\n",
    "By flipping the labels of the bottom 20% of data with the lowest preference strength, the model could more effectively learn preference information for modeling, as demonstrated below.\n",
    "\n",
    "![](../images/secret4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9707ec7-3fc0-4a83-9f80-efee57040d7a",
   "metadata": {},
   "source": [
    "### Label Smoothing\n",
    "\n",
    "Label smoothing is another widely known technique to mitigate the overfitting problem by penalizing\n",
    "overconfident model outputs:\n",
    "\n",
    "$$\n",
    "L_{\\text{LS}} = -\\mathbb{E}_{(x,y)\\sim\\mathcal{D}}[(1-\\alpha)\\log(p_{\\psi}(y_{c}\\succ y_r|x)) + \\alpha\\log(1-p_{\\psi}(y_{c}\\succ y_r|x))]\n",
    "$$\n",
    "\n",
    "where $\\alpha$ is the smoothing parameter.\n",
    "\n",
    "![](../images/secret-margin-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999d0432-1ea7-4eaf-85da-0e4173a704e4",
   "metadata": {},
   "source": [
    "### Adaptive Margin\n",
    "\n",
    "Using preference\n",
    "strength information, we can guide the reward model to assign more discrepant scores to responses\n",
    "with higher preference strength, which has been shown to be beneficial for preference modeling. Therefore, we add an adaptive margin component to the loss of the reward model:\n",
    "\n",
    "$$\n",
    "L(r_{\\psi}) = -\\mathbb{E}_{(x,y)\\sim\\mathcal{D}}[\\log \\sigma(r_{\\psi}(x, y_c) - r_{\\psi}(x, y_r) - \\hat{u}(x,y))]\n",
    "$$\n",
    "\n",
    "where the marginal function $\\hat{u}(x,y)$ serves as a continuous measure of preference strength. Adding a margin to all the data effectively enhances the performance of\n",
    "preference modeling:\n",
    "\n",
    "![](../images/secret-margin-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bacefb3-8cae-4ade-9e9a-c144d6f24513",
   "metadata": {},
   "source": [
    "### Takeaways\n",
    "\n",
    "* **Label Flipping** and **Label Smoothing** can effectively avoid the impact of noisy preferences\n",
    "and improve performance, provided that you can accurately identify noisy preference data.\n",
    "\n",
    "* When learning data with strong preference strength, the reward model may be prone to\n",
    "overfitting, which can be mitigated by using **Label Smoothing**.\n",
    "\n",
    "* **Adaptive margin** almost always benefits all preference data and can be widely applied to\n",
    "reward modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fecbff4-9b66-4eb7-9347-6f07629db34a",
   "metadata": {},
   "source": [
    "## How to Better Model Human Preference?\n",
    "\n",
    "In this report, we mainly consider four methods to improve reward modeling. In our\n",
    "practical experiments, these methods show improvements over the original reward modeling method:\n",
    "\n",
    "* **Flip**: Flip the noise data labels in the preference data.\n",
    "\n",
    "* **Margin**: Add an adaptive margin to the loss function for all preference pairs.\n",
    "\n",
    "* **Flip + Margin**: Flip the noise data labels in the preference data and add an adaptive margin\n",
    "to the loss function for all preference pairs.\n",
    "\n",
    "* **Soft Label + Margin**: Apply label smoothing to data with the preference strength less than\n",
    "0 and add an adaptive margin to the loss function for all preference pairs.\n",
    "\n",
    "![](../images/secret-how.png)\n",
    "\n",
    "```{caution}\n",
    "Apply label smoothing to data with the preference strength less than\n",
    "0 need first flip the labels?\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9f901b-a204-414a-bfc2-0e8e527ca220",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}