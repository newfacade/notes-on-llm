{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "652b9eb2-3487-449b-b595-6bc249805200",
   "metadata": {},
   "source": [
    "# Self-Rewarding Language Models\n",
    "\n",
    "```{note}\n",
    "Our approach first assumes access to a base pretrained language model, and a small amount\n",
    "of human-annotated seed data. We then build a model that aims to possess two skills\n",
    "simultaneously:\n",
    "\n",
    "1. **Instruction following**: given a prompt that describes a user request, the ability to\n",
    "generate a high quality, helpful (and harmless) response.\n",
    "\n",
    "2. **Self-Instruction creation**: the ability to generate and evaluate new instructionfollowing\n",
    "examples to add to its own training set.\n",
    "\n",
    "These skills are used so that the model can perform self-alignment, i.e., they are the\n",
    "components used to iteratively train itself using AI Feedback (AIF).\n",
    "```\n",
    "\n",
    "![](../images/self-reward.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22cbcd0-252c-4fb2-93e6-2a2ff584eff5",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "**Seed instruction following data:** We are given a seed set of human-authored (instruction\n",
    "prompt, response) general instruction following examples that we use for training in a\n",
    "supervised fine-tuning (SFT) manner, starting from a pretrained base language model.\n",
    "Subsequently this will be referred to as Instruction Fine-Tuning (IFT) data.\n",
    "\n",
    "**Seed LLM-as-a-Judge instruction following data:** We also assume we are provided\n",
    "a seed set of (evaluation instruction prompt, evaluation result response) examples which\n",
    "can also be used for training. While this is not strictly necessary, as the model using IFT\n",
    "data will already be capable of training an LLM-as-a-Judge, we show that such training\n",
    "data can give improved results. Subsequently\n",
    "this will be referred to as Evaluation Fine-Tuning (EFT) data.\n",
    "\n",
    "![](../images/self-reward2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267ecbfe-62c5-4710-8b6c-9428e806d1e2",
   "metadata": {},
   "source": [
    "## Self-Instruction Creation\n",
    "\n",
    "Using the model we have trained, we can make it self-modify its own training set. Specifically,\n",
    "we generate additional training data for the next iteration of training:\n",
    "\n",
    "1. **Generate a new prompt**: We generate a new prompt $x_i$ using few-shot prompting, sampling prompts from the original seed IFT data.\n",
    "\n",
    "2. **Generate candidate responses**: We then generate $N$ diverse candidate responses $\\{y_{i}^{1},\\dots,y_{i}^{N}\\}$ for the given prompt $x_i$ from our model using sampling.\n",
    "\n",
    "3. **Evaluate candidate responses**: Finally, we use the LLM-as-a-Judge ability of our same model to evaluate its own candidate responses with scores $r_{i}^{n}\\in[0,5]$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7f3b4e-f3a9-4392-91ff-ebb7d8135404",
   "metadata": {},
   "source": [
    "## Instruction Following Training\n",
    "\n",
    "After performing the self-instruction creation procedure, we can\n",
    "then augment the seed data with additional examples for training, which we refer to as AI\n",
    "Feedback Training (AIFT) data. We try two variants of such feedback:\n",
    "\n",
    "* **Preference pairs**: We construct training data of the form (instruction prompt $x_i$,\n",
    "winning response $y_{i}^{w}$, losing response $y_{i}^{l}$). To form the winning and losing pair\n",
    "we take the highest and lowest scoring responses from the $N$ evaluated candidate\n",
    "responses.\n",
    "\n",
    "* **Positive examples only**: We add additional examples of (instruction\n",
    "prompt, response) to the seed set for supervised fine-tuning. In this setup we only add examples\n",
    "where the candidate response was evaluated to give a perfect score of $r_{i}^{n}=5$.\n",
    "\n",
    "We find that learning from preference pairs gives superior performance, and hence recommend that approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed24ffb4-b497-4043-b4e4-e3d8f77288bc",
   "metadata": {},
   "source": [
    "## Overall Self-Alignment Algorithm\n",
    "\n",
    "Our overall procedure trains a series of models $M_1, . . . ,M_T$ where\n",
    "each successive model $t$ uses augmented training data created by the ${t âˆ’ 1}^{\\text{th}}$ model. We thus\n",
    "define $\\text{AIFT}(M_t)$ to mean AI Feedback Training data created using model $M_t$.\n",
    "\n",
    "**Model Sequence**\n",
    "\n",
    "* $M_0$: Base pretrained LLM with no fine-tuning.\n",
    "\n",
    "* $M_1$: Initialized with $M_0$, then fine-tuned on the IFT+EFT seed data using SFT.\n",
    "\n",
    "* $M_2$: Initialized with $M_1$, then trained with $\\text{AIFT}(M_1)$ data using DPO.\n",
    "\n",
    "* $M_3$: Initialized with $M_2$, then trained with $\\text{AIFT}(M_2)$ data using DPO."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b0ae22-6846-4bb4-a442-19c2e1719da5",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "\n",
    "### Experimental Setup\n",
    "\n",
    "**Base Model:** We use Llama 2 70B as our base pretrained model.\n",
    "\n",
    "**IFT Seed Data:** We use the human-authored examples provided in the Open Assistant\n",
    "dataset for instruction fine-tuning.\n",
    "\n",
    "**EFT Seed Data:** The Open Assistant data also provides multiple ranked human responses\n",
    "per prompt from which we can construct evaluation fine-tuning data.\n",
    "\n",
    "**Instruction Following Evaluation:** We evaluate head-to-head performance between various models\n",
    "using GPT-4 as an evaluator over 256 test prompts using the AlpacaEval evaluation prompt. We try the prompt in both orders comparing pairwise.\n",
    "\n",
    "**Reward Modeling Evaluation:** We evaluate the correlation with human rankings on the evaluation set\n",
    "we derived from the Open Assistant dataset.\n",
    "\n",
    "### Instruction Following Ability Results\n",
    "\n",
    "* EFT+IFT seed training performs similarly to IFT alone\n",
    "\n",
    "* Iteration 2 ($M_2$) improves over Iteration 1 ($M_1$) and SFT Baseline\n",
    "\n",
    "* Iteration 3 ($M_3$) improves over Iteration 2 ($M_2$)\n",
    "\n",
    "![](../images/self-reward-3.png)\n",
    "\n",
    "### Reward Modeling Ability Results\n",
    "\n",
    "* EFT augmentation improves over SFT baseline\n",
    "\n",
    "* Reward Modeling ability improves with Self-Training\n",
    "\n",
    "* Importance of the LLM-as-a-Judge Prompt: we also tried various\n",
    "other prompts to decide the most effective one to use, our prompt describes\n",
    "the points as additive, covering various aspects of quality is the best.\n",
    "\n",
    "![](../images/self-reward-5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ec35f3-c831-4c2e-8a2a-e86035ec95c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}