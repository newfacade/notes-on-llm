{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61e50fd3-fc40-41e0-944a-d5d468804938",
   "metadata": {},
   "source": [
    "# West-of-N\n",
    "\n",
    "```{note}\n",
    "$\\text{Best-of-N} + \\text{Worst-of-N} = \\text{West-of-N}$\n",
    "```\n",
    "\n",
    "![](../images/westofn1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e87c03d-cc2e-4e48-b019-f315a13b906a",
   "metadata": {},
   "source": [
    "## Related Work\n",
    "\n",
    "**Best-of-N Sampling.** Best-of-N, or rejection sampling is typically implemented by taking\n",
    "the top-scored generation within a pool of $N$ candidates. In practice, Best-of-N strategies steer the\n",
    "output distribution towards high-reward generations, which has been shown to improve the\n",
    "performance of language models trained with supervised\n",
    "finetuning.\n",
    "\n",
    "**Synthetic Preference Data.** Human preference data collection remains\n",
    "expensive, time-consuming and noisy; this motivates\n",
    "the use of synthetic data. RL from AI Feedback (RLAIF), is to use large language\n",
    "models to label response pairs instead of relying\n",
    "on human labeling. RL from Contrast\n",
    "Distillation (RLCD) use\n",
    "contrasting positive and negative prompts to produce pairs of high- and low-quality responses.\n",
    "\n",
    "![](../images/westofn2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d472ceea-20a8-48f1-8ca4-5e20649283b5",
   "metadata": {},
   "source": [
    "## Self-Training for Preference Modeling\n",
    "\n",
    "Assume access to some `initial preference dataset` $\\mathcal{D}_{L} = \\{(x,y_{+},y_{-}):y_{+}\\succ y_{-}\\}$, which could consist of human\n",
    "preferences or other synthetically-generated data. We use this data to train a base\n",
    "preference model parameterized by $\\theta$: let $P_{\\theta}(y_{+}\\succ y_{-}|x)$ model the probability of response $y_{+}$ being preferred over $y_{-}$ for a query $x$.\n",
    "\n",
    "A simple strategy to generate synthetic preference data for\n",
    "unlabeled query $x$ is to sample two responses $y_{1}$, $y_{2}$ from the\n",
    "generation policy $\\pi_{x}$, and to pseudo-label the preference pair based on $P_{\\theta}(y_1\\succ y_2|x)$:\n",
    "\n",
    "$$\n",
    "f_{\\pm}(x) = \n",
    "\\begin{cases}\n",
    "(y_1, y_2)\\quad&\\text{if}\\quad P_{\\theta}(y_1\\succ y_2|x)>0.5\\\\\n",
    "(y_2, y_1)&\\text{else.}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "RL from AI Feedback can be seen as an\n",
    "example of this approach. In this special case, $\\mathcal{D}_{L}=\\emptyset$ and pseudolabeling\n",
    "is achieved through few-shot prompting.\n",
    "\n",
    "## West-of-N Self-Training\n",
    "\n",
    "As in any self-training effort, the above pseudolabeling approach\n",
    "is highly dependent on the performance of base\n",
    "model $P_{\\theta}$: an imperfect model will often assign incorrect\n",
    "labels to preference pairs. This is mitigated in prior\n",
    "self-training work by only retaining samples with highconfidence\n",
    "pseudolabels\n",
    "\n",
    "Extending this idea, we propose to maximize the probability\n",
    "of correctly labeling a pair of on-policy responses to a given\n",
    "query, according to the base preference model:\n",
    "\n",
    "$$\n",
    "\\underset{(y_{+},y_{-})\\sim\\pi(x)}{\\max}P_{\\theta}(y_+\\succ y_-|x)\n",
    "$$\n",
    "\n",
    "In practice, this objective can be approximated by sampling\n",
    "a pool of $N$ candidate outputs from the policy and identify\n",
    "the best- and worst-scored ones.\n",
    "\n",
    "```{tip}\n",
    "**Pseudo-Preference Filtering.** To further improve the\n",
    "quality of generated preference pairs, these can be filtered\n",
    "based on the confidence of their preference label and\n",
    "their coverage of the relevant response distribution.\n",
    "\n",
    "We\n",
    "measure model confidence in labeling a preference through\n",
    "the prediction $P_{\\theta}(y_+\\succ y_-|x)$, and only retain West-of-N\n",
    "pairs above a certain quantile. Similarly, we also apply a\n",
    "likelihood threshold of both positive and negative responses $\\pi(y_{+}|x)$ and $\\pi(y_{-}|x)$, to ensure the responses being compared\n",
    "remain in-distribution. We determine final threshold\n",
    "values through validation performance.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805b1ba8-a873-4310-b7d2-a9826d027110",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}