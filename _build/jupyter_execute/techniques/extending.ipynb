{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "484d55a1-6a0c-4fff-b3d0-63dfd39990ae",
   "metadata": {},
   "source": [
    "# Extending context window of LLMs\n",
    "\n",
    "```{note}\n",
    "We present Position Interpolation (PI){cite}`chen2023extendingcontextwindowlarge` that extends the context window sizes of\n",
    "RoPE{cite}`su2023roformerenhancedtransformerrotary`-based pretrained LLMs such as LLaMA{cite}`touvron2023llamaopenefficientfoundation` models to up to 32768 with minimal fine-tuning (within 1000 steps), demonstrating strong empirical results on various tasks that require long context. Meanwhile, the extended model by Position Interpolation preserve quality relatively well on tasks within its original context window.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0327f2a9-b207-4f61-a287-d0dd85a186e7",
   "metadata": {},
   "source": [
    "## Background: Rotary Position Embedding (RoPE)\n",
    "\n",
    "Transformer models require explicit positional information to be injected, typically in the form of\n",
    "positional encodings, to represent the order of inputs. We consider Rotary Position Embedding, which is the position encoding used in the LLaMA model.\n",
    "\n",
    "Given a position index $m\\in[0, c)$ and an embedding vector $\\mathbf{x} := [x_0, x_1, . . . , x_{d−1}]^{\\intercal}$, where\n",
    "$d$ is the dimension of the attention head, RoPE defines a vector-valued complex function $f(\\mathbf{x}, m)$ as\n",
    "follows\n",
    "\n",
    "$$f(\\mathbf{x}, m) = \\left[(x_{0} + ix_{1})e^{im\\theta_{0}}, (x_{2} + ix_{3})e^{im\\theta_{1}},\\dots,(x_{d-2} + ix_{d-1})e^{im\\theta_{d/2-1}}\\right]^{\\intercal}$$\n",
    "\n",
    "where $i:=\\sqrt{-1}$ is the imaginary unit and $\\theta_{j}=10000^{-2j/d}$. Using RoPE, the self-attention score\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "a(m,n) &= \\text{Re}\\left \\langle f(\\mathbf{q}, m), f(\\mathbf{k}, n)  \\right \\rangle \\\\\n",
    "&= \\text{Re}\\left[\\sum_{j=0}^{d/2-1}(q_{2j} + iq_{2j+1})(k_{2j} - ik_{2j+1})e^{i(m-n)\\theta_{j}}\\right]\\\\\n",
    "&= \\sum_{j=0}^{d/2-1}(q_{2j}k_{2j} + q_{2j+1}k_{2j+1})\\cos((m-n)\\theta_{j}) + (q_{2j}k_{2j+1} - q_{2j+1}k_{2j})\\sin((m-n)\\theta_{j})\\\\\n",
    "&=: a(m-n)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "is only dependent on relative position $m− n$ through trigonometric functions. Here $\\mathbf{q}$ and $\\mathbf{k}$ are the\n",
    "query and key vector for a specific attention head. At each layer, RoPE is applied on both query and\n",
    "key embeddings for computing attention scores.\n",
    "\n",
    "```{tip}\n",
    "$\\left \\langle x,y \\right \\rangle := x\\bar{y}\\quad \\text{for }x,y\\in\\mathbb{C}$ and write complex numbers in the polar coordinate system. Similarily:\n",
    "\n",
    "$$b(m,n) = \\text{Im}\\left \\langle f(\\mathbf{q}, m), f(\\mathbf{k}, n)  \\right \\rangle =: b(m-n)$$\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91e185c-8fa3-4f0c-90b4-db828f5cad5a",
   "metadata": {},
   "source": [
    "## Position interpolation\n",
    "\n",
    "Large language models (LLMs) typically come with a pre-defined context window size. For example,\n",
    "inputs to LLaMA models must be fewer than 2048 tokens. This pre-set\n",
    "context window limit is frequently exceeded in application. However, training an LLM from scratch with long context\n",
    "windows requires significant investments. This naturally leads to a question: Can we extend the\n",
    "context window of an existing pre-trained LLM?\n",
    "\n",
    "One straightforward approach is to fine-tune an existing pre-trained Transformer with a longer context\n",
    "window. However, empirically, we found that models trained this way adapt to long context\n",
    "windows very slowly.\n",
    "\n",
    "Here, we introduce Position Interpolation to enable context window extensions for certain\n",
    "existing pre-trained LLMs, including LLaMA. The key idea is, instead of extrapolation, we directly\n",
    "down-scale the position indices so that the maximum position index matches the previous context\n",
    "window limit in the pre-training stage.\n",
    "\n",
    "![](../images/extending.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0781ff95-179e-4ac8-ab6a-735f51224371",
   "metadata": {},
   "source": [
    "## 高频外推低频内插\n",
    "\n",
    "* 低维（$i\\to 0$）部分频率高（$\\theta_{i}\\to 1$）\n",
    "* 高维（$i\\to d/2-1$）部分频率低（$\\theta_{i}\\to 1/10000$）\n",
    "\n",
    "原本在低维度上，旋转角度较大，意味着这些维度上的信号变化非常迅速，能够精细地区分相邻位置。如果在低维度进行内插，对用低维区分不同位置间的能力影响更大，这种现象称之为`高频信息的损失`。因此我们可采用高频外推，低频内插的方式。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213fd6f2-f872-4751-b930-ca542707a7a3",
   "metadata": {},
   "source": [
    "(yarn)=\n",
    "## YaRN: Efficient ContextWindow Extension of Large Language Models\n",
    "\n",
    "### Loss of High Frequency information - \"NTK-aware\" interpolation\n",
    "\n",
    "Instead of scaling every dimension\n",
    "of RoPE equally by a factor $s=\\frac{L'}{L}$, we spread out the interpolation pressure across multiple dimensions\n",
    "by scaling high frequencies less and low frequencies more.\n",
    "\n",
    "**Definition 1** The \"NTK-aware\" interpolation is a modification of RoPE with the\n",
    "following functions.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "g(m) &= m \\\\\n",
    "h(\\theta_{d}) &= b'^{-2d/|D|},\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "b' = b\\cdot s^{\\frac{|D|}{|D|-2}}.\n",
    "$$\n",
    "\n",
    "```{tip}\n",
    "For small $d$:\n",
    "\n",
    "$$b'^{-2d/|D|}\\approx b^{2d/|D|}$$\n",
    "\n",
    "thus extrapolation.\n",
    "\n",
    "To let $(L', b')$ equals to $(L, b)$ on the lowest frequency ($d=\\frac{|D|}{2}-1$), thus interpolation, we have\n",
    "\n",
    "$$\n",
    "\\frac{L'}{b'^{\\frac{2}{|D|}(\\frac{|D|}{2}-1)}} = \\frac{L}{b^{\\frac{2}{|D|}(\\frac{|D|}{2}-1)}}\n",
    "$$\n",
    "\n",
    "this leads to $b' = b\\cdot s^{\\frac{|D|}{|D|-2}}$.\n",
    "```\n",
    "\n",
    "### Loss of Relative Local Distances - \"NTK-by-parts\" interpolation\n",
    "\n",
    "We choose not\n",
    "to interpolate the higher frequency dimensions at all while always interpolating the lower frequency\n",
    "dimensions. In particular,\n",
    "\n",
    "* if the wavelength $\\lambda$ is much smaller than the context size $L$, we do not interpolate;\n",
    "* if the wavelength $\\lambda$ is equal to or bigger than the context size $L$, we want to only interpolate\n",
    "and avoid any extrapolation (unlike the previous \"NTK-aware\" method);\n",
    "* dimensions in-between can have a bit of both, similar to the \"NTK-aware\" interpolation.\n",
    "\n",
    "In the $d$-th hidden state, the ratio $r$ depends on $d$ in the following way:\n",
    "\n",
    "$$\n",
    "r(d) = \\frac{L}{\\lambda_{d}} = \\frac{L}{2\\pi b'^{\\frac{2d}{|D|}}}.\n",
    "$$\n",
    "\n",
    "In order to define the boundary of the different interpolation strategies as above, we introduce\n",
    "two extra parameters $\\alpha,\\beta$ and define the ramp function $\\gamma$ to be\n",
    "\n",
    "$$\n",
    "\\gamma(r)=\n",
    "\\begin{cases}\n",
    "0, \\quad&\\text{if }r<\\alpha\\\\\n",
    "1, \\quad&\\text{if }r>\\beta\\\\\n",
    "\\frac{r-\\alpha}{\\beta - \\alpha}, &\\text{otherwise.}\n",
    "\\end{cases}\n",
    "$$\n",
    "With the help of the ramp function, the \"NTK-by-parts\" method can be described as follows.\n",
    "\n",
    "**Definition 2** The \"NTK-by-parts\" interpolation is a modification of RoPE with the\n",
    "following functions\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "g(m) &= m\\\\\n",
    "h(\\theta) &= \\left(1 - \\gamma(r(d))\\right)\\frac{\\theta_d}{s} + \\gamma(r(d))\\theta_{d}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### Dynamic Scaling - \"Dynamic NTK\" interpolation\n",
    "\n",
    "In a lot of use cases, multiple forward-passes are performed with varying sequence lengths from 1 to\n",
    "the maximal context size. A typical example is the autoregressive generation where the sequence\n",
    "lengths increment by 1 after each step. There are two ways of applying an interpolation method that\n",
    "uses a scale factor $s$:\n",
    "\n",
    "1. Fixed scale factor $s=L'/L$.\n",
    "2. In each forward-pass, the position embedding updates the scale factor $s=\\max(1, l'/L)$ where $l'$ is the sequence length of the current sequence.\n",
    "\n",
    "### YaRN\n",
    "\n",
    "In addition to the previous interpolation techniques, we also observe that introducing a temperature t\n",
    "on the logits before the attention softmax has a uniform impact on perplexity, that is\n",
    "\n",
    "$$\n",
    "\\text{softmax}\\left(\\frac{\\mathbf{q}_{m}^{T}\\mathbf{k}_{n}}{t\\sqrt{D}}\\right).\n",
    "$$\n",
    "\n",
    "We can use a \"length scaling\" trick which scales both $\\mathbf{q}_{m}$ and $\\mathbf{k}_{n}$ by a\n",
    "constant factor $\\sqrt{{1}/{t}}$ by simply scaling the complex RoPE embeddings by the same amount. Combining it with the \"NTK-by-parts\" interpolation, we have\n",
    "the YaRN method.\n",
    "\n",
    "**Definition 3** By the \"YaRN method\", we refer to a combination of the attention scaling and\n",
    "the \"NTK-by-parts\" interpolation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0451d94-21e6-4371-9b5f-d81349511ea1",
   "metadata": {},
   "source": [
    "## RoPE 的远程衰减\n",
    "\n",
    "计算 $a(m,n)$ 时：\n",
    "\n",
    "* $m$ 和 $n$ 越近，$\\mathbf{R}_{n-m}$ 旋转得越少，高频维度少低频维度多。\n",
    "* $m$ 和 $n$ 越远，$\\mathbf{R}_{n-m}$ 旋转得越多，有很多高频维度转了很多圈，随机性很大，一部分正负抵消一部分振荡。\n",
    "\n",
    "最终导致 RoPE 远程衰减曲线如下：\n",
    "\n",
    "![](../images/rope_3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cdf974-b95c-43c8-bb94-f5b074a517b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}