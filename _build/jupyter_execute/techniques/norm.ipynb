{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e91c368e-f53b-4264-871d-75f5beb2e0db",
   "metadata": {},
   "source": [
    "(normalization)=\n",
    "# Normalization\n",
    "\n",
    "```{tip}\n",
    "To improve the\n",
    "training stability, Llama{cite}`touvron2023llamaopenefficientfoundation` normalize the input of each\n",
    "transformer sub-layer, instead of normalizing the\n",
    "output. Using RMSNorm as the normalizing function.\n",
    "```\n",
    "\n",
    "```{figure} ../images/post-pre-norm.svg\n",
    "```\n",
    "\n",
    "## Batch Normalization\n",
    "\n",
    "$$y = \\frac{x - \\mathbf{E}[x]}{\\sqrt{\\mathbf{Var}[x] + \\epsilon}}*\\gamma+\\beta$$\n",
    "\n",
    "$\\mathbf{E}[x]$ and $\\mathbf{Var}[x]$ are calculated over $(N, L)$ and they are vectors of size $C$, where $N$ denote batch size, $L$ denote sequence length and $C$ denote number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5596d425-7b09-4b52-ae4b-0ce50f6e5040",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# (N, L, C)\n",
    "batch, seq_len, num_features = 2, 3, 4\n",
    "x = torch.randn(batch, seq_len, num_features).permute(0, 2, 1)  # BatchNorm1d requires (N, C, L) input\n",
    "bn = nn.BatchNorm1d(num_features)\n",
    "\n",
    "t1 = bn(x)\n",
    "t2 = (x - x.mean(axis=[0, 2], keepdim=True)) / x.std(axis=[0, 2], unbiased=False, keepdim=True)\n",
    "torch.allclose(t1, t2, atol=1e-3, rtol=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c666911-0c10-4f24-9261-c9bd70c6250e",
   "metadata": {},
   "source": [
    "(layer_normalization)=\n",
    "## Layer Normalization\n",
    "\n",
    "$$y = \\frac{x - \\mathbf{E}[x]}{\\sqrt{\\mathbf{Var}[x] + \\epsilon}}*\\gamma+\\beta$$\n",
    "\n",
    "$\\mathbf{E}[x]$ and $\\mathbf{Var}[x]$ are calculated over $C$ and they are vectors of size $(N,L)$.\n",
    "\n",
    "```{tip}\n",
    "LayerNorm normalize each example and each position `independently`.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5ede768-2125-49f0-b53d-3e08704680f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (N, L, C)\n",
    "batch, seq_len, num_features = 2, 3, 4\n",
    "x = torch.randn(batch, seq_len, num_features)\n",
    "ln = nn.LayerNorm(num_features)\n",
    "\n",
    "t1 = ln(x)\n",
    "t2 = (x - x.mean(axis=-1, keepdim=True)) / x.std(axis=-1, unbiased=False, keepdim=True)\n",
    "torch.allclose(t1, t2, atol=1e-3, rtol=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e546196-72c4-4be3-93a5-4c2d3359bb32",
   "metadata": {},
   "source": [
    "## RMSNorm\n",
    "\n",
    "$$y = \\frac{x}{\\sqrt{\\mathbf{MeanSquare}[x] + \\epsilon}}*\\gamma$$\n",
    "\n",
    "$\\mathbf{MeanSquare}[x]$ is calculated over $C$ and is a vector of size $(N, L)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69b95489-5290-461b-bfce-f23179981e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(torch.nn.Module):\n",
    "    \"\"\"RMSNorm in Llama 3\"\"\"\n",
    "    \n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (N, L, C)\n",
    "        # weight shape: (C,)\n",
    "        output = self._norm(x.float()).type_as(x)\n",
    "        return output * self.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6df645d-8f8c-4077-bdc0-3503d0ac40c8",
   "metadata": {},
   "source": [
    "## Why Layer Normalization\n",
    "\n",
    "Layer Normalization (LN) is preferred over Batch Normalization (BN) in Transformer models for several key reasons.\n",
    "\n",
    "1. **Batch Size Sensitivity**\n",
    "    * BN normalizes across the batch dimension, meaning it computes statistics (mean and variance) using all samples in the mini-batch. This can lead to instability or poor performance when the batch size is small.\n",
    "    * LN normalizes across the features within each individual example, independent of the batch.\n",
    "2. **Sequence Length Variability**\n",
    "    * BN assumes fixed-length inputs because it computes statistics over the entire batch, including all tokens in the sequence. However, in NLP tasks, sequences often have variable lengths, and padding is used to make them uniform. Padding tokens can distort the batch statistics.\n",
    "    *  LN operates independently for each token in the sequence, normalizing across the feature dimensions for each token.\n",
    "3. **Independence from Batch Statistics**\n",
    "    *  Since BN relies on batch statistics (mean and variance), it introduces interdependence between examples in the same batch during training. This can lead to issues during inference when the model processes one example at a time (batch size of 1).\n",
    "    *  LN does not depend on batch statistics, which ensures consistent behavior during both training and inference.\n",
    "4. **Transformer Architecture Compatibility**\n",
    "    * Transformers process sequences in parallel using self-attention mechanisms, where each token interacts with every other token in the sequence. The batch-wise nature of BN might interfere with these interactions.\n",
    "5. **Training Stability**\n",
    "    * BN can introduce instability during training due to its reliance on batch statistics, especially when the batch size is small or when there is significant variability in the input data. This can slow down convergence or lead to suboptimal solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e46008e-a00d-4cd6-82f4-5dc73ffdfd34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}