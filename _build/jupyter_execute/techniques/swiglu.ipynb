{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d50f3bb1-4344-4fd7-bb2a-b42afe5c8fec",
   "metadata": {},
   "source": [
    "# SwiGLU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5c96f0-ddbc-459a-bc41-93bff47cef31",
   "metadata": {},
   "source": [
    "## Swish\n",
    "\n",
    "We propose a new activation function that we name Swish:\n",
    "\n",
    "$$f(x) = x\\cdot\\sigma(x)$$\n",
    "\n",
    "where $\\sigma(x) = (1 + \\exp(-x))^{-1}$ is the sigmoid function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0535e74-3076-4c3b-83c4-39c83cba6073",
   "metadata": {},
   "source": [
    "## FFN\n",
    "\n",
    "The \"position-wise feed-forward networks\" (FFN) takes a vector $x$ (the hidden representation at a particular position in the sequence) and passes it through two learned linear transformations, (represented by the matrices $W_{1}$ and $W_{2}$ and bias vectors $b_{1}$ and $b_{2}$). A rectified-linear (ReLU) activation function applied between the two linear transformations.\n",
    "\n",
    "$$\\text{FFN}(x, W_{1}, W_{2}, b_{1}, b_{2}) = \\max(0, xW_{1}+b_{1})W_{2} + b_{2}$$\n",
    "\n",
    "If we use a version with no bias:\n",
    "\n",
    "$$\\text{FFN}_{\\text{ReLU}}(x, W_{1}, W_{2}) = \\max(0, xW_{1})W_{2}$$\n",
    "\n",
    "Subsequent work has proposed replacing the ReLU with other nonlinear activation functions such as Gaussian Error Linear Units, $\\text{GELU}(x) = x\\Phi(x) = xP(X\\le x)$ where $X\\sim \\mathcal{N}(0,1)$, and $\\text{Swish}_{\\beta} = x\\sigma(\\beta x)$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{FFN}_{\\text{GELU}}(x, W_{1}, W_{2}) &= \\text{GELU}(xW_{1})W_{2}\\\\\n",
    "\\text{FFN}_{\\text{Swish}}(x, W_{1}, W_{2}) &= \\text{Swish}_{1}(xW_{1})W_{2}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86417134-009f-4e33-a020-10cd08436f20",
   "metadata": {},
   "source": [
    "## Gated Linear Units (GLU) and Variants\n",
    "\n",
    "GLU is a neural network layer defined as the component-wise product of two linear transformations of the input, one of which is sigmoid-activated. They also suggest\n",
    "omitting the activation, which they call a \"bilinear\" layer.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{GLU}(x, W, V, b, c) &= \\sigma(xW + b)\\otimes(xV+c)\\\\\n",
    "\\text{Bilinear}(x, W, V, b, c) &= (xW + b)\\otimes(xV+c)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We can also define GLU variants using other activation functions:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{ReGLU}(x, W, V, b, c) &= \\max(0, xW + b)\\otimes(xV+c)\\\\\n",
    "\\text{GEGLU}(x, W, V, b, c) &= \\text{GELU}(xW+b)\\otimes(xV+c)\\\\\n",
    "\\text{SwiGLU}(x, W, V, b, c, \\beta) &= \\text{Swish}_{\\beta}(xW + b)\\otimes(xV+c)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We propose additional variations on the Transformer FFN layer which use GLU or one of\n",
    "its variants in place of the first linear transformation and the activation function. Again, we omit the bias\n",
    "terms.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{FFN}_{\\text{GLU}}(x, W, V, W_{2}) &= (\\sigma(xW + b)\\otimes(xV+c))W_{2}\\\\\n",
    "\\text{FFN}_{\\text{Bilinear}}(x, W, V, W_{2}) &= ((xW + b)\\otimes(xV+c))W_{2}\\\\\n",
    "\\text{FFN}_{\\text{ReGLU}}(x, W, V, W_{2}) &= (\\max(0, xW + b)\\otimes(xV+c))W_{2}\\\\\n",
    "\\text{FFN}_{\\text{GEGLU}}(x, W, V, W_{2}) &= (\\text{GELU}(xW + b)\\otimes(xV+c))W_{2}\\\\\n",
    "\\text{FFN}_{\\text{SwiGLU}}(x, W, V, W_{2}) &= (\\text{Swish}_{1}(xW + b)\\otimes(xV+c))W_{2}\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "All of these layers have three weight matrices, as opposed to two for the original FFN. To keep the\n",
    "number of parameters and the amount of computation constant, we reduce the number of hidden units $d_{ff}$\n",
    "(the second dimension of $W$ and $V$ and the first dimension of $W2$) by a factor of $\\frac{2}{3}$ when comparing these\n",
    "layers to the original two-matrix version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97d6435-bfc1-4668-9353-917842d1bc78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}