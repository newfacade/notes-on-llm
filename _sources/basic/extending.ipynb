{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "484d55a1-6a0c-4fff-b3d0-63dfd39990ae",
   "metadata": {},
   "source": [
    "# Extending context window of LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0327f2a9-b207-4f61-a287-d0dd85a186e7",
   "metadata": {},
   "source": [
    "## Background: Rotary Position Embedding (RoPE)\n",
    "\n",
    "Transformer models require explicit positional information to be injected, typically in the form of\n",
    "positional encodings, to represent the order of inputs. We consider Rotary Position Embedding, which is the position encoding used in the LLaMA model.\n",
    "\n",
    "Given a position index $m\\in[0, c)$ and an embedding vector $\\mathbf{x} := [x_0, x_1, . . . , x_{d−1}]^{\\intercal}$, where\n",
    "$d$ is the dimension of the attention head, RoPE defines a vector-valued complex function $f(\\mathbf{x}, m)$ as\n",
    "follows\n",
    "\n",
    "$$f(\\mathbf{x}, m) = \\left[(x_{0} + ix_{1})e^{im\\theta_{0}}, (x_{2} + ix_{3})e^{im\\theta_{1}},\\dots,(x_{d-2} + ix_{d-1})e^{im\\theta_{d/2-1}}\\right]^{\\intercal}$$\n",
    "\n",
    "where $i:=\\sqrt{-1}$ is the imaginary unit and $\\theta_{j}=10000^{-2j/d}$. Using RoPE, the self-attention score\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "a(m,n) &= \\text{Re}\\left \\langle f(\\mathbf{q}, m), f(\\mathbf{k}, n)  \\right \\rangle \\\\\n",
    "&= \\text{Re}\\left[\\sum_{j=0}^{d/2-1}(q_{2j} + iq_{2j+1})(k_{2j} - ik_{2j+1})e^{i(m-n)\\theta_{j}}\\right]\\\\\n",
    "&= \\sum_{j=0}^{d/2-1}(q_{2j}k_{2j} + q_{2j+1}k_{2j+1})\\cos((m-n)\\theta_{j}) + (q_{2j}k_{2j+1} - q_{2j+1}k_{2j})\\sin((m-n)\\theta_{j})\\\\\n",
    "&=: a(m-n)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "is only dependent on relative position $m− n$ through trigonometric functions. Here $\\mathbf{q}$ and $\\mathbf{k}$ are the\n",
    "query and key vector for a specific attention head. At each layer, RoPE is applied on both query and\n",
    "key embeddings for computing attention scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91e185c-8fa3-4f0c-90b4-db828f5cad5a",
   "metadata": {},
   "source": [
    "## Position interpolation\n",
    "\n",
    "Large language models (LLMs) typically come with a pre-defined context window size. For example,\n",
    "inputs to LLaMA models must be fewer than 2048 tokens. This pre-set\n",
    "context window limit is frequently exceeded in application. However, training an LLM from scratch with long context\n",
    "windows requires significant investments. This naturally leads to a question: Can we extend the\n",
    "context window of an existing pre-trained LLM?\n",
    "\n",
    "One straightforward approach is to fine-tune an existing pre-trained Transformer with a longer context\n",
    "window. However, empirically, we found that models trained this way adapt to long context\n",
    "windows very slowly.\n",
    "\n",
    "Here, we introduce Position Interpolation to enable context window extensions for certain\n",
    "existing pre-trained LLMs, including LLaMA. The key idea is, instead of extrapolation, we directly\n",
    "down-scale the position indices so that the maximum position index matches the previous context\n",
    "window limit in the pre-training stage.\n",
    "\n",
    "![](../images/extending.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c743290-93ff-4dd1-b9dd-a8d22acc6b6d",
   "metadata": {},
   "source": [
    "## Position extrapolation\n",
    "\n",
    "* 低维（$i\\to 0$）部分频率高（$\\theta_{i}\\to 1$）\n",
    "* 高维（$i\\to d/2-1$）部分频率低（$\\theta_{i}\\to 1/10000$）\n",
    "\n",
    "原本在低维度上，旋转角度较大，意味着这些维度上的信号变化非常迅速，能够精细地区分相邻位置。如果在低维度进行内插，对用低维区分不同位置间的能力影响更大，这种现象称之为`高频信息的损失`。\n",
    "\n",
    "因此我们可采用高频外推，低频内插的方式。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0451d94-21e6-4371-9b5f-d81349511ea1",
   "metadata": {},
   "source": [
    "## RoPE 的远程衰减\n",
    "\n",
    "计算 $a(m,n)$ 时：\n",
    "\n",
    "* $m$ 和 $n$ 越近，$\\mathbf{R}_{n-m}$ 旋转得越少，高频维度少低频维度多。\n",
    "* $m$ 和 $n$ 越远，$\\mathbf{R}_{n-m}$ 旋转得越多，有很多高频维度转了很多圈，随机性很大，一部分正负抵消一部分振荡。\n",
    "\n",
    "最终导致 RoPE 远程衰减曲线如下：\n",
    "\n",
    "![](../images/rope_3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cdf974-b95c-43c8-bb94-f5b074a517b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
