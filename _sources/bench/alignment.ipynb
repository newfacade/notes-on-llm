{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d462cab4-28f5-450d-9fc7-829ac152db49",
   "metadata": {},
   "source": [
    "# Alignment Benchmarks\n",
    "\n",
    "## IFEval\n",
    "\n",
    "```{note}\n",
    "Instruction-Following Evaluation for Large Language Models{cite}`zhou2023instructionfollowingevaluationlargelanguage`\n",
    "```\n",
    "\n",
    "One core capability of Large Language Models (LLMs) is to follow natural language\n",
    "instructions. We introduce Instruction-Following\n",
    "Eval (IFEval) for large language models. IFEval is a straightforward and easy-toreproduce\n",
    "evaluation benchmark. It focuses on a set of “verifiable instructions”\n",
    "such as “write in more than 400 words” and “mention the keyword of AI at least\n",
    "3 times”. We identified 25 types of those verifiable instructions and constructed\n",
    "around 500 prompts, with each prompt containing one or more verifiable instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbc7d6e-5868-4b58-8421-a5c553b69985",
   "metadata": {},
   "source": [
    "## Arena-Hard\n",
    "\n",
    "[Arena-Hard-Auto-v0.1](https://github.com/lmarena/arena-hard-auto) is an automatic evaluation tool for instruction-tuned LLMs. It contains 500 challenging user queries sourced from Chatbot Arena. We prompt GPT-4-Turbo as judge to compare the models' responses against a baseline model (default: GPT-4-0314). Notably, Arena-Hard-Auto has the highest correlation and separability to [Chatbot Arena](https://lmarena.ai/) among popular open-ended LLM benchmarks. If you are curious to see how well your model might perform on Chatbot Arena, we recommend trying Arena-Hard-Auto.\n",
    "\n",
    "Although both Arena-Hard-Auto and Chatbot Arena Category Hard employ similar pipeline to select hard prompts, Arena-Hard-Auto employs automatic judge as a cheaper and faster approximator to human preference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f58c79-9dc8-4941-8d2a-55f496b92566",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
