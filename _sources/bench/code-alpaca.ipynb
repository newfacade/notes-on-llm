{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20033c32-cdfa-48ee-a2db-47f616ed60c8",
   "metadata": {},
   "source": [
    "# Code Alpaca\n",
    "\n",
    "github: https://github.com/sahil280114/codealpaca?tab=readme-ov-file#fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e36136f-4e2f-4cd1-88f8-e178ab790587",
   "metadata": {},
   "source": [
    "## Stanford Alpaca\n",
    "\n",
    "The current Alpaca model is fine-tuned from a 7B LLaMA model on 52K instruction-following data generated by the techniques in the Self-Instruct, with some modifications.\n",
    "\n",
    "* We used `text-davinci-003` to generate the instruction data instead of `davinci`.\n",
    "\n",
    "* We wrote a new prompt (prompt.txt)  that explicitly gave the requirement of instruction generation to `text-davinci-003`.\n",
    "\n",
    "* We adopted much more aggressive batch decoding, i.e., generating 20 instructions at once, which significantly reduced the cost of data generation.\n",
    "\n",
    "* We simplified the data generation pipeline by discarding the difference between classification and non-classification instructions.\n",
    "\n",
    "* We only generated a single instance for each instruction.\n",
    "\n",
    "This produced an instruction-following dataset with 52K examples obtained at a much lower cost. We also find our 52K generated data to be much more diverse than the data released by self-instruct.\n",
    "\n",
    "`prompt.txt`:\n",
    "\n",
    "```{tip}\n",
    "You are asked to come up with a set of 20 diverse task instructions. These task instructions will be given to a GPT model and we will evaluate the GPT model for completing the instructions.\n",
    "\n",
    "Here are the requirements:<br>\n",
    "1. Try not to repeat the verb for each instruction to maximize diversity.<br>\n",
    "2. The language used for the instruction also should be diverse. For example, you should combine questions with imperative instrucitons.<br>\n",
    "3. The type of instructions should be diverse. The list should include diverse types of tasks like open-ended generation, classification, editing, etc.<br>\n",
    "4. A GPT language model should be able to complete the instruction. For example, do not ask the assistant to create any visual or audio output. For another example, do not ask the assistant to wake you up at 5pm or set a reminder because it cannot perform any action.<br>\n",
    "5. The instructions should be in English.<br>\n",
    "6. The instructions should be 1 to 2 sentences long. Either an imperative sentence or a question is permitted.<br>\n",
    "7. You should generate an appropriate input to the instruction. The input field should contain a specific example provided for the instruction. It should involve realistic data and should not contain simple placeholders. The input should provide substantial content to make the instruction challenging but should ideally not exceed 100 words.<br>\n",
    "8. Not all instructions require input. For example, when a instruction asks about some general information, \"what is the highest peak in the world\", it is not necssary to provide a specific context. In this case, we simply put \"<noinput>\" in the input field.<br>\n",
    "9. The output should be an appropriate response to the instruction and the input. Make sure the output is less than 100 words.\n",
    "\n",
    "List of 20 tasks:\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e329878-705f-4a7c-9332-d9427796c008",
   "metadata": {},
   "source": [
    "## Code Alpaca\n",
    "\n",
    "Code Alpaca is fully based on Stanford Alpaca ,and only changes the data used for training.\n",
    "\n",
    "* Modified prompt to focus on code generation/editing/optimization tasks instead of general tasks.\n",
    "\n",
    "* Modified seed tasks to only be related to code generation.\n",
    "\n",
    "This produced an instruction-following dataset with 20K examples obtained at a much lower cost (less than $200).\n",
    "\n",
    "### Data\n",
    "\n",
    "`data/code_alpaca_20k.json` contains 20K instruction-following data used for fine-tuning the Code Alpaca model. This JSON file is a list of dictionaries, each dictionary contains the following fields `instruction`, `input` and `output`.\n",
    "\n",
    "We used the following prompts for fine-tuning the model:\n",
    "\n",
    "* for examples with a non-empty input field:\n",
    "\n",
    "    ```\n",
    "    Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "    ### Instruction:\n",
    "    {instruction}\n",
    "\n",
    "    ### Input:\n",
    "    {input}\n",
    "\n",
    "    ### Response:\n",
    "    ```\n",
    "\n",
    "* for examples with an empty input field:\n",
    "\n",
    "    ```\n",
    "    Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "    ### Instruction:\n",
    "    {instruction}\n",
    "\n",
    "    ### Response:\n",
    "    ```\n",
    "\n",
    "During inference, we use the user instruction with an empty input field (second option)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db210f5-e366-4dc9-b214-391c7dc551c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
