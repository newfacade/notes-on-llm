{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "903ed895-f5d3-4235-9892-fcaebfc739a0",
   "metadata": {},
   "source": [
    "# General Benchmarks\n",
    "\n",
    "## MMLU\n",
    "\n",
    "```{note}\n",
    "MEASURING MASSIVE MULTITASK LANGUAGE UNDERSTANDING{cite}`hendrycks2021measuringmassivemultitasklanguage`.\n",
    "```\n",
    "\n",
    "To bridge the gap between the wide-ranging knowledge that models see during pretraining and the\n",
    "existing measures of success, we introduce a new benchmark for assessing models across a diverse\n",
    "set of subjects that humans learn. We design the benchmark (multiple-choice-questions) to measure knowledge acquired during\n",
    "pretraining by evaluating models exclusively in zero-shot and few-shot settings. This makes the\n",
    "benchmark more challenging and more similar to how we evaluate humans. The benchmark covers\n",
    "57 subjects across STEM, the humanities, the social sciences, and more. It ranges in difficulty from\n",
    "an elementary level to an advanced professional level, and it tests both world knowledge and problem\n",
    "solving ability. Subjects range from traditional areas, such as mathematics and history, to more specialized areas like law and ethics. The granularity and breadth of the\n",
    "subjects makes the benchmark ideal for identifying a modelâ€™s blind spots.\n",
    "\n",
    "```{figure} ../images/mmlu-1.png\n",
    "---\n",
    "height: 500px\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584b5942-051b-4eb6-b0de-862a7ef0a0b7",
   "metadata": {},
   "source": [
    "## MMLU-Redux\n",
    "\n",
    "[MMLU-Redux](https://github.com/aryopg/mmlu-redux) is a carefully annotated version of the MMLU (Massive Multitask Language Understanding) dataset to provide a more accurate and reliable benchmark for evaluating the performance of language models.\n",
    "\n",
    "MMLU-Redux consists of 30 MMLU subjects, each containing 100 randomly sampled questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d9c5c8-5727-4809-aef4-a3e412bdcd64",
   "metadata": {},
   "source": [
    "## MMLU-Pro\n",
    "\n",
    "We introduce [MMLU-Pro](https://github.com/TIGER-AI-Lab/MMLU-Pro), an enhanced benchmark designed to evaluate language understanding models across broader and more challenging tasks. Building on the Massive Multitask Language Understanding (MMLU) dataset, MMLU-Pro integrates more challenging, reasoning-focused questions and increases the answer choices per question from four to ten, significantly raising the difficulty and reducing the chance of success through random guessing. MMLU-Pro comprises over 12,000 rigorously curated questions from academic exams and textbooks, spanning 14 diverse domains including Biology, Business, Chemistry, Computer Science, Economics, Engineering, Health, History, Law, Math, Philosophy, Physics, Psychology, and Others.\n",
    "\n",
    "Our experimental results show that MMLU-Pro not only raises the challenge, causing a significant drop in accuracy by 16% to 33% compared to MMLU but also demonstrates greater stability under varying prompts. With 24 different prompt styles tested, the sensitivity of model scores to prompt variations decreased from 4-5% in MMLU to just 2% in MMLU-Pro. Additionally, we found that models utilizing Chain of Thought (CoT) reasoning achieved better performance on MMLU-Pro compared to direct answering, which starkly contrasts the findings on the original MMLU, indicating that MMLU-Pro includes more complex reasoning questions.\n",
    "\n",
    "```{figure} ../images/mmlu-2.png\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
