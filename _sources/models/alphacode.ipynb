{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3512f4d2-deb0-4f2d-8dd0-e7c8cc3eb18f",
   "metadata": {},
   "source": [
    "# AlphaCode\n",
    "\n",
    "```{note}\n",
    "In simulated evaluations on recent programming competitions on the Codeforces platform,\n",
    "AlphaCode achieved on average a ranking of top 54.3% in competitions with more than 5,000 participants.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbb8ce1-07a8-4fee-af7b-05da4a111521",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "The metric we use is “percentage of problems solved using $n$ submissions from $k$\n",
    "samples per problem”, denoted as $n@k$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac18695-a5dc-4331-bc77-d63d186a24a0",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "Our pre-training dataset is based on a snapshot of selected public GitHub repositories.\n",
    "\n",
    "Models pre-trained on GitHub can generate good code and solve simple programming problems, but they can solve very few competitive programming problems.\n",
    "\n",
    "To facilitate fine-tuning and evaluation, we curated a new dataset of competitive programming\n",
    "problems, named **CodeContests**. The dataset includes problems, solutions and test cases we scraped\n",
    "from the Codeforces platform, along with existing public competitive programming datasets mixed\n",
    "into our training set.\n",
    "\n",
    "![](../images/alpha2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7121347-9aef-4102-821e-f1daf6e8aa5d",
   "metadata": {},
   "source": [
    "## Approach\n",
    "\n",
    "AlphaCode chooses an encoder-decoder transformer architecture. We pre-trained our models on the GitHub dataset with a standard cross-entropy next-token prediction loss for the decoder and a masked language modeling loss for the encoder. The masked language modeling loss was essential for improving the representation\n",
    "learning of the encoder.\n",
    "\n",
    "![](../images/alpha3.png)\n",
    "\n",
    "### Fine-tuning\n",
    "\n",
    "We fine-tuned our model on our CodeContests dataset. During fine-tuning, we used the natural\n",
    "language problem description for the encoder and the program solution for the decoder. Similar to\n",
    "pre-training, we used both the standard next-token prediction and masked language modeling losses.\n",
    "We also adopted additional conditioning and modifications that we found improved the overall solve\n",
    "rate: tempering, value conditioning and prediction, and GOLD.\n",
    "\n",
    "**Tempering.** We use $T=0.2 < 1$ at training. At sampling time, we divided the logits by another temperature $T'$ tuned on the validation set.\n",
    "\n",
    "**Value conditioning & prediction.** CodeContests contains both correct and incorrect problem\n",
    "submissions. We used value conditioning and prediction to discriminate between these two types of\n",
    "submissions. In value conditioning, we inserted whether or not a submission was correct into the problem description so that the model can condition on this information, At sampling time, the model was always conditioned on the sample being correct.\n",
    "\n",
    "![](../images/alpha4.png)\n",
    "\n",
    "In value prediction, we added an auxiliary value prediction\n",
    "task during training such that the last layer token representations before projecting to logits are also\n",
    "used in a small Transformer to classify whether the submission is correct. Value prediction was not used during sampling. \n",
    "\n",
    "**GOLD.** CodeContests contains several orders\n",
    "of magnitude more solutions than descriptions. Standard maximum likelihood objectives\n",
    "minimise loss by putting some weight on each solution in the training set, whereas\n",
    "our metric measures whether a model can find a single correct solution in the submission. To resolve this discrepancy, we adopted a variation of GOLD, an offline RL algorithm which **allows the model to both learn from tokens it already assigns\n",
    "high likelihood to, and to ignore tokens that are not in its distribution**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7df6d0c-a2dc-40b2-9332-af984ccd7a65",
   "metadata": {},
   "source": [
    "### Large scale sampling\n",
    "\n",
    "Sampling from transformer models can be easily parallelized, which allowed us to scale to **millions\n",
    "of samples per problem** – a critical driving force for performance improvement. To ensure sufficient\n",
    "diversity in such a large number of samples, we:\n",
    "\n",
    "1. generate half of the samples in Python and half in C++,\n",
    "2. randomize the problem tags and ratings in the natural language prompt,\n",
    "3. use a relatively high sampling temperature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ef2af3-d82e-4d7c-a5f1-42bcc481e301",
   "metadata": {},
   "source": [
    "### Filtering\n",
    "\n",
    "One powerful tool for\n",
    "selecting these submissions is filtering samples to only those that pass the example tests given in\n",
    "the problem statement.\n",
    "\n",
    "### Clustering\n",
    "\n",
    "Filtering using example tests can still leave thousands of candidate programs per problem. Randomly\n",
    "picking from this pool wastes the limited submission budget on programs that are syntactically\n",
    "different but semantically equivalent. Semantically equivalent programs could be detected if we had\n",
    "additional test inputs, by executing all remaining programs on these inputs and grouping programs\n",
    "that produce the same outputs together into clusters. We could then avoid repeatedly picking from\n",
    "the same clusters.\n",
    "\n",
    "After clustering on program behaviour we found that selecting one solution from each cluster from\n",
    "largest to smallest performed best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba515aa-946a-411a-9660-61d298e68397",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
