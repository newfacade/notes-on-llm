{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed792666-3a71-489c-9f4c-7d10052bfd73",
   "metadata": {},
   "source": [
    "# DeepSeekMoE\n",
    "\n",
    "## Preliminaries: Mixture-of-Experts for Transformers\n",
    "\n",
    "A standard Transformer language model is constructed by stacking $L$ layers of standard\n",
    "Transformer blocks, where each block can be represented as follows:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{u}_{1:T}^{l} &= \\text{Self-Att}(\\mathbf{h}_{1:T}^{l-1}) + \\mathbf{h}_{1:T}^{l-1}\\\\\n",
    "\\mathbf{h}_{t}^{l} &= \\text{FFN}(\\mathbf{u}_{t}^{l}) + \\mathbf{u}_{t}^{l}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $T$ denotes the sequence length, $\\text{Self-Att}(·)$ denotes the self-attention module, $\\text{FFN}(·)$\n",
    "denotes the Feed-Forward Network (FFN), $\\mathbf{u}_{1:T}^{l}\\in\\mathbb{R}^{T\\times d}$ are the hidden states of all tokens after\n",
    "the $l$-th attention module, and $\\mathbf{h}_{t}^{l}\\in\\mathbb{R}^{d}$ is the output hidden state of the $t$-th token after the $l$-th Transformer block. We omit the layer normalization in the above formulations for brevity.\n",
    "\n",
    "A typical practice to construct an MoE language model usually substitutes FFNs in a Transformer\n",
    "with MoE layers. An MoE layer is composed of multiple experts, where each expert is\n",
    "structurally identical to a standard FFN. Then, each token will be assigned to one or two experts. If the $l$-th FFN is substituted with an MoE layer:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{h}_{t}^{l} &= \\sum_{i=1}^{N}(g_{i,t}\\text{FFN}_{i}(\\mathbf{u_{t}^{l}})) + \\mathbf{u}_{t}^{l}\\\\\n",
    "g_{i,t} &= \n",
    "\\begin{cases}\n",
    "s_{i,t},\\quad &s_{i,t}\\in\\text{Topk}(\\{s_{j,t}|1\\le j\\le N\\}, K)\\\\\n",
    "0, &\\text{otherwise}\n",
    "\\end{cases}\\\\\n",
    "s_{i,t} &= \\text{Softmax}_{i}({\\mathbf{u}_{t}^{l}}^{\\intercal}\\mathbf{e}_{i}^{l})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $N$ denotes the total number of experts, $\\text{FFN}_{i}$ is the $i$-th expert FFN, $g_{i,t}$ denotes the\n",
    "gate value for the $i$-th expert, $s_{i,t}$ denotes the token-to-expert affinity, $\\text{Topk}(\\cdot,K)$ denotes the set\n",
    "comprising $K$ highest affinity scores among those calculated for the $t$-th token and all $N$ experts, and $\\mathbf{e}_{i}^{l}$ is the centroid of the $i$-th expert in the $l$-th layer (parameter of the gate).\n",
    "\n",
    "## Fine-Grained Expert Segmentation\n",
    "\n",
    "While maintaining a consistent number of expert parameters and\n",
    "computational cost, we segment the experts with a finer grain. The finer expert segmentation\n",
    "enables a more flexible and adaptable combination of activated experts.\n",
    "\n",
    "To be specific, we segment each expert FFN into $m$ smaller\n",
    "experts by reducing the FFN intermediate hidden dimension to $\\frac{1}{m}$ times its original size. Since\n",
    "each expert becomes smaller, in response, we also increase the number of activated experts to\n",
    "$m$ times to keep the same computation cost.\n",
    "\n",
    "![](../images/deepseek-moe.png)\n",
    "\n",
    "## Shared Expert Isolation\n",
    "\n",
    "Tokens assigned to different experts may necessitate some\n",
    "common knowledge or information. As a result, multiple experts may converge in acquiring\n",
    "shared knowledge in their respective parameters, thereby resulting in redundancy in expert\n",
    "parameters. However, if there are shared experts dedicated to capturing and consolidating\n",
    "common knowledge across varying contexts, the parameter redundancy among other routed\n",
    "experts will be alleviated.\n",
    "\n",
    "Towards this objective, in addition to the fine-grained expert segmentation strategy, we\n",
    "further isolate $K_{s}$ experts to serve as shared experts. In order to maintain a constant\n",
    "computational cost, the number of activated experts among the other routed experts will be\n",
    "decreased by $K_{s}$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{h}_{t}^{l} &= \\sum_{i=1}^{K_s}\\text{FFN}_{i}(\\mathbf{u}_{t}^{l}) + \\sum_{i=K_s}^{mN}(g_{i,t}\\text{FFN}_{i}(\\mathbf{u_{t}^{l}})) + \\mathbf{u}_{t}^{l}\\\\\n",
    "g_{i,t} &= \n",
    "\\begin{cases}\n",
    "s_{i,t},\\quad &s_{i,t}\\in\\text{Topk}(\\{s_{j,t}|K_s\\le j\\le mN\\}, mK-K_s)\\\\\n",
    "0, &\\text{otherwise}\n",
    "\\end{cases}\\\\\n",
    "s_{i,t} &= \\text{Softmax}_{i}({\\mathbf{u}_{t}^{l}}^{\\intercal}\\mathbf{e}_{i}^{l})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "## Load Balance Consideration\n",
    "\n",
    "Automatically learned routing strategies may encounter the issue of load imbalance.\n",
    "\n",
    "**Expert-Level Balance Loss.** Imbalance leeds to higher loss.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathcal{L}_{\\text{ExpBal}} &= \\alpha_{1}\\sum_{i=1}^{mN-K_s}f_{i}P_{i}\\\\\n",
    "f_{i} &= \\frac{mN-K_{s}}{(mK-K_s)T}\\sum_{t=1}^{T}\\mathbb{1}(\\text{Token }t\\text{ selects Expert }i)\\\\\n",
    "P_{i} &= \\frac{1}{T}\\sum_{t=1}^{T}s_{i,t}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**Device-Level Balance Loss.** In addition to the expert-level balance loss, we additionally\n",
    "design a device-level balance loss to ensure balanced computation across different devices. If we partition all routed experts into $D$ groups\n",
    "$\\{\\mathcal{E}_{1}, \\mathcal{E}_{2}, \\dots, \\mathcal{E}_{D}\\}$, and deploy each group on a single device, the device-level balance loss is\n",
    "computed as follows:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathcal{L}_{\\text{DevBal}} &= \\alpha_{2}\\sum_{i=1}^{D}f_{i}'P_{i}'\\\\\n",
    "f_{i}' &= \\frac{1}{|\\mathcal{E}_{i}|}\\sum_{j\\in\\mathcal{E}_{i}}f_{j}\\\\\n",
    "P_{i}' &= \\sum_{j\\in\\mathcal{E}_{i}}P_{j}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60608046-d0a2-4a54-9984-996830c0516e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
