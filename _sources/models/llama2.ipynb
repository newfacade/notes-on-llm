{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0b5dbbf-a572-4ffc-859c-3c0f5e511e53",
   "metadata": {},
   "source": [
    "# Llama 2\n",
    "\n",
    "```{note}\n",
    "Llama 2 is a collection of pretrained and fine-tuned LLMs ranging from 7b to 70b parameters. The fine-tuned LLMs, called Llama2-chat, are optimized for dialogue use cases.\n",
    "```\n",
    "\n",
    "## Pretrain\n",
    "\n",
    "![](../images/llama2-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b4e684-44a0-426d-8241-8a4d36f74b9b",
   "metadata": {},
   "source": [
    "## SFT\n",
    "\n",
    "Llama 2-Chat is the result of several months of research and iterative applications of alignment techniques, including both instruction tuning and RLHF, requiring significant computational and annotation resources.\n",
    "\n",
    "**Quality Is All You Need.** A limited set of clean instruction-tuning\n",
    "data can be sufficient to reach a high level of quality. We found that SFT annotations in the order of tens of thousands was enough to achieve a high-quality result. We stopped annotating SFT after collecting a total of 27,540 annotations.\n",
    "\n",
    "```{caution}\n",
    "To ensure the model sequence\n",
    "length is properly filled, we concatenate all the prompts and answers from the training set. A special token is\n",
    "utilized to separate the prompt and answer segments. We utilize an autoregressive objective and zero-out\n",
    "the loss on tokens from the user prompt, so as a result, we backpropagate only on answer tokens. Finally, we\n",
    "fine-tune the model for 2 epochs.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e119b8d-c169-4a6a-a5e9-543d23ad6269",
   "metadata": {},
   "source": [
    "## RLHF\n",
    "\n",
    "### Human Preference Data Collection\n",
    "\n",
    "We ask annotators to first write a prompt, then choose\n",
    "between two sampled model responses, based on provided criteria. we also ask annotators to label the degree to which they prefer their chosen response over the alternative: either their choice is significantly better, better, slightly better, or negligibly better/ unsure.\n",
    "\n",
    "As we collected more preference data, our\n",
    "reward models improved, and we were able to train progressively better versions for Llama 2-Chat. It is important before a new Llama 2-Chat tuning iteration to\n",
    "gather new preference data using the latest Llama 2-Chat iterations. This step helps keep the reward model\n",
    "on-distribution and maintain an accurate reward for the latest model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6669c86-0ed5-4bcb-85d1-ab5af90a7765",
   "metadata": {},
   "source": [
    "### Reward Modeling\n",
    "\n",
    "We train two separate reward\n",
    "models, one optimized for helpfulness (referred to as Helpfulness RM) and another for safety. We initialize our reward models from pretrained chat model checkpoints, as it ensures that both models\n",
    "benefit from knowledge acquired in pretraining.\n",
    "\n",
    "**Training Objectives.**  To train the reward model, we use the collected pairwise human preference data and a binary ranking loss:\n",
    "\n",
    "$$\\mathcal{L}_{\\text{ranking}} = -\\log(\\sigma(r_{\\theta}(x, y_{c}) - r_{\\theta}(x, y_{r})))$$\n",
    "\n",
    "where $r_{\\theta}(x, y)$ is a scalar score output for prompt $x$ and completion $y$ with model weight $\\theta$. $y_{c}$ is the preferred response and $y_{r}$ is the rejected counterpart.\n",
    "\n",
    "Built on top of this binary ranking loss, given that our preference ratings is decomposed as a scale of four points (e.g., significantly better), it can be useful to leverage this information to explicitly\n",
    "teach the reward model to assign more discrepant scores to the generations that have more differences. To\n",
    "do so, we further add a margin component in the loss:\n",
    "\n",
    "$$\\mathcal{L}_{\\text{ranking}} = -\\log(\\sigma(r_{\\theta}(x, y_{c}) - r_{\\theta}(x, y_{r}) - m(r)))$$\n",
    "\n",
    "where the margin $m(r)$ is a discrete function of the preference rating."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca3a884-ff0a-42d2-8619-4008e58b7e40",
   "metadata": {},
   "source": [
    "### Iterative Fine-Tuning\n",
    "\n",
    "As we received more batches of human preference data annotation, we were able to train better reward\n",
    "models and collect more prompts. We therefore trained successive versions for RLHF models, referred to\n",
    "here as RLHF-V1, . . . , RLHF-V5.\n",
    "\n",
    "We explored RLHF fine-tuning with two main algorithms:\n",
    "\n",
    "**Rejection Sampling fine-tuning (RS).** At each iterative stage, we sample K answers for each prompt from the most recent model. We score each\n",
    "sample given the best reward model accessible at the time of the experiment, and then select the best answer for a given prompt. \n",
    "\n",
    "In earlier versions of our model, up to RLHF V3, our approach was to confine answer\n",
    "selection solely to the “bag” of samples gathered from the preceding iteration. For example, RLHF V3 was\n",
    "trained using only samples from RLHF V2. However, this method led to a regression in some capabilities. In response, on subsequent iterations, we modified our strategy, incorporating top-performing samples from\n",
    "all prior iterations, such as those used in RLHF-V1 and RLHF-V2.\n",
    "\n",
    "We illustrate the benefit of Rejection Sampling in Figure 7. The delta between the maximum and median\n",
    "curves can be interpreted as the potential gain of fine-tuning on the best output.\n",
    "\n",
    "![](../images/llama2-rejective.png)\n",
    "\n",
    "We perform rejection sampling only with our largest 70B Llama 2-Chat. All smaller\n",
    "models are fine-tuned on rejection sampled data from the larger model, thus distilling the large-model\n",
    "capabilities into the smaller ones.\n",
    "\n",
    "**PPO.** We further train our language model following the RL scheme which uses the\n",
    "reward model as an estimate for the true reward function (human preference) and the pretrained language\n",
    "model as the policy to optimize. During this phase, we seek to optimize the following objective:\n",
    "\n",
    "$$\\arg\\underset{\\pi}{\\max} \\mathbb{E}_{p\\sim\\mathcal{D},g\\sim\\pi}[R(g|p)]$$\n",
    "\n",
    "We iteratively improve the policy by sampling prompts $p$ from our dataset $\\mathcal{D}$ and generations $g$ from the\n",
    "policy $\\pi$ and use the PPO algorithm and loss function to achieve this objective.\n",
    "\n",
    "The final reward function we use during optimization:\n",
    "\n",
    "$$R(g|p) = \\tilde{R}_{c}(g|p) - \\beta D_{KL}(\\pi_{\\theta}(g|p) \\| \\pi_{0}(g|p))$$\n",
    "\n",
    "contains a penalty term for diverging from the original policy $\\pi_{0}$. We find this constraint is useful for training stability, and to reduce reward\n",
    "hacking whereby we would achieve high scores from the reward model but low scores from human evaluation.\n",
    "\n",
    "We define $R_{c}$ to be a piecewise combination of the safety $(R_{s})$ and helpfulness $(R_h)$ reward models. We also find it important to whiten (to transform input data so that it has zero mean and unit variance)\n",
    "the final linear scores:\n",
    "\n",
    "$$\n",
    "R_{c}(g|p) = \n",
    "\\begin{cases}\n",
    "R_{s}(g|p)\\quad &\\text{if IS_SAFETY($p$) or }R_{s}(g|p)<0.15\\\\\n",
    "R_{h}(g|p)\\quad &\\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\tilde{R}_{c}(g|p) = \\text{WHITEN}(\\text{LOGIT}(R_{c}(g|p)))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29698d02-7ded-4307-8d16-90d838106b92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
