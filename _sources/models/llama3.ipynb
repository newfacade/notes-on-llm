{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94b7c2dd-f4ef-4692-9efe-6d3ab3abaacd",
   "metadata": {},
   "source": [
    "# Llama 3\n",
    "\n",
    "```{note}\n",
    "This paper presents a\n",
    "new set of foundation models, called [Llama 3](https://arxiv.org/abs/2407.21783). It is a herd of language models that natively support\n",
    "multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with\n",
    "405B parameters and a context window of up to 128K tokens. We find that Llama 3 delivers comparable quality to leading language\n",
    "models such as GPT-4 on a plethora of tasks.\n",
    "```\n",
    "\n",
    "```{figure} ../images/llama3-1.png\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4618e0eb-0de5-49df-8f41-316dff51cae2",
   "metadata": {},
   "source": [
    "## Post-Training\n",
    "\n",
    "We produce the aligned Llama 3 models by applying several rounds of post-training. Each\n",
    "round of post-training involves supervised finetuning (SFT) followed by Direct Preference Optimization (DPO{cite}`rafailov2024directpreferenceoptimizationlanguage`) on examples collected either via human annotations or generated synthetically.\n",
    "\n",
    "### Modeling\n",
    "\n",
    "```{figure} ../images/llama3-2.png\n",
    "```\n",
    "\n",
    "#### Chat Dialog Format\n",
    "\n",
    "To tune LLMs for human-AI interaction, we need to define a chat dialog protocol for the model to understand\n",
    "human instructions and perform conversational tasks.\n",
    "\n",
    "```{figure} ../images/llama3-tokenizer.svg\n",
    "```\n",
    "\n",
    "#### Reward Modeling\n",
    "\n",
    "We train a reward model (RM) covering different capabilities on top of the pre-trained checkpoint. We use `all` of our preference data for reward\n",
    "modeling after filtering out samples with similar responses.\n",
    "\n",
    "#### Supervised Finetuning\n",
    "\n",
    "The reward model is then used to perform rejection sampling on our human annotation prompts, Together with this rejection-sampled data and other data sources\n",
    "(including synthetic data), we finetune the pre-trained language model using a standard cross entropy loss\n",
    "on the target tokens (while masking loss on prompt tokens).\n",
    "\n",
    "#### Direct Preference Optimization\n",
    "\n",
    "We further train our SFT models with DPO for human\n",
    "preference alignment. For training, we primarily use the `most recent` batches of preference data collected using\n",
    "the best performing models from the previous alignment rounds.\n",
    "the following algorithmic modifications to DPO:\n",
    "\n",
    "* Masking out formatting tokens in DPO loss\n",
    "* Regularization with NLL loss (DPOP)\n",
    "\n",
    "#### Model Averaging\n",
    "\n",
    "Finally, we average models obtained from experiments using various versions of data or hyperparameters at\n",
    "each RM, SFT, or DPO stage.\n",
    "\n",
    "#### Iterative Rounds\n",
    "\n",
    "Following Llama 2, we apply the above methods in six rounds. In each cycle, we collect new preference\n",
    "annotations and SFT data, sampling synthetic data from the latest models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9622b25-4f39-41ec-a7f3-e6622d2b26f3",
   "metadata": {},
   "source": [
    "### Post-training Data\n",
    "\n",
    "#### Preference Data\n",
    "\n",
    "We deploy multiple models for annotation after\n",
    "each round and sample two responses from `two different models` for each user prompt. These models can\n",
    "be trained with different data mixes and alignment recipes, allowing for different capability strength (e.g.,\n",
    "code expertise) and increased data diversity. We ask `annotators` to rate the strength of their preference by\n",
    "categorizing it into one of four levels, based on how much more they prefer the chosen response over the\n",
    "rejected one: significantly better, better, slightly better, or marginally better. We also incorporate an editing\n",
    "step after preference ranking to encourage annotators to further improve the preferred response. Annotators\n",
    "edit the chosen response directly or prompt the model with feedback to refine its own response. Consequently,\n",
    "a portion of our preference data has three responses ranked $(edited > chosen > rejected)$.\n",
    "\n",
    "```{caution}\n",
    "How to collect multi-turn preference data from two different models? One possible solution:<br>\n",
    "1. $query_1 \\to (chosen_1, rejected_1)$\n",
    "2. $answer_1 := chosen_1$\n",
    "3. $(query_1, answer_1, query_2) \\to (chosen_2, rejected_2)$\n",
    "```\n",
    "\n",
    "In each round of post-training, we use all the preference data that is available at the time for reward modeling,\n",
    "while only using the latest batches from various capabilities for DPO training. For both reward modeling and\n",
    "DPO, we use samples that are labeled as the chosen response being significantly better or better than the\n",
    "rejected counterpart for training and `discard samples with similar responses`.\n",
    "\n",
    "#### SFT Data\n",
    "\n",
    "Our finetuning data is largely comprised of the following sources:\n",
    "\n",
    "* Prompts from our human annotation collection with rejection-sampled responses\n",
    "* Synthetic data targeting specific capabilities (See [](llama3_capabilities) for more details)\n",
    "* Small amounts of human-curated data\n",
    "\n",
    "**Rejection sampling.** During rejection sampling (RS), for each prompt collected during human annotation we sample $K$ (typically between 10 and 30) outputs from the latest chat model policy (usually\n",
    "the best performing checkpoint from the previous post-training iteration, or the best performing checkpoint\n",
    "for a particular capability) and use our reward model to select the best candidate. \n",
    "\n",
    "```{tip}\n",
    "In later rounds of post-training, we introduce system prompts to steer RS responses to conform with\n",
    "desirable tone, style, or formatting, which might be different for different capabilities.\n",
    "```\n",
    "\n",
    "**Overall data composition.** In [](llama3_data_process) we describe techniques for categorizing topic, complexity, and quality of our data\n",
    "samples. In each round of post-training, we adjust our overall data mix carefully across these axes to tune\n",
    "performance across a wide range of benchmarks.\n",
    "\n",
    "(llama3_data_process)=\n",
    "#### Data Processing and Quality Control\n",
    "\n",
    "Given that most of our training data is model-generated, it requires careful cleaning and quality control.\n",
    "\n",
    "* **Topic classification:** We first finetune Llama 3 8B into a topic classifier, and perform inference over\n",
    "all data to classify it into both coarsely-grained buckets (“mathematical reasoning”) and fine-grained buckets (“geometry and trigonometry”).\n",
    "\n",
    "* **Quality scoring:** We use both reward model and Llama-based signals to obtain a quality score for each\n",
    "sample. For an RM-based score, we consider data that is in the top quartile of RM scores as high quality.\n",
    "For a Llama-based score, we prompt Llama 3 checkpoint to rate each sample on a three-point scale for\n",
    "general English data and a two-point scale\n",
    "for coding data (Bug Identification and User Intention), and consider samples that obtain the maximum\n",
    "score as high quality. The RM and Llama-based scores have high disagreement rates, and we find that\n",
    "combining these signals yield the best recall on our internal test set. Ultimately, we select examples\n",
    "that are marked as high quality by the RM `or` the Llama-based filter.\n",
    "\n",
    "* **Difficulty scoring:** Because we are also interested in prioritizing examples that are more complex for\n",
    "the model, we score data using two measures of difficulty: Instag{cite}`lu2023instaginstructiontagginganalyzing` and Llama-based\n",
    "scoring.\n",
    "\n",
    "* **Semantic deduplication:** Finally, we perform semantic deduplication. We first cluster complete dialogs using RoBERTa and within each cluster\n",
    "sort them by quality score $\\times$ difficulty score. We then do greedy selection by iterating through all sorted\n",
    "examples, and only keeping the ones that have maximum cosine similarity less than a threshold to the\n",
    "examples seen so far in the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f04470-7311-42cf-8410-d79aac66c610",
   "metadata": {},
   "source": [
    "(llama3_capabilities)=\n",
    "### Capabilities\n",
    "\n",
    "#### Code\n",
    "\n",
    "Here, we present our work on improving coding capabilities via training a code expert, generating synthetic data for SFT, improving formatting\n",
    "with system prompt steering, and creating quality filters to remove bad samples from our training data.\n",
    "\n",
    "**Expert training.** We train a code expert which we use to collect high quality human annotations for code\n",
    "throughout subsequent rounds of post-training. This is accomplished by branching the main pre-training run\n",
    "and continuing pre-training on a 1T token mix of mostly (>85%) code data. For the last several\n",
    "thousand steps of training we perform long-context finetuning (LCFT) to extend the expert’s context length\n",
    "to 16K tokens on a high quality mix of repo-level code data. Finally, we follow the similar post-training\n",
    "modeling recipes described earlier to align this model, except with SFT and DPO data mixes primarily\n",
    "targeting code.\n",
    "\n",
    "```{tip}\n",
    "The code expert model is used for both preference data annotation (recall that responses of preference data are sampled from two different models) and rejection sampling.\n",
    "```\n",
    "\n",
    "**Synthetic data generation.** We use Llama 3 and the code expert to generate a large quantity of synthetic SFT dialogs. We describe three high-level approaches for generating synthetic code data used during SFT.\n",
    "\n",
    "1. **Synthetic data generation: execution feedback.** The 8B and 70B models show significant performance\n",
    "improvements when trained on data generated by a larger, more competent model. However, our initial\n",
    "experiments revealed that training Llama 3 405B on its own generated data is not helpful (and can\n",
    "even degrade performance). To address this limitation, we introduced execution feedback as a source of\n",
    "truth, in particular, we generate large\n",
    "dataset of approximately one million synthetic coding dialogues using the following process:\n",
    "\n",
    "    * **Problem description generation:** Magicoder{cite}`wei2024magicoderempoweringcodegeneration`.\n",
    "    \n",
    "    * **Solution generation:** Then, we prompt Llama 3 to solve each problem in a given programming language.\n",
    "      ```{tip}\n",
    "      We observe that adding general rules of good programming to the prompt improves the generated solution quality. Also, we find it is helpful to require the model to explain its thought process in comments.\n",
    "      ```\n",
    "\n",
    "    * **Correctness analysis:** We extract the source code from the generated solution and applied a combination of static and dynamic analysis techniques to test its correctness, including:\n",
    "  \n",
    "        * **Static analysis:** We run all generated code through a parser and a linter to ensure syntactic correctness.\n",
    "        \n",
    "        * **Unit test generation and execution:** For each problem and solution, we prompt the model to generate unit tests, executed in a containerized environment together with the solution, catching run-time execution errors and some semantic errors.\n",
    "\n",
    "     * **Error feedback and iterative self-correction:** When a solution fails at any step, we prompt the model to revise it. The prompt included the original problem description, the faulty solution, and feedback from the parser/linter/tester. After a unit test execution failure, the model could either fix the code to pass the existing tests or modify its unit tests to accommodate the generated code. Only dialogs that pass all checks are included in the final dataset, used for supervised finetuning (SFT).\n",
    "  \n",
    "     * **Fine-tuning and iterative improvement:** The finetuning process is conducted over multiple rounds, with each round building on the previous one. After each round, the model is improved, generating higher-quality synthetic data for the next round.\n",
    "  \n",
    "2. **Synthetic data generation: programming language translation.** We observe a performance gap between\n",
    "major programming languages (e.g., Python/C++) and less common ones (e.g., Typescript/PHP). This\n",
    "is not surprising as we have less training data for less common programming languages. To mitigate\n",
    "this, we supplement our existing data by translating data from common programming languages to\n",
    "less common languages. This is achieved\n",
    "by prompting Llama 3 and ensuring quality via syntax parsing, compilation, and execution.\n",
    "\n",
    "3. **Synthetic data generation: backtranslation.** To improve certain coding capabilities (e.g., documentation,\n",
    "explanations and debugging) where execution feedback is less informative for determining quality, we employ an\n",
    "alternative multi-step approach. Beginning with code\n",
    "snippets from a variety of languages in our pre-training data:\n",
    "\n",
    "    * **Generate:** We prompt Llama 3 to generate data that represents our target capability (e.g., we ask the model to explain a piece of code).\n",
    "    \n",
    "    * **Backtranslate:** We then prompt the model to “backtranslate” the synthetically generated data to the original code (e.g., we ask the model to generate code only from its explanation).\n",
    "  \n",
    "    * **Filter:** Using the original code as a reference, we prompt the Llama 3 to determine the quality of the output (e.g., we ask the model how faithful the backtranslated code is to the original). We then use the generated examples that have the highest self-verification scores in SFT.\n",
    "  \n",
    "**System prompt steering during rejection sampling.** During the rejection sampling process, we used code specific\n",
    "system prompts to improve code readability, documentation, thoroughness, and specificity.\n",
    "\n",
    "```{figure} ../images/llama3-3.png\n",
    "```\n",
    "\n",
    "**Filtering training data with execution and model-as-judge signals.** We occasionally\n",
    "encounter quality issues in our rejection-sampled data. Detecting these\n",
    "issues in our rejection-sampled data is not as straightforward as it is for our synthetic code data, as the\n",
    "rejection-sampled responses typically contain a mix of natural language and code for which the code may not always be expected to be executable. To address this, we utilize the “model-as-judge” approach,\n",
    "where earlier versions of Llama 3 assess and assign a binary (0/1) score based on two criteria: code correctness\n",
    "and code style. We retain only those samples that achieve a perfect score of 2. Initially, this stringent filtering\n",
    "led to a regression in downstream benchmark performance, primarily because it disproportionately `removed\n",
    "examples with challenging prompts`. To counteract this, we strategically revise the responses of some coding\n",
    "data categorized as most challenging until they met the Llama-based “model-as-judge” criteria. By refining\n",
    "these challenging problems, the coding data achieves a balance between quality and difficulty, resulting in\n",
    "optimal downstream performance.\n",
    "\n",
    "```{caution}\n",
    "How to revise the responses automatically?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8b4f72-aa6f-4885-93be-7612d4562345",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "### Pre-trained Language Model\n",
    "\n",
    "```{figure} ../images/llama3-4.png\n",
    "---\n",
    "height: 400px\n",
    "---\n",
    "```\n",
    "```{figure} ../images/llama3-5.png\n",
    "```\n",
    "```{figure} ../images/llama3-6.png\n",
    "```\n",
    "\n",
    "### Post-trained Language Model\n",
    "\n",
    "```{figure} ../images/llama3-7.png\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c950d4d9-7a03-4fa4-8f71-4f46871b974a",
   "metadata": {},
   "source": [
    "## Takeaway\n",
    "\n",
    "```{note}\n",
    "Preference data:\n",
    "* Response from two different models with different data mixes and alignment recipes\n",
    "* Human annotate significantly better, better, slightly better, or marginally better. Only use significantly better and better.\n",
    "* Discard samples with similar responses\n",
    "\n",
    "SFT data:\n",
    "* Rejection sampling using RM, system prompt steering during rejection sampling.\n",
    "* Synthetic data: \n",
    "    * Execution feedback (Magicoder)\n",
    "    * Programming language translation\n",
    "    * Backtranslation (code explain, debug)\n",
    "\n",
    "Data process and quality control:\n",
    "* Quality scoring\n",
    "    * LLM as judge (for challenging prompts, revise responses until met criteria)\n",
    "    * Or top quartile of RM\n",
    "* Difficulty scoring\n",
    "* Semantic deduplication\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93e83a8-cbf6-405c-a2a6-75b95392b962",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
