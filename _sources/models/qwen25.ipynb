{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2c057ba-304e-4360-afab-7be0e04a3f5b",
   "metadata": {},
   "source": [
    "# Qwen 2.5\n",
    "\n",
    "```{note}\n",
    "Compared to previous iterations, Qwen 2.5 has\n",
    "been significantly improved during both the pre-training and post-training stages. In\n",
    "terms of pre-training, we have scaled the high-quality pre-training datasets from the\n",
    "previous 7 trillion tokens to 18 trillion tokens. In terms of post-training,\n",
    "we implement intricate supervised finetuning with over 1 million samples, as well as\n",
    "multistage reinforcement learning, including `offline learning DPO` and `online learning\n",
    "GRPO`.<br>\n",
    "To handle diverse and varied use cases effectively, we present Qwen2.5 LLM series in rich\n",
    "configurations. Specifically, the open-weight flagship Qwen2.5-72B-Instruct outperforms\n",
    "a number of open and proprietary models and demonstrates competitive performance to\n",
    "the state-of-the-art open-weight model, Llama-3-405B-Instruct, which is around 5 times\n",
    "larger.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5aecd7-d3f7-4a4b-b6d2-7c9dec8b0b00",
   "metadata": {},
   "source": [
    "## Architecture & Tokenizer\n",
    "\n",
    "Basically, the Qwen2.5 series include dense models for opensource, and MoE models for API service.\n",
    "\n",
    "For dense models, we maintain the Transformer-based decoder architecture as Qwen2. The architecture incorporates several key components:\n",
    "Grouped Query Attention (GQA) for efficient KV cache utilization, SwiGLU activation\n",
    "function for non-linear activation, Rotary Positional Embeddings (RoPE) for encoding position information, QKV bias in the attention mechanism and\n",
    "RMSNorm with pre-normalization to ensure stable training.\n",
    "\n",
    "Building upon the dense model architectures, we extend it to MoE model architectures. This is achieved\n",
    "by replacing standard feed-forward network (FFN) layers with specialized MoE layers.\n",
    "\n",
    "For tokenization, we utilize Qwen’s tokenizer, which implements byte-level byte-pair\n",
    "encoding (BBPE) with a vocabulary of 151,643\n",
    "regular tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c750024-c765-4758-abe4-1aa6cf6dd5ce",
   "metadata": {},
   "source": [
    "## Pre-training\n",
    "\n",
    "### Pre-training Data\n",
    "\n",
    "Qwen2.5 demonstrates significant enhancements in pre-training data quality compared to its predecessor\n",
    "Qwen2. These improvements stem from several key aspects:\n",
    "\n",
    "1. Better data filtering.\n",
    "2. Better math and code data.\n",
    "3. Better synthetic data.\n",
    "4. Better data mixture.\n",
    "\n",
    "### Scaling Law for Hyper-parameters\n",
    "\n",
    "We develop scaling laws for hyper-parameter based on the pre-training data of Qwen2.5. While previous studies primarily used scaling laws to determine optimal model sizes given compute budgets, we\n",
    "leverage them to identify optimal hyperparameters across model architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a71d68e-c441-4ffb-82c2-5d007f019dbc",
   "metadata": {},
   "source": [
    "## Post-training\n",
    "\n",
    "Qwen 2.5 introduces two significant advancements in its post-training design compared to Qwen 2:\n",
    "\n",
    "1. Expanded Supervised Fine-tuning Data Coverage: The supervised fine-tuning process leverages\n",
    "a massive dataset comprising millions of high-quality examples.\n",
    "2. Two-stage Reinforcement Learning: The reinforcement learning (RL) process in Qwen 2.5 is\n",
    "divided into two distinct stages: Offline RL and Online RL.\n",
    "    * Offline RL: This stage focuses on developing capabilities that are challenging for the reward\n",
    "model to evaluate, such as reasoning, factuality, and instruction-following.\n",
    "    * Online RL: The Online RL phase leverages the reward model’s ability to detect nuances in\n",
    "output quality.\n",
    "\n",
    "### Supervised Fine-tuning\n",
    "\n",
    "In this section, we detail the key enhancements made during the SFT phase of Qwen2.5, focusing on\n",
    "several critical areas:\n",
    "\n",
    "**Coding:** To enhance coding capabilities, we incorporate the instruction tuning data of Qwen2.5-\n",
    "Coder. We use multiple language-specific agents into a collaborative framework,\n",
    "generating diverse and high-quality instruction pairs across nearly 40 programming languages.\n",
    "We expand our instruction dataset by `synthesizing new examples from code-related Q&A\n",
    "websites` and gathering `algorithmic code snippets from GitHub`. A `comprehensive multilingual\n",
    "sandbox` is used to perform static code checking and validate code snippets through automated\n",
    "unit testing, ensuring code quality and correctness.\n",
    "\n",
    "### Offline Reinforcement Learning\n",
    "\n",
    "Compared to Online Reinforcement Learning (RL), Offline RL enables the pre-preparation of training\n",
    "signals, which is particularly advantageous for tasks where standard answers exist but are challenging to\n",
    "evaluate using reward model. In this study, we focus on objective query domains such as mathematics,\n",
    "coding, instruction following, and logical reasoning, where obtaining accurate evaluations can be complex.\n",
    "In the previous phase, we extensively employ strategies like `execution feedback` and answer matching to\n",
    "ensure the quality of responses. For the current phase, we reuse that pipeline, employing the SFT model\n",
    "to resample responses for a new set of queries. Responses that pass our quality checks are used as positive\n",
    "examples, while those that fail are treated as negative examples for Direct Preference Optimization (DPO). To further enhance the reliability and accuracy of the training signals, we\n",
    "make use of both `human and automated review` processes.\n",
    "\n",
    "### Online Reinforcement Learning\n",
    "\n",
    "To develop a robust reward model for online RL, we adhere to a set of carefully defined labeling criteria:\n",
    "\n",
    "* Truthfulness\n",
    "* Helpfulness\n",
    "* Conciseness\n",
    "* Relevance\n",
    "* Harmlessness\n",
    "* Debiasing\n",
    "\n",
    "The queries utilized to train the reward model are drawn from two distinct datasets: `publicly available\n",
    "open-source data` and a `proprietary query set characterized by higher complexity`. Responses are generated\n",
    "from checkpoints of the Qwen models, which have been fine-tuned using different methods—SFT,\n",
    "DPO, and RL—at various stages of training. To introduce diversity, those responses are sampled at\n",
    "`different temperature settings`. Preference pairs are created through both human and automated labeling\n",
    "processes, and the training data for DPO is also integrated into this dataset.\n",
    "\n",
    "In our online reinforcement learning (RL) framework, we employ Group Relative Policy Optimization. `The query set utilized for training the reward model is identical to the one used\n",
    "in the RL training phase`. The sequence in which queries are processed during training is determined by\n",
    "the variance of their response scores, as evaluated by the reward model. Specifically, queries with higher\n",
    "variance in response scores are prioritized to ensure more effective learning. We sample 8 responses\n",
    "for each query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a098c2-d869-4f2a-854c-9ec72a4ea741",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "### Base Models\n",
    "\n",
    "```{figure} ../images/qwen25-1.png\n",
    "```\n",
    "\n",
    "### Instruction-tuned Model\n",
    "\n",
    "```{figure} ../images/qwen25-2.png\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63121fc-55f4-4d25-aea2-ff511c46974a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
