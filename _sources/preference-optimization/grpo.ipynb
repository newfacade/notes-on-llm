{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ce2f2a4-36c3-4ac8-8e98-cd1c02c97386",
   "metadata": {},
   "source": [
    "# Group Relative Policy Optimization (GRPO)\n",
    "\n",
    "```{note}\n",
    "Reinforcement learning (RL) has been proven to be effective in further improving the overall ability of LLMs after the Supervised Fine-Tuning (SFT) stage. DeepSeek Math introduces an RL algorithm, Group Relative Policy Optimization (GRPO), which has proven to be both efficient and effective.\n",
    "```\n",
    "\n",
    "Paper: https://arxiv.org/pdf/2402.03300"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fcd044-18c1-49cd-abef-aae895621d2a",
   "metadata": {},
   "source": [
    "## PPO Review\n",
    "\n",
    "Proximal Policy Optimization (PPO) is an actor-critic RL algorithm that is\n",
    "widely used in the RL fine-tuning stage of LLMs. In particular, it optimizes\n",
    "LLMs by maximizing the following surrogate objective:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "J_{PPO}(\\theta) = &\\underset{q\\sim P(Q),o\\sim\\pi_{\\theta_{\\text{old}}}(O|q)}{\\mathbb{E}}\\frac{1}{|o|}\\sum_{t=1}^{|o|}\\min\\\\\n",
    "&\\left[\\frac{\\pi_{\\theta}(o_{t}|q,o<t)}{\\pi_{\\theta_{\\text{old}}}(o_{t}|q,o<t)}A_{t}, \\text{clip}\\left(\\frac{\\pi_{\\theta}(o_{t}|q,o<t)}{\\pi_{\\theta_{\\text{old}}}(o_{t}|q,o<t)}, 1-\\epsilon, 1+\\epsilon\\right)A_{t}\\right],\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $\\pi_{\\theta}$ and $\\pi_{\\theta_{\\text{old}}}$ are the current and old policy models, and $q$, $o$ are questions and outputs\n",
    "sampled from the question dataset and the old policy $\\pi_{\\theta_{\\text{old}}}$ respectively. $\\epsilon$ s a clipping-related\n",
    "hyper-parameter introduced in PPO for stabilizing training. $A_{t}$ is the advantage, which is\n",
    "computed by applying Generalized Advantage Estimation (GAE), based on the rewards $\\{r\\ge t\\}$ and a learned value function $V_{\\psi}$.\n",
    "\n",
    "1. Compute per-token rewards:\n",
    "    * $\\text{KL}(t) = \\log({\\pi_{\\theta_{\\text{old}}}(o_{t}|q,o<t)}/{\\pi_{\\text{ref}}(o_{t}|q,o<t)})$\n",
    "    * If $t$ is not the last token $r_{t} = -\\beta\\text{KL}(t)$\n",
    "    * If $t$ is the last token $r_{t} = r_{\\phi}(q, o) - \\beta\\text{KL}(t)$\n",
    "    * $\\sum_{t=1}^{T}r_{t} = r_{\\phi}(q, o) - \\beta\\log({\\pi_{\\theta_{\\text{old}}}(o|q)}/{\\pi_{\\text{ref}}(o|q)})$ is the reward PPO aims to optimize.\n",
    "    \n",
    "2. Compute TD error $\\delta_{t} = r_{t} + \\gamma V_{\\psi}(t+1) - V_{\\psi}(t) $.\n",
    "\n",
    "3. Compute Advantage Function using GAE: $A_{t} = \\sum(\\gamma\\lambda)^{l}\\delta_{t+l}$.\n",
    "\n",
    "```{tip}\n",
    "For an event $X$ with probability $p$, it's self information is\n",
    "\n",
    "$$I(X) = -\\log p(x)$$\n",
    "\n",
    "The less probable an event is, the more surprising it is and the more information it yields. The term\n",
    "\n",
    "$$\\log\\frac{p(x)}{q(x)} = -\\log q(x) - (-\\log p(x))$$\n",
    "\n",
    "can be interpreted as our relative surprise. The KL divergence between $P$ and $Q$ is\n",
    "\n",
    "$$\\mathbb{E}_{x\\sim P}\\left[\\log\\frac{p(x)}{q(x)}\\right]$$\n",
    "\n",
    "can be interpreted as the expected relative surprise from using $Q$ instead of $P$ when the actual distribution is $P$. It measures how one probability distribution $P$ is different from the reference probability distribution $Q$.\n",
    "```\n",
    "\n",
    "```{tip}\n",
    "In PPO, a value function needs to\n",
    "be trained alongside the policy model and to mitigate over-optimization of the reward model.<br>\n",
    "While in the LLM context, usually only the last token is assigned a\n",
    "reward score by the reward model, which may complicate the training of a value function that is\n",
    "accurate at each token.<br>\n",
    "Additionaly, as the value function employed in PPO is typically another model of comparable size as\n",
    "the policy model, it brings a substantial memory and computational burden.<br>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c49329d-392b-4782-86a3-931ac1e4347c",
   "metadata": {},
   "source": [
    "## GRPO\n",
    "\n",
    "![](../images/grpo1.png)\n",
    "\n",
    "To address this, we propose Group Relative Policy\n",
    "Optimization (GRPO), for each question $q$, GRPO samples a\n",
    "group of outputs $\\{o_1, o_2, \\dots , o_G\\}$ from the old policy $\\pi_{\\theta_{old}}$ and then optimizes the policy model\n",
    "by maximizing the following objective:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "J_{GRPO}(\\theta) = &\\underset{q\\sim P(Q),o\\sim\\pi_{\\theta_{\\text{old}}}(O|q)}{\\mathbb{E}}\\frac{1}{G}\\sum_{i=1}^{G}\\frac{1}{|o_{i}|}\\sum_{t=1}^{|o_{i}|}\\{\\min\\\\\n",
    "&\\left[\\frac{\\pi_{\\theta}(o_{i,t}|q,o_{i}<t)}{\\pi_{\\theta_{\\text{old}}}(o_{i,t}|q,o_{i}<t)}\\hat{A}_{i,t}, \\text{clip}\\left(\\frac{\\pi_{\\theta}(o_{i,t}|q,o_{i}<t)}{\\pi_{\\theta_{\\text{old}}}(o_{i,t}|q,o_{i}<t)}, 1-\\epsilon, 1+\\epsilon\\right)\\hat{A}_{i,t}\\right] - \\beta\\mathbb{D}_{KL}[\\pi_{\\theta}||\\pi_{ref}]\\},\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $\\epsilon$ and $\\beta$ are hyper-parameters, and $\\hat{A}_{i,t}$ is the advantage calculated based on relative\n",
    "rewards of the outputs inside each group only. Also note that, instead of adding KL\n",
    "penalty in the reward, GRPO regularizes by directly adding the KL divergence between the\n",
    "trained policy and the reference policy to the loss, avoiding complicating the calculation of $\\hat{A}_{i,t}$. And different from the KL penalty term used in PPO, we estimate the KL divergence with the\n",
    "following unbiased estimator:\n",
    "\n",
    "$$\\mathbb{D}_{KL}[\\pi_{\\theta}||\\pi_{ref}] = \\frac{\\pi_{\\text{ref}}(o_{i,t}|q, o_{i}<t)}{\\pi_{\\theta}(o_{i,t}|q,o_{i}<t)} - \\log\\frac{\\pi_{\\text{ref}}(o_{i,t}|q, o_{i}<t)}{\\pi_{\\theta}(o_{i,t}|q,o_{i}<t)} - 1,$$\n",
    "\n",
    "which is guaranteed to be positive.\n",
    "\n",
    "```{tip}\n",
    "A good estimator is unbiased (it has the right mean) and has low variance.<br>\n",
    "Since:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbb{E}_{x\\sim P}\\left[\\frac{Q(x)}{P(x)} - 1\\right] =& \\sum_{x}P(x)\\left(\\frac{Q(x)}{P(x)} - 1\\right)\\\\\n",
    "=& \\sum_{x}Q(x)- \\sum_{x}P(x) \\\\\n",
    "=& 1 - 1 \\\\\n",
    "=& 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "So $\\mathbb{E}_{x\\sim P}\\left[\\frac{Q(x)}{P(x)} - \\log\\frac{Q(x)}{P(x)} - 1\\right]$ is an unbiased estimator of the KL divergence.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7593e813-c242-4fc9-af81-6a393fe4e6be",
   "metadata": {},
   "source": [
    "## Outcome Supervision RL with GRPO\n",
    "\n",
    "Formally, for each question $q$, a group of outputs $\\{o_1,o_{2},\\dots,o_{G}\\}$ are sampled from the old\n",
    "policy model $\\pi_{\\theta_{\\text{old}}}$. A reward model is then used to score the outputs, yielding $G$ rewards $\\mathbf{r}=\\{r_1,r_{2},\\dots,r_{G}\\}$ correspondingly.\n",
    "\n",
    "Subsequently, these rewards are normalized by subtracting\n",
    "the group average and dividing by the group standard deviation. Outcome supervision provides\n",
    "the normalized reward at the end of each output $o_{i}$ and sets the advantages $\\hat{A}_{i,t}$ of **all tokens** in\n",
    "the output as the normalized reward:\n",
    "\n",
    "$$\\hat{A}_{i,t} = \\frac{r_i - \\text{mean}(\\mathbf{r})}{\\text{std}(\\mathbf{r})}.$$\n",
    "\n",
    "```{tip}\n",
    "GRPO 的优势:<br>\n",
    "1. 没有 critic model，省资源。<br>\n",
    "2. Group by query，学同一个 query 中相对好的 response，query 间相互隔开。不会出现只学那些简单且 reward 高的 query 的情况。\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44ce0e6-60ef-4c2e-bb4f-e5520e0d36e4",
   "metadata": {},
   "source": [
    "## Process Supervision RL with GRPO\n",
    "\n",
    "Outcome supervision only provides a reward at the end of each output, which may not be\n",
    "sufficient and efficient to supervise the policy in complex mathematical tasks. We also explore process supervision, which provides a reward at the end of\n",
    "each reasoning step.\n",
    "\n",
    "Formally, given the question $q$ and $G$ sampled outputs $\\{o_1,o_2,\\dots,o_G\\}$, a\n",
    "**process reward model** is used to score each step of the outputs, yielding corresponding rewards: \n",
    "\n",
    "$$\\mathbf{R} = \\{\\{r_{1}^{\\text{index}(1)},\\dots,r_{1}^{\\text{index}(K_1)}\\},\\dots,\\{r_{G}^{\\text{index}(1)},\\dots,r_{G}^{\\text{index}(K_{G})}\\}\\},$$\n",
    "\n",
    "where $\\text{index}(j)$ is the end token index of the $j$-th step, and $K_i$ is the total number of steps in the $i$-th output. We also normalize these\n",
    "rewards with the average and the standard deviation\n",
    "\n",
    "$$\\tilde{r}_{i}^{\\text{index}(j)} = \\frac{r_{i}^{\\text{index}(j)} - \\text{mean}(\\mathbf{R})}{\\text{std}(\\mathbf{R})}$$\n",
    "\n",
    "Subsequently,\n",
    "the process supervision calculates the advantage of each token as the sum of the normalized\n",
    "rewards from the following steps\n",
    "\n",
    "$$\\hat{A}_{i,t} = \\sum_{\\text{index}(j)\\ge t}\\tilde{r}_{i}^{\\text{index}(j)}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bf8730-e2b3-473a-9907-444f4d7ecde0",
   "metadata": {},
   "source": [
    "## Iterative RL with GRPO\n",
    "\n",
    "As the reinforcement learning training process progresses, the old reward model may not be\n",
    "sufficient to supervise the current policy model. Therefore, we also explore the iterative RL\n",
    "with GRPO. As shown in Algorithm 1,\n",
    "\n",
    "![](../images/grpo2.png)\n",
    "\n",
    "in iterative GRPO, we generate new training sets for the\n",
    "reward model based on the sampling results from the policy model and continually train the\n",
    "old reward model using a replay mechanism that incorporates 10% of historical data. Then, we\n",
    "set the reference model as the policy model, and continually train the policy model with the\n",
    "new reward model.\n",
    "\n",
    "```{caution}\n",
    "1. GRPO 相比 PPO 的效果? Monte-Carlo sampling 会不会使得训练变得很慢？\n",
    "2. KL 散度直接放在 loss 里和放在 reward 里的区别？\n",
    "3. Outcome supervision or Process supervision?\n",
    "4. How to train process reward model?\n",
    "5. Iterative RL with GRPO 的效果？\n",
    "6. Group by query + Monte-Carlo sampling + critic model 会不会更好。\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05dcec1-e0bc-44ab-849a-9fd05016973d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
