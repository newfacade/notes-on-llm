{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46575422-22f4-498f-880b-f554b6180f0b",
   "metadata": {},
   "source": [
    "# RSO\n",
    "\n",
    "```{note}\n",
    "The maximum likelihood estimator\n",
    "(MLE) of the target optimal policy requires labeled preference pairs sampled from\n",
    "that policy, DPOâ€™s lack of a reward model constrains its ability to sample preference\n",
    "pairs from the optimal policy. To address these limitations, we introduce a novel\n",
    "approach called Statistical Rejection Sampling Optimization (RSO) that aims to\n",
    "source preference data from the target optimal policy using rejection sampling.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8958482e-420e-4ba2-b4b4-9c6c687aa9c5",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "\n",
    "**Learning from Human Feedback** algorithms take two inputs:\n",
    "\n",
    "1. $\\pi_{\\text{sft}}(y|x):$ a supervised fine-tuned policy (SFT), where $x$ is the prompt and $y$ is the response.\n",
    "2. $\\mathcal{D}_{\\text{hf}} = \\{x^{(i)}, y_{w}^{(i)}, y_{l}^{(i)}\\}_{i=1}^{N}:$ a human preference dataset that distinguishes the better response from the worse given the same prompt.\n",
    "\n",
    "**KL-Constrained Reward Maximization Objective** Starting with a reward function $r(x, y)$ and input prompt distribution $\\mathcal{P}$, the DPO and RLHF optimizes for the following objective:\n",
    "\n",
    "$$\\max_{\\pi}\\mathbb{E}_{x\\sim\\mathcal{P},y\\sim\\pi}\\left[r(x, y)\\right] - \\beta\\mathbb{D}_{\\text{KL}}\\left[\\pi(y|x)\\ ||\\ \\pi_{\\text{sft}}(y|x)\\right]$$\n",
    "\n",
    "**Optimal Policy** DPO showed that the optimal policy $\\pi_{r}(y|x)$ that maximizes the\n",
    "above objective is:\n",
    "\n",
    "$$\\pi_{r}(y|x) = \\frac{1}{Z(x)}\\pi_{\\text{sft}}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x, y)\\right)$$\n",
    "\n",
    "for all $x\\in\\mathcal{P}$, where $Z(x) = \\sum_{y}\\pi_{\\text{sft}}(y|x)\\exp(\\frac{1}{\\beta}r(x, y))$. Rearrange the Equation we get\n",
    "\n",
    "$$r(x, y) = \\beta\\log\\frac{\\pi_{r}(y|x)}{\\pi_{\\text{sft}}(y|x)} + \\beta\\log Z(x)$$\n",
    "\n",
    "These two equations establish the relation between optimal policy and the reward function. One\n",
    "can infer the other. In reality, the final goal is to have a good policy for response generation and $\\pi_{r}(y|x)$ is usually of more interest. The key is to effectively estimate the $\\pi_{r}(y|x)$ from the human\n",
    "preference data.\n",
    "\n",
    "**Preference Model** Let the ground-truth reward function be $r^{\\ast}$, and let $\\pi^{\\ast}$ be the optimal policy associated\n",
    "with $r^{\\ast}$. For two responses $(y_{1}, y_{2})$ from the same input $x$, one can assume that\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbb{P}(y_{1}\\succ y_{2}|x) &= g(r^{\\ast}(x, y_{1}) - r^{\\ast}(x, y_{2}))\\\\\n",
    "&=g\\left(\\beta\\log\\frac{\\pi^{\\ast}(y_{1}|x)}{\\pi_{\\text{sft}}(y_{1}|x)} -  \\beta\\log\\frac{\\pi^{\\ast}(y_{2}|x)}{\\pi_{\\text{sft}}(y_{2}|x)}\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $g:\\mathbb{R}\\to[0, 1]$ is a monotonically non-decreasing function that converts the reward difference\n",
    "into winning probability. Specifically, if we set $g$ as sigmoid function $\\sigma$, we get the Bradley-Terry (BT) model.\n",
    "\n",
    "**Policy Estimation on Preference Pairs** In statistical density estimation\n",
    "problem, the preference pairs should be generated from $\\pi^{\\ast}$, the density to be estimated, while DPO\n",
    "uses preference pairs from some unknown distribution. Thus DPO is not the MLE of $\\pi^{\\ast}$, this motivates us to develop\n",
    "an approach that can obtain preference pairs from $\\pi^{\\ast}$.\n",
    "\n",
    "**Reward Model** Usually the reward model is a pointwise score assigned to a (prompt, response)\n",
    "pair. The model is trained based on BT model. We argue\n",
    "that it is an easier and more straightforward way to train a pairwise reward model from (prompt,\n",
    "worse response, better response) triplets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f54b7c8-b3c4-42c2-aa7d-fe95a70151d4",
   "metadata": {},
   "source": [
    "## RSO APPROACH\n",
    "\n",
    "![](../images/rso2.png)\n",
    "\n",
    "**Choice of preference data distribution** In reality, we have\n",
    "access to $\\mathcal{D}_{\\text{hf}} = \\{(x^{(i)}, y_{w}^{(i)}, y_{l}^{(i)})| y_{w}^{(i)}, y_{l}^{(i)}\\sim \\pi_{\\text{unk}}(y|x^{(i)})\\}$ where $\\pi_{\\text{unk}}$ denotes some mixed\n",
    "unknown policies. The mixed unknown policies can include SFT policy, current RLHF policy, or\n",
    "policies from other agents, or even web mined preference pairs.\n",
    "\n",
    "Given $\\mathcal{D}_{\\text{hf}}$, we consider the following three choices:\n",
    "\n",
    "1. directly fit the policy on $\\mathcal{D}_{\\text{hf}}$, this is the approach used in DPO.\n",
    "\n",
    "2. first train a reward-ranking model $\\rho_{\\psi}(x, y_{1}, y_{2})$ on $\\mathcal{D}_{\\text{hf}}$. Then use $\\pi_{\\text{sft}}(y|x)$ to sample response pairs and label them by $\\rho_{\\psi}$. The results in a preference dataset $\\mathcal{D}_{\\text{sft}}$.\n",
    "\n",
    "3. first train a reward-ranking model $\\rho_{\\psi}(x, y_{1}, y_{2})$ on $\\mathcal{D}_{\\text{hf}}$. Then use $\\pi_{r_{\\psi}}(y|x)$ induced by $r_{\\psi}$ to sample response pairs, where $r_{\\psi}$ is induced from $\\rho_{\\psi}(x, y_{1}, y_{2})$. After that we label response pairs using $\\rho_{\\psi}$ to construct the preference dataset $\\mathcal{D}_{r_{\\psi}}$.\n",
    "\n",
    "Statistically speaking, since we are estimating $\\pi^{\\ast}(y|x)$, it is desired to draw samples from $\\pi^{\\ast}(y|x)$. $\\pi_{r_{\\psi}}$ is closer to $\\pi^{\\ast}$ than $\\pi_{\\text{unk}}$ used in direct and $\\pi_{\\text{sft}}$ used in sft-sample-rank. However,\n",
    "sampling from $\\pi_{r_{\\psi}}$ is not straightforward, and we propose a statistical rejection sampling\n",
    "approach to achieve this.\n",
    "\n",
    "### STATISTICAL REJECTION SAMPLING ALGORITHM\n",
    "\n",
    "![](../images/rso.png)\n",
    "\n",
    "If we want to generate a distribution of density $\\pi_{r_{\\psi}}$, we can:\n",
    "\n",
    "1. Generate $y\\sim\\pi_{\\text{sft}}(y|x)$ and $u\\sim U[0, 1]$.\n",
    "\n",
    "2. Let $M = \\min\\{m| m\\cdot\\pi_{\\text{sft}}(y|x) \\ge \\pi_{r_{\\psi}}(y|x) \\text{ for all } y\\}$. If $u<\\frac{\\pi_{\\psi}(y|x)}{M\\pi_{\\text{sft}}(y|x)}$, then we accept $y$, otherwise, we reject $y$.\n",
    "\n",
    "**Derivation of Statistical Rejection Sampling Algorithm:** We have\n",
    "\n",
    "$$\\pi_{r_{\\psi}}(y|x) = \\frac{1}{Z_{\\psi}(x)}\\pi_{\\text{sft}}(y|x)\\exp\\left(\\frac{1}{\\beta}r_{\\psi}(x, y)\\right)$$\n",
    "\n",
    "which is equivalent to \n",
    "\n",
    "$$\\frac{\\pi_{r_{\\psi}}(y|x)}{\\pi_{\\text{sft}}(y|x)} = \\frac{1}{Z_{\\psi}(x)}\\exp\\left(\\frac{1}{\\beta}r_{\\psi}(x, y)\\right).$$\n",
    "\n",
    "Let $M_{D_{x}} = \\min\\{m|m\\cdot\\pi_{\\text{sft}}(y|x) \\ge \\pi_{r_{\\psi}}(y|x) \\text{ for all } y\\notin D_{x}\\} = \\max_{y\\notin D_{x}}\\frac{\\pi_{r_{\\psi}}(y|x)}{\\pi_{\\text{sft}}(y|x)}$, then\n",
    "\n",
    "$$M_{D_{x}} = \\frac{1}{Z_{\\psi}(x)}\\max_{y\\notin D_{x}}\\left[\\exp\\left(\\frac{1}{\\beta}r_{\\psi}(x, y)\\right)\\right]$$\n",
    "\n",
    "Then we have\n",
    "\n",
    "$$\\frac{\\pi_{r_{\\psi}}(y|x)}{M_{D_{x}}\\pi_{\\text{sft}}(y|x)} = \\exp\\left(\\frac{1}{\\beta}\\left(r_{\\psi}(x, y) - \\max_{y\\notin D_{x}}r_{\\psi}(x, y)\\right)\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b247bcd-2c49-4106-9240-36dedcb71a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import numpy as np\n",
    "\n",
    "def conduct_rejection_sampling(response_candidates: List[str],\n",
    "                               response_rewards: List[float],\n",
    "                               num_samples: int,\n",
    "                               beta: float) -> List[str]:\n",
    "    \"\"\"\n",
    "    Conducts rejection sampling guided by rewards.\n",
    "\n",
    "    Args:\n",
    "        response_candidates: Response candidates from sft policy.\n",
    "        response_rewards: Response rewards corresponding to each candidate.\n",
    "        num_samples: Number of samples to sub-sample.\n",
    "        beta: Beta parameter in KL-constrained reward maximization objective.\n",
    "\n",
    "    Returns:\n",
    "        A list of rejection sampled sequences from the optimal policy.\n",
    "    \"\"\"\n",
    "\n",
    "    # Combine candidates and their rewards into a dictionary for easy access\n",
    "    candidates = {candidate: reward for candidate, reward in zip(response_candidates, response_rewards)}\n",
    "    \n",
    "    # List to store accepted candidates\n",
    "    accepted = []\n",
    "\n",
    "    # Continue sampling until we have the desired number of samples\n",
    "    while len(accepted) < num_samples:\n",
    "        # Find the maximum reward among the candidates\n",
    "        max_reward = max(candidates.values())\n",
    "\n",
    "        # List to track candidates to be removed after iteration\n",
    "        to_remove = []\n",
    "\n",
    "        # Iterate through candidates and perform rejection sampling\n",
    "        for candidate, reward in candidates.items():\n",
    "            # Sample a uniform random number\n",
    "            u = np.random.uniform()\n",
    "\n",
    "            # Accept the candidate based on the comparison with max_reward adjusted by beta\n",
    "            if u < np.exp((reward - max_reward) / beta):\n",
    "                accepted.append(candidate)\n",
    "                to_remove.append(candidate)\n",
    "\n",
    "                # Break if we have enough samples\n",
    "                if len(accepted) == num_samples:\n",
    "                    break\n",
    "\n",
    "        # Remove accepted candidates from the pool to avoid re-sampling\n",
    "        for candidate in to_remove:\n",
    "            candidates.pop(candidate)\n",
    "\n",
    "    return accepted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a13b7a5-0e40-4cfd-96d2-7598b06909a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
