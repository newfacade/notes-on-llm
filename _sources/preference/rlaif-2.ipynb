{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee398eb1-2201-472b-9dd4-80b63882e0be",
   "metadata": {},
   "source": [
    "## RLAIF vs. RLHF\n",
    "\n",
    "```{note}\n",
    "RL from AI Feedback (RLAIF) offers a promising\n",
    "alternative that trains the reward model (RM) on\n",
    "preferences generated by an off-the-shelf LLM. Our results suggest that RLAIF can\n",
    "achieve performance on-par with using human\n",
    "feedback, offering a potential solution to the scalability\n",
    "limitations of RLHF.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fe564d-7e60-4bc8-8981-ab28edf7d11b",
   "metadata": {},
   "source": [
    "## Methodology\n",
    "\n",
    "### Preference Labeling with LLMs\n",
    "\n",
    "Given a piece of text and two candidate responses,\n",
    "the “off-the-shelf” LLM is asked to rate which response is preferred. The\n",
    "prompt is structured as follows:\n",
    "\n",
    "1. *Preamble* - Introduction and instructions describing the task\n",
    "2. *Few-shot exemplars (optional)* - An example input context, a pair of responses, a chain-of-thought rationale (optional), and a preference label\n",
    "3. *Sample to annotate* - An input context and a pair of responses to be labeled\n",
    "4. *Ending* - The ending text to prompt the LLM (e.g. “Preferred Response=”)\n",
    "\n",
    "After the prompt is given to the LLM, we extract the `log-probabilities`\n",
    "of generating the tokens “1” and “2” and compute\n",
    "the softmax to obtain a preference distribution.\n",
    "\n",
    "![](../images/rlaif2-1.png)\n",
    "\n",
    "**Addressing Position Bias**\n",
    "\n",
    "The order in which candidates are shown to an LLM can\n",
    "bias which candidate it prefers. To mitigate the effect of position bias, two inferences are\n",
    "made for every pair of candidates, where the order in which\n",
    "candidates are presented to the LLM is `reversed for the\n",
    "second inference`. The results from both inferences are then\n",
    "averaged to obtain the final preference distribution.\n",
    "\n",
    "**Eliciting CoT**\n",
    "\n",
    "We experiment with eliciting chain-of-thought (CoT) reasoning from our AI labelers through a\n",
    "two-step inference procedure:\n",
    "\n",
    "1. We replace the Ending of the standard prompt with a sentence asking for thoughts\n",
    "and explanation (e.g. “Consider the coherence, accuracy,\n",
    "coverage, and overall quality of each summary and explain\n",
    "which one is better. Rationale:”)\n",
    "\n",
    "2. Decode a response from the LLM. Then, we concatenate the original prompt,\n",
    "the response, and the standard Ending string together, and\n",
    "follow the aforementioned scoring procedure to obtain a preference\n",
    "distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de4ec55-fd96-4cc7-b614-4472e5e9d524",
   "metadata": {},
   "source": [
    "### Reinforcement Learning from AI Feedback\n",
    "\n",
    "**Canonical RLAIF**\n",
    "\n",
    "A reward model (RM) is trained on the LLM-generated\n",
    "preference labels. Since our approach produces soft labels (e.g. [0.6, 0.4]),\n",
    "we train the RM with a cross-entropy loss on the softmax\n",
    "of the scores generated by the RM:\n",
    "\n",
    "$$\n",
    "-\\left[0.6\\times\\log\\sigma(r_{\\phi}(x, y_1) - r_{\\phi}(x, y_2)) + 0.4\\times\\log\\sigma(r_{\\phi}(x, y_2) - r_{\\phi}(x, y_1))\\right]\n",
    "$$\n",
    "\n",
    "**Direct-RLAIF (D-RLAIF)**\n",
    "\n",
    "One issue with RLAIF is that the reward model may become\n",
    "“stale” as the policy is trained. \n",
    "\n",
    "We introduce direct-RLAIF (d-RLAIF) - a simple alternative\n",
    "to canonical RLAIF that directly uses LLM feedback as the\n",
    "reward signal in RL. In d-RLAIF, the LLM is prompted to rate the quality of\n",
    "a generation between 1 and 10. A prompt instructs the LLM on how to rate a generation.\n",
    "Then, the likelihood of each score token between 1 and 10\n",
    "is computed, the likelihoods are normalized to a probability distribution, a weighted score is calculated as $s(y|x) = \\sum_{i=1}^{10}iP(i|y, x)$, and finally the score is again normalized\n",
    "to the range $[−1, 1]$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bf75f3-230c-4bae-abda-76335a54627b",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "We evaluate our results with three metrics - *AI Labeler\n",
    "Alignment*, *Win Rate*, and *Harmless Rate*.\n",
    "\n",
    "* *AI Labeler Alignment* measures the accuracy of AI-labeled\n",
    "preferences with respect to human preferences. (a soft AI-labeled preference is first converted to\n",
    "a binary representation, e.g. [0.6, 0.4] → [1, 0])\n",
    "\n",
    "* *Win Rate* evaluates the end-to-end quality of two policies\n",
    "by measuring how often one policy is preferred by human annotators over another.\n",
    "\n",
    "* *Harmless Rate* measures the percentage of responses that are\n",
    "considered harmless by human evaluators."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0476886f-c3a5-4334-83d8-3746dd9eeda5",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "### RLAIF vs. RLHF\n",
    "\n",
    "RLAIF achieves performance gains on par with or better\n",
    "than RLHF on all three tasks.\n",
    "\n",
    "![](../images/rlaif2-2.png)\n",
    "\n",
    "We observe that RLAIF\n",
    "and RLHF policies tend to generate longer responses than\n",
    "the SFT policy, which may bias human evaluation. We\n",
    "conduct post-hoc analysis to control for length and find that\n",
    "both RLAIF and RLHF policies still outperform the SFT\n",
    "policy.\n",
    "\n",
    "One natural question that arises is whether there is value in\n",
    "combining human and AI feedback. We experimented with\n",
    "combining both types of feedback but did not see an improvement\n",
    "beyond using human feedback alone.\n",
    "\n",
    "### Prompting Techniques\n",
    "\n",
    "![](../images/rlaif2-3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b88b91-f256-44a2-bf4b-6955c89e2e9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
