{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a537ffe-1b5a-4999-bed6-a870234670ef",
   "metadata": {},
   "source": [
    "# Self-Taught Evaluators\n",
    "\n",
    "```{note}\n",
    "Model-based evaluation is at the heart of successful\n",
    "model development.<br/>\n",
    "In this\n",
    "work, we present an approach that aims to improve\n",
    "evaluators without human annotations,\n",
    "using synthetic training data only.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f4059d-8f7e-4d13-a304-e6d89aab38ea",
   "metadata": {},
   "source": [
    "## Method\n",
    "\n",
    "We consider the setting of pairwise evaluation using\n",
    "the LLM-as-a-Judge approach that takes:\n",
    "\n",
    "* an input (user instruction) $x$.\n",
    "\n",
    "* two possible assistant responses $y^{(A)}$ and $y^{(B)}$ to the user instruction $x$.\n",
    "\n",
    "* the evaluation prompt asking to evaluate and choose the winning\n",
    "answer.\n",
    "\n",
    "![](../images/self-taught1.png)\n",
    "\n",
    "The goal of the LLM-as-a-Judge model is to\n",
    "output a preference of which response $y$ is better:\n",
    "$A$ or $B$. Such models can be used as pairwise reward\n",
    "models to build training data for preference optimization,\n",
    "e.g., for training methods like DPO, Iterative DPO and Self-Rewarding methods. They can also be used for evaluation.\n",
    "\n",
    "We propose a novel recipe for training such an\n",
    "evaluator.\n",
    "\n",
    "### Initialization\n",
    "\n",
    "We assume we have access to a pool of user instructions $\\{x_i\\}$, and an initial seed LLM.\n",
    "\n",
    "### Instruction Selection\n",
    "\n",
    "We next select a challenging,\n",
    "balanced distribution of user instructions\n",
    "from the uncurated set by categorizing\n",
    "them via LLM.\n",
    "\n",
    "We classify each input using an LLM into a\n",
    "given category, for example coding, reasoning,\n",
    "brainstorming. We are then free to select data\n",
    "from within those categories, and to discard certain\n",
    "categories not deemed to be useful for training.\n",
    "\n",
    "![](../images/self-taught2.png)\n",
    "\n",
    "### Response Pair Construction\n",
    "\n",
    "Given the instruction $x_{i}$, we first prompt an\n",
    "instruction-following LLM to generate a baseline\n",
    "response $y_{i}^{w}$ as usual. We then prompt the LLM\n",
    "to generate a “noisy” version of the original instruction $x_{i}^{'}=\\phi(x_{i})$. We do this using this prompt:\n",
    "\n",
    "![](../images/self-taught3.png)\n",
    "\n",
    "where we ask to “generate\n",
    "a modified instruction that is highly relevant but\n",
    "not semantically identical to the instruction above\n",
    "from the user.” We then prompt the LLM for a\n",
    "high-quality response $y_{i}^{l}$ to $x_{i}^{'}$, which would not\n",
    "be a good response for $x_{i}$.\n",
    "\n",
    "This paired data is then used to construct training\n",
    "examples:\n",
    "\n",
    "$$\n",
    "(x_{i}, y_{i}^{(A)}, y_{i}^{(B)})\n",
    "$$\n",
    "\n",
    "where we randomize the order of whether the winner\n",
    "is $w = A$ or $w = B$, which is important to deal\n",
    "with position bias for LLM-as-a-Judge inference.\n",
    "\n",
    "### Iterative Training\n",
    "\n",
    "We then iterate the following\n",
    "two steps.\n",
    "\n",
    "#### Judgment Annotation\n",
    "\n",
    "Our LLM-as-a-Judge model is used to generate\n",
    "evaluation judgments (reasoning chains and\n",
    "verdicts) $\\{j_{i}\\}$ for each training example $e_{i} = (x_{i}, y_{i}^{(A)}, y_{i}^{(B)})$ in the following manner: for a\n",
    "given input $e_{i}$, we collect $N$ diverse evaluations $\\mathcal{J} = \\{j_{i}^{1},\\dots,j_{i}^{N}\\}$ by sampling from the model.\n",
    "We then apply rejection sampling to filter $\\mathcal{J}$ by removing\n",
    "$j_{i}^{n}$ when the final verdict disagrees with the\n",
    "ground truth labeling, We then select a single correct solution from the pool of correct solutions.\n",
    "If no such judgment exists then we\n",
    "discard the example.\n",
    "\n",
    "This now allows us to construct our final training\n",
    "examples of synthetic preferences for fine-tuning:\n",
    "\n",
    "$$((x_{i}, y_{i}^{(A)}, y_{i}^{(B)}), j_{i}).$$\n",
    "\n",
    "#### Model Fine-tuning (Iterative Training)\n",
    "\n",
    "Our Self-Taught Evaluator (LLM-as-a-Judge\n",
    "model) is first initialized with the seed LLM. The\n",
    "model is then trained in an iterative manner. At\n",
    "each iteration, we annotate the training examples\n",
    "with judgments using\n",
    "the current model, giving training examples $\\{((x_{i}, y_{i}^{(A)}, y_{i}^{(B)}), j_{i})\\}$. These are used to train the\n",
    "next iteration’s model by fine-tuning.\n",
    "\n",
    "```{tip}\n",
    "Note that we\n",
    "initialize from the seed model at each iteration.\n",
    "```\n",
    "\n",
    "![](../images/self-taught.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a55a119-8f32-4529-be5c-01dd3bc322ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
