{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa136d30-0040-4066-afed-349565ca5752",
   "metadata": {},
   "source": [
    "# LIMA: Less Is More for Alignment\n",
    "\n",
    "```{note}\n",
    "Large language models are trained in two stages:\n",
    "1. pretraining\n",
    "2. instruction tuning and reinforcement learning\n",
    "\n",
    "We measure the relative importance of these two stages by training [LIMA](https://arxiv.org/abs/2305.11206), a 65B\n",
    "parameter LLaMa language model fine-tuned with the standard supervised loss on\n",
    "only 1,000 carefully curated prompts and responses. LIMA demonstrates remarkably strong\n",
    "performance, strongly suggest that almost all knowledge in large\n",
    "language models is learned during pretraining, and only limited instruction tuning\n",
    "data is necessary to teach models to produce high quality output.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db58e02-b9db-4a50-8177-35e21bc7dcd5",
   "metadata": {},
   "source": [
    "## Alignment Data\n",
    "\n",
    "We curate 1,000 examples that approximate real user prompts and\n",
    "high-quality responses. We select 750 top questions and answers from community forums, such as\n",
    "Stack Exchange and wikiHow, sampling for quality and diversity. In addition, we manually write 250\n",
    "examples of prompts and responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab956fe9-d09a-4409-a89a-08a259943589",
   "metadata": {},
   "source": [
    "## Training LIMA\n",
    "\n",
    "We follow standard fine-tuning hyperparameters: we fine-tune for 15 epochs using\n",
    "AdamW with $\\beta_1=0.9$, $\\beta_2=0.95$, and weight decay of 0.1.\n",
    "Without warmup steps, we set the initial learning rate to $1e^{-5}$ and linearly decaying to $1e^{-6}$ by\n",
    "the end of training. The batch size is set to 32 examples (64 for smaller models), and texts longer\n",
    "than 2048 tokens are trimmed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd45925-cd96-4b5f-bb0f-f25c55dd7f41",
   "metadata": {},
   "source": [
    "## Human Evaluation\n",
    "\n",
    "```{figure} ../images/lima-1.png\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0109ab0-427e-456b-a67b-e44cb247b996",
   "metadata": {},
   "source": [
    "## Ablations on Data Diversity, Quality, and Quantity\n",
    "\n",
    "We investigate the effects of training data diversity, quality, and quantity through ablation experiments.\n",
    "We observe that, for the purpose of alignment, scaling up input `diversity and output quality` have\n",
    "measurable positive effects, while scaling up quantity alone might not.\n",
    "\n",
    "```{figure} ../images/lima-2.png\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8be938-607f-4777-bb7c-bbb0cd2d8dbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
