{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed792666-3a71-489c-9f4c-7d10052bfd73",
   "metadata": {},
   "source": [
    "(deepseekmoe)=\n",
    "# DeepSeekMoE\n",
    "\n",
    "```{note}\n",
    "We propose the `DeepSeekMoE`{cite}`dai2024deepseekmoeultimateexpertspecialization` architecture towards ultimate expert specialization. It\n",
    "involves two principal strategies:<br>\n",
    "1. Finely segmenting the experts into $mN$ ones and activating\n",
    "$mK$ from them, allowing for a more flexible combination of activated experts.<br>\n",
    "2. Isolating $Ks$\n",
    "experts as shared ones, aiming at capturing common knowledge and mitigating redundancy\n",
    "in routed experts.\n",
    "```\n",
    "\n",
    "## Preliminaries: Mixture-of-Experts for Transformers\n",
    "\n",
    "A standard Transformer language model is constructed by stacking $L$ layers of standard\n",
    "Transformer blocks, where each block can be represented as follows:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{u}_{1:T}^{l} &= \\text{Self-Att}(\\mathbf{h}_{1:T}^{l-1}) + \\mathbf{h}_{1:T}^{l-1}\\\\\n",
    "\\mathbf{h}_{t}^{l} &= \\text{FFN}(\\mathbf{u}_{t}^{l}) + \\mathbf{u}_{t}^{l}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $T$ denotes the sequence length, $\\text{Self-Att}(·)$ denotes the self-attention module, $\\text{FFN}(·)$\n",
    "denotes the Feed-Forward Network (FFN), $\\mathbf{u}_{1:T}^{l}\\in\\mathbb{R}^{T\\times d}$ are the hidden states of all tokens after\n",
    "the $l$-th attention module, and $\\mathbf{h}_{t}^{l}\\in\\mathbb{R}^{d}$ is the output hidden state of the $t$-th token after the $l$-th Transformer block. We omit the layer normalization in the above formulations for brevity.\n",
    "\n",
    "A typical practice to construct an MoE language model usually substitutes FFNs in a Transformer\n",
    "with MoE layers. An MoE layer is composed of multiple experts, where each expert is\n",
    "structurally identical to a standard FFN. Then, each token will be assigned to one or two experts. If the $l$-th FFN is substituted with an MoE layer:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{h}_{t}^{l} &= \\sum_{i=1}^{N}(g_{i,t}\\text{FFN}_{i}(\\mathbf{u_{t}^{l}})) + \\mathbf{u}_{t}^{l}\\\\\n",
    "g_{i,t} &= \n",
    "\\begin{cases}\n",
    "s_{i,t},\\quad &s_{i,t}\\in\\text{Topk}(\\{s_{j,t}|1\\le j\\le N\\}, K)\\\\\n",
    "0, &\\text{otherwise}\n",
    "\\end{cases}\\\\\n",
    "s_{i,t} &= \\text{Softmax}_{i}({\\mathbf{u}_{t}^{l}}^{\\intercal}\\mathbf{e}_{i}^{l})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $N$ denotes the total number of experts, $\\text{FFN}_{i}$ is the $i$-th expert FFN, $g_{i,t}$ denotes the\n",
    "gate value for the $i$-th expert, $s_{i,t}$ denotes the token-to-expert affinity, $\\text{Topk}(\\cdot,K)$ denotes the set\n",
    "comprising $K$ highest affinity scores among those calculated for the $t$-th token and all $N$ experts, and $\\mathbf{e}_{i}^{l}$ is the centroid of the $i$-th expert in the $l$-th layer (parameter of the gate).\n",
    "\n",
    "```python\n",
    "# Part of MoEGate\n",
    "self.gating_dim = config.hidden_size\n",
    "self.weight = nn.Parameter(torch.empty((self.n_routed_experts, self.gating_dim)))\n",
    "```\n",
    "\n",
    "## Fine-Grained Expert Segmentation\n",
    "\n",
    "While maintaining a consistent number of expert parameters and\n",
    "computational cost, we segment the experts with a finer grain. The finer expert segmentation\n",
    "enables a more flexible and adaptable combination of activated experts.\n",
    "\n",
    "To be specific, we segment each expert FFN into $m$ smaller\n",
    "experts by reducing the FFN intermediate hidden dimension to $\\frac{1}{m}$ times its original size. Since\n",
    "each expert becomes smaller, in response, we also increase the number of activated experts to\n",
    "$m$ times to keep the same computation cost.\n",
    "\n",
    "![](../images/deepseek-moe.png)\n",
    "\n",
    "## Shared Expert Isolation\n",
    "\n",
    "Tokens assigned to different experts may necessitate some\n",
    "common knowledge or information. As a result, multiple experts may converge in acquiring\n",
    "shared knowledge in their respective parameters, thereby resulting in redundancy in expert\n",
    "parameters. However, if there are shared experts dedicated to capturing and consolidating\n",
    "common knowledge across varying contexts, the parameter redundancy among other routed\n",
    "experts will be alleviated.\n",
    "\n",
    "Towards this objective, in addition to the fine-grained expert segmentation strategy, we\n",
    "further isolate $K_{s}$ experts to serve as shared experts. In order to maintain a constant\n",
    "computational cost, the number of activated experts among the other routed experts will be\n",
    "decreased by $K_{s}$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{h}_{t}^{l} &= \\sum_{i=1}^{K_s}\\text{FFN}_{i}(\\mathbf{u}_{t}^{l}) + \\sum_{i=K_s}^{mN}(g_{i,t}\\text{FFN}_{i}(\\mathbf{u_{t}^{l}})) + \\mathbf{u}_{t}^{l}\\\\\n",
    "g_{i,t} &= \n",
    "\\begin{cases}\n",
    "s_{i,t},\\quad &s_{i,t}\\in\\text{Topk}(\\{s_{j,t}|K_s\\le j\\le mN\\}, mK-K_s)\\\\\n",
    "0, &\\text{otherwise}\n",
    "\\end{cases}\\\\\n",
    "s_{i,t} &= \\text{Softmax}_{i}({\\mathbf{u}_{t}^{l}}^{\\intercal}\\mathbf{e}_{i}^{l})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "## Load Balance Consideration\n",
    "\n",
    "Automatically learned routing strategies may encounter the issue of load imbalance.\n",
    "\n",
    "### Expert-Level Balance Loss\n",
    "\n",
    "Imbalance leeds to higher loss.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathcal{L}_{\\text{ExpBal}} &= \\alpha_{1}\\sum_{i=1}^{mN-K_s}f_{i}P_{i}\\\\\n",
    "f_{i} &= \\frac{mN-K_{s}}{(mK-K_s)T}\\sum_{t=1}^{T}\\mathbb{1}(\\text{Token }t\\text{ selects Expert }i)\\\\\n",
    "P_{i} &= \\frac{1}{T}\\sum_{t=1}^{T}s_{i,t}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "* $f_{i}$: normalized frequency of the $i$-th expert, $\\sum_{i=1}^{N'}f_{i}=N'$.\n",
    "* $P_{i}$: normalized weight of the $i$-th expert, $\\sum_{i=1}^{N'}P_{i}=N'$.\n",
    "\n",
    "### Device-Level Balance Loss\n",
    "\n",
    "In addition to the expert-level balance loss, we additionally\n",
    "design a device-level balance loss to ensure balanced computation across different devices. If we partition all routed experts into $D$ groups $\\{\\mathcal{E}_{1}, \\mathcal{E}_{2}, \\dots, \\mathcal{E}_{D}\\}$, and deploy each group on a single device, the device-level balance loss is\n",
    "computed as follows:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathcal{L}_{\\text{DevBal}} &= \\alpha_{2}\\sum_{i=1}^{D}f_{i}'P_{i}'\\\\\n",
    "f_{i}' &= \\frac{1}{|\\mathcal{E}_{i}|}\\sum_{j\\in\\mathcal{E}_{i}}f_{j}\\\\\n",
    "P_{i}' &= \\sum_{j\\in\\mathcal{E}_{i}}P_{j}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### Communication Balance Loss\n",
    "\n",
    "Introduced in DeepSeek-V2{cite}`deepseekai2024deepseekv2strongeconomicalefficient`.\n",
    "\n",
    "When expert parallelism is employed, the routed experts will be distributed across multiple\n",
    "devices. For each token, its MoE-related communication frequency is proportional to the\n",
    "number of devices covered by its target experts. Due to the fine-grained expert segmentation in\n",
    "DeepSeekMoE, the number of activated experts can be large, so the MoE-related communication\n",
    "will be more costly if we apply expert parallelism.\n",
    "\n",
    "For DeepSeek-V2, beyond the naive top-K selection of routed experts, we additionally ensure\n",
    "that the target experts of each token will be distributed on at most $M$ devices, which are selected according to\n",
    "the sum of the highest ${K_r}/{M}$ affinity scores of the experts distributed on each node..\n",
    "Then, we perform top-K selection among experts on these $M$ devices.\n",
    "\n",
    "Finally, we introduce a communication balance loss to ensure\n",
    "that the communication of each device is balanced. Although the device-limited routing mechanism\n",
    "guarantees that the sending communication of each device is bounded, if a certain device receives more tokens than other devices, the practical communication efficiency will also be\n",
    "affected. In order to mitigate this issue, we design a communication balance loss as follows:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathcal{L}_{\\text{CommBal}} &= \\alpha_{3}\\sum_{i=1}^{D}f_{i}''P_{i}''\\\\\n",
    "f_{i}'' &= \\frac{D}{MT}\\sum_{t=1}^{T}\\mathbb{1}(\\text{Token }t\\text{ is sent to Device }i)\\\\\n",
    "P_{i}'' &= \\sum_{j\\in\\mathcal{E}_{i}}P_{j}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "```{tip}\n",
    "Suppose $T=3$ and expert1, expert2, expert3 are in the same device, think about two situations:\n",
    "\n",
    "1. token1 selects expert1, expert2 and expert3.\n",
    "2. token1 selects expert1, token2 selects expert2, token3 selects expert3.\n",
    "\n",
    "They have the same device-Level Balance, but different communication balance.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bdc8b9-a9e0-4fd5-91bd-a876482be8f0",
   "metadata": {},
   "source": [
    "## Token-Dropping Strategy\n",
    "\n",
    "Introduced in DeepSeek-V2{cite}`deepseekai2024deepseekv2strongeconomicalefficient`.\n",
    "\n",
    "In order to further mitigate the computation\n",
    "wastage caused by unbalanced load, we introduce a device-level token-dropping strategy during\n",
    "training.\n",
    "\n",
    "This approach first computes the average computational budget for each device, which\n",
    "means that the capacity factor for each device is equivalent to 1.0. Then, we drop tokens with the lowest affinity scores on each device until reaching the\n",
    "computational budget. In addition, we ensure that the tokens belonging to approximately 10%\n",
    "of the training sequences will never be dropped. In this way, we can flexibly decide whether\n",
    "to drop tokens during inference according to the efficiency requirements, and always ensure\n",
    "consistency between training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456c1621-a97d-4406-9325-ca591231122f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
