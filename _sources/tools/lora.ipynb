{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "888075c1-2617-4dd1-a58a-77edb9ff20a7",
   "metadata": {},
   "source": [
    "# LORA\n",
    "\n",
    "```{note}\n",
    "We propose Low-Rank Adaptation, or LoRA, which freezes the pretrained\n",
    "model weights and injects trainable rank decomposition matrices into each\n",
    "layer of the Transformer architecture, greatly reducing the number of trainable parameters\n",
    "for downstream tasks.\n",
    "```\n",
    "\n",
    "![](../images/lora.png)\n",
    "\n",
    "For $h=W_{0}x$, our modified forward pass yields:\n",
    "\n",
    "$$h=W_{0}x+\\delta Wx= W_{0}x + BAx$$\n",
    "\n",
    "where $B\\in\\mathbb{R}^{d\\times r}, A\\in\\mathbb{R}^{r\\times k}$, and the rank $r\\ll\\min(d, k)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3290cb2-b15f-4ee2-8583-9bb7f6f60304",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
