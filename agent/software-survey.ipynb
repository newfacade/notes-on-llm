{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75061905-62d2-4da6-b7b9-7f1ac9e5e71f",
   "metadata": {},
   "source": [
    "# Agents in Software Engineering\n",
    "\n",
    "```{note}\n",
    "In [this paper](https://arxiv.org/abs/2409.09030), we\n",
    "conduct the first survey of the studies on combining\n",
    "LLM-based agents with SE and present\n",
    "a framework of LLM-based agents in SE\n",
    "which includes three key modules: perception,\n",
    "memory, and action.\n",
    "```\n",
    "\n",
    "```{figure} ../images/se-agent0.png\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6f76bd-c4a9-45ec-931b-ba8925b18d57",
   "metadata": {},
   "source": [
    "## Perception\n",
    "\n",
    "```{tip}\n",
    "The\n",
    "perception module receives external environment\n",
    "information of various modalities and converts it\n",
    "into an input form that the LLM can understand\n",
    "and process.\n",
    "```\n",
    "\n",
    "### Textual Input\n",
    "\n",
    "**Token-based Input.** Token-based input is the most mainstream input mode,\n",
    "which directly regards the code as natural language\n",
    "text and directly uses the token sequence as the\n",
    "input of LLM, ignoring the characteristics of code.\n",
    "\n",
    "**Tree/Graph-based Input.** Based on the\n",
    "characteristics of code, tree/graph-based input can convert code into tree structures\n",
    "such as abstract syntax trees or graph structures\n",
    "like control flow graphs to model the structural\n",
    "information of code.\n",
    "\n",
    "**Hybrid-based Input.**\n",
    "\n",
    "### Visual Input\n",
    "\n",
    "Visual input uses visual image data such as UI\n",
    "sketches or UML design drawings as model input\n",
    "and makes inference decisions through modeling\n",
    "and analysis of images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3932c5d7-fffe-4fbd-bbec-c1316e721ba5",
   "metadata": {},
   "source": [
    "## Memory\n",
    "\n",
    "```{tip}\n",
    "The memory modules can provide additional\n",
    "useful information to help LLM make reasoning\n",
    "decisions.\n",
    "```\n",
    "\n",
    "### Semantic Memory\n",
    "\n",
    "Semantic memory stores acknowledged world\n",
    "knowledge of LLM-based agents, usually in the\n",
    "form of external knowledge retrieval bases which\n",
    "include `documents` and `APIs`. For example, [Ren et al. (2023)](https://arxiv.org/abs/2309.15606)\n",
    "propose KPC, a novel Knowledge-driven Prompt\n",
    "Chaining-based code generation approach, which\n",
    "utilizes fine-grained exception-handling knowledge\n",
    "extracted from API documentation to assist LLMs\n",
    "in code generation.\n",
    "\n",
    "### Episodic Memory\n",
    "\n",
    "Episodic memory records content related to the\n",
    "`current case` and experience information from `previous\n",
    "decision-making processes`.\n",
    "\n",
    "Content related\n",
    "to the current case (such as relevant information\n",
    "found in the search database, samples provided by\n",
    "In-context learning (ICL) technology, etc.) can provide\n",
    "additional knowledge for LLM reasoning. For example, [Li et al. (2023c)](https://arxiv.org/abs/2303.17780) propose a\n",
    "new prompting technique named AceCoder, which\n",
    "selects similar programs as examples in prompts.\n",
    "\n",
    "### Procedural Memory\n",
    "\n",
    "**Implicit knowledge** is stored in the LLM parameters.\n",
    "Existing work usually proposes new LLMs\n",
    "with rich implicit knowledge to complete various downstream tasks, by training the model with a\n",
    "large amount of data.\n",
    "\n",
    "**Explicit knowledge** is written in the agentâ€™s\n",
    "code, enabling the agent to operate automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67b4f49-e45e-4b8e-a0b0-2d20c97cddfa",
   "metadata": {},
   "source": [
    "## Action\n",
    "\n",
    "```{tip}\n",
    "The action module includes two types: internal and\n",
    "external actions. The external actions interact with\n",
    "the external environment to obtain feedback information.\n",
    "```\n",
    "\n",
    "### Internal Action\n",
    "\n",
    "**Reasoning Action.** A rigorous reasoning process\n",
    "is the key to completing tasks by LLM-based agents and Chain-of-Though (CoT) is an effective\n",
    "way of reasoning. As shown in Figure 3, existing work has explored\n",
    "different forms of CoT.\n",
    "\n",
    "```{figure} ../images/se-agent1.png\n",
    "```\n",
    "\n",
    "**Retrieval Action.** The retrieval action can retrieve\n",
    "relevant information from the knowledge\n",
    "base to assist the reasoning action in making correct\n",
    "decisions. The input used for retrieval and\n",
    "the output content obtained by retrieval have different\n",
    "types. Specifically, it\n",
    "can be divided into the following types:\n",
    "\n",
    "* Text-Code. For example, [Zan et al. (2022a)](https://arxiv.org/abs/2210.17236) propose a novel\n",
    "framework with APIRetriever and APICoder modules.\n",
    "Specifically, the APIRetriever retrieves useful\n",
    "APIs, and then the APICoder generates code using these retrieved APIs.\n",
    "* Text-Text.\n",
    "\n",
    "Existing retrieval\n",
    "methods can be divided into sparse-based\n",
    "retrieval, dense-based retrieval.\n",
    "\n",
    "* The dense-based retrieval method converts\n",
    "the input into a high-dimensional vector and\n",
    "then compares the semantic similarity to select the\n",
    "k samples with the highest similarity.\n",
    "\n",
    "* The\n",
    "sparse-based retrieval method calculates metrics\n",
    "such as BM25 or TF-IDF to evaluate the text similarity\n",
    "between samples.\n",
    "\n",
    "**Learning Action.** Learning actions are continuously\n",
    "learning and updating knowledge by learning\n",
    "and updating semantic and procedural memories.\n",
    "\n",
    "#### External Action\n",
    "\n",
    "**Dialogue with Human/Agents** Agents can interact\n",
    "with humans or other agents, and get rich information\n",
    "in the interaction process as feedback.\n",
    "\n",
    "**Digital Environment Agents** can interact with\n",
    "digital systems, such as OJ platforms, web pages,\n",
    "compilers, and other external tools, and the information\n",
    "obtained during the interaction process\n",
    "can be used as feedback to optimize themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a88252-d611-4d37-92ef-7d9a333775a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
