{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bea7753-26aa-4b99-b2da-d78e1aa2a97e",
   "metadata": {},
   "source": [
    "# GPT\n",
    "\n",
    "```{note}\n",
    "Improving Language Understanding by Generative Pre-Training{cite}`radford2018improving`\n",
    "```\n",
    "```{note}\n",
    "We demonstrate that large\n",
    "gains on a wide range of NLP tasks can be realized by `generative pre-training` of a language model\n",
    "on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each\n",
    "specific task.<br/>\n",
    "For our model architecture, we use the Transformer{cite}`vaswani2023attentionneed`, this model choice provides us with a more structured memory for handling long-term dependencies in text.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a370f936-82cf-4529-bb1f-b6152873716e",
   "metadata": {},
   "source": [
    "## Framework\n",
    "\n",
    "Our training procedure consists of two stages. The first stage is learning a high-capacity language\n",
    "model on a large corpus of text. This is followed by a fine-tuning stage, where we adapt the model to\n",
    "a discriminative task with labeled data.\n",
    "\n",
    "### Unsupervised pre-training\n",
    "\n",
    "Given an unsupervised corpus of tokens $\\mathcal{U} = \\{u_1,\\dots,u_n\\}$, we use a standard language modeling\n",
    "objective to maximize the following likelihood:\n",
    "\n",
    "$$\n",
    "L_{1}(\\mathcal{U}) = \\sum_{i}\\log P(u_i|u_{i-k},\\dots,u_{i-1};\\Theta)\n",
    "$$\n",
    "\n",
    "where $k$ is the size of the context window, and the conditional probability $P$ is modeled using a neural\n",
    "network with parameters $\\Theta$. These parameters are trained using stochastic gradient descent.\n",
    "\n",
    "In our experiments, we use a multi-layer Transformer decoder for the language model, which is\n",
    "a variant of the transformer. This model applies a multi-headed self-attention operation over the\n",
    "input context tokens followed by position-wise feedforward layers to produce an output distribution\n",
    "over target tokens:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "h_{0} &= UW_{e} + W_{p}\\\\\n",
    "h_{l} &= \\text{transformerBlock}(h_{l-1})\\quad\\forall i\\in[1,n]\\\\\n",
    "P(u) &= \\text{softmax}(h_n W_{e}^{T})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $U$ is the context vector of tokens, $n$ is the number of layers, $W_e$ is the token\n",
    "embedding matrix, and $W_p$ is the position embedding matrix.\n",
    "\n",
    "### Supervised fine-tuning\n",
    "\n",
    "After training the model with the language modeling\n",
    "objective, we adapt the parameters to the supervised target\n",
    "task. We assume a labeled dataset $\\mathcal{C}$, where each instance consists of a sequence of input tokens $x^{1},\\dots,x^{m}$, along with a label $y$. The inputs are passed through our pre-trained model to obtain\n",
    "the final transformer blockâ€™s activation $h_{l}^{m}$, which is then fed into an added linear output layer with\n",
    "parameters $W_y$ to predict $y$:\n",
    "\n",
    "$$\n",
    "P(y|x^1,\\dots,x^{m}) = \\text{softmax}(h_{l}^{m}W_y)\n",
    "$$\n",
    "\n",
    "This gives us the following objective to maximize:\n",
    "\n",
    "$$\n",
    "L_{2}(\\mathcal{C}) = \\sum_{(x,y)}P(y|x^{1},\\dots,x^{m})\n",
    "$$\n",
    "\n",
    "We additionally found that including language modeling as an auxiliary objective to the fine-tuning\n",
    "helped learning by (a) improving generalization of the supervised model, and (b) accelerating\n",
    "convergence. Specifically, we optimize the following objective (with weight $\\lambda$):\n",
    "\n",
    "$$\n",
    "L_{3}(\\mathcal{C}) = L_{2}(C) + \\lambda L_{1}(\\mathcal{C})\n",
    "$$\n",
    "\n",
    "Overall, the only extra parameters we require during fine-tuning are $W_{y}$, and embeddings for delimiter\n",
    "tokens.\n",
    "\n",
    "```{figure} ../images/gpt-1.png\n",
    "---\n",
    "height: 400px\n",
    "name: gpt-1\n",
    "---\n",
    "```\n",
    "\n",
    "### Task-specific input transformations\n",
    "\n",
    "For some tasks, like text classification, we can directly fine-tune our model as described above.\n",
    "Certain other tasks, like question answering or textual entailment, have structured inputs such as\n",
    "ordered sentence pairs, or triplets of document, question, and answers. Since our pre-trained model\n",
    "was trained on contiguous sequences of text, we require some modifications to apply it to these tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55599959-0af4-4a71-92bd-c58d9189f610",
   "metadata": {},
   "source": [
    "## Why decoder-only\n",
    "\n",
    "```{tip}\n",
    "1. **Autoregressive Generation:** Decoder-only models, like GPT, are inherently autoregressive. This makes them well-suited for text generation tasks where the goal is to predict the next word or phrase based on the preceding context.\n",
    "2. **Simplicity and Scalability:** Decoder-only models are simpler in structure, this simplicity makes them easier to scale up in terms of model size and training data.\n",
    "3. **Training Efficiency:** Decoder-only models can be trained using a straightforward causal language modeling objective. Encoder-decoder models often require more complex training objectives, such as sequence-to-sequence learning\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb232ce4-4fd9-41eb-9c07-e892d60299c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
