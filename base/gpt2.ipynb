{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "455374ee-2da9-4344-a61b-c64e6682339e",
   "metadata": {},
   "source": [
    "# GPT2\n",
    "\n",
    "```{note}\n",
    "Language models are unsupervised multitask learners{cite}`radford2019language`\n",
    "```\n",
    "```{note}\n",
    "Natural language processing tasks, such as question\n",
    "answering, machine translation, reading comprehension,\n",
    "and summarization, are typically\n",
    "approached with supervised learning on taskspecific\n",
    "datasets. We demonstrate that language\n",
    "models begin to learn these tasks without any explicit\n",
    "supervision when trained on a new dataset\n",
    "of millions of webpages called WebText.\n",
    "```\n",
    "\n",
    "```{figure} ../images/gpt2-2.png\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26a7910-b72e-48ff-8cfa-f3f53f9e9c46",
   "metadata": {},
   "source": [
    "## Approach\n",
    "\n",
    "At the core of our approach is language modeling.\n",
    "\n",
    "### Training Dataset\n",
    "\n",
    "Most prior work trained language models on a single domain\n",
    "of text, such as news articles, Wikipedia, or fiction books. Our approach motivates building `as large and\n",
    "diverse a dataset as possible` in order to collect natural language\n",
    "demonstrations of tasks in as varied of domains and\n",
    "contexts as possible.\n",
    "\n",
    "A promising source of diverse and nearly unlimited text is\n",
    "web scrapes such as Common Crawl. While these archives\n",
    "are many orders of magnitude larger than current language\n",
    "modeling datasets, they have significant data quality issues.\n",
    "\n",
    "Instead, we created a new web scrape which emphasizes\n",
    "document quality. We scraped all outbound links from\n",
    "Reddit, a social media platform, which received at least 3\n",
    "karma. The resulting dataset, WebText, contains the text subset\n",
    "of these 45 million links.\n",
    "\n",
    "### Input Representation\n",
    "\n",
    "We use Byte Pair Encoding (BPE), this input representation allows us to combine the empirical\n",
    "benefits of word-level LMs with the generality of byte-level\n",
    "approaches.\n",
    "\n",
    "### Model\n",
    "\n",
    "We use a Transformer based architecture\n",
    "for our LMs. The model largely follows the details\n",
    "of the OpenAI GPT{cite}`radford2018improving` model with a few modifications. Layer normalization was moved to the input of each sub-block, depicted in [](normalization), and an\n",
    "additional layer normalization was added after the final self-attention\n",
    "block."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11b8b14-3e72-4f23-9d12-7f6bb17e120e",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "\n",
    "We trained and benchmarked four LMs with approximately\n",
    "log-uniformly spaced sizes. The architectures are summarized\n",
    "in Table 2. The smallest model is equivalent to the\n",
    "original GPT, and the second smallest equivalent to the\n",
    "largest model from BERT{cite}`devlin2019bertpretrainingdeepbidirectional`. Our largest\n",
    "model, which we call GPT-2, has over an order of magnitude\n",
    "more parameters than GPT.\n",
    "\n",
    "```{figure} ../images/gpt2-1.png\n",
    "---\n",
    "height: 200px\n",
    "---\n",
    "```\n",
    "\n",
    "### Summarization\n",
    "\n",
    "We test GPT-2â€™s ability to perform summarization on the\n",
    "CNN and Daily Mail dataset. To induce\n",
    "summarization behavior we add the text `TL;DR:` after\n",
    "the article and generate 100 tokens with Top-$k$ random sampling with $k = 2$ which reduces repetition\n",
    "and encourages more abstractive summaries than greedy decoding. We use the first 3 generated sentences in these 100\n",
    "tokens as the summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bd0224-195f-4eb1-a2cf-198a8ae9acba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
