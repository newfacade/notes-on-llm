{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "befd4941-dd86-41ad-bba2-20b33ccf863e",
   "metadata": {},
   "source": [
    "# Magicoder\n",
    "\n",
    "```{note}\n",
    "We introduce Magicoder, a series of LLMs for code that significantly closes the gap with top\n",
    "code models while having no more than 7B parameters. Magicoder models are\n",
    "trained on 75K synthetic instruction data using OSS-INSTRUCT, a novel approach\n",
    "to enlightening LLMs with open-source code snippets to generate high-quality\n",
    "instruction data for code.\n",
    "```\n",
    "\n",
    "paper: https://arxiv.org/pdf/2312.02120"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68691498-3344-4a03-8aff-8f0510e15f9f",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "To further push the boundaries\n",
    "of code generation with open source LLMs, **SELF-INSTRUCT** is adopted to\n",
    "bootstrap the instruction-following ability of LLMs. In the realm of code, practitioners commonly\n",
    "devise synthetic coding instructions using a stronger teacher model (e.g., ChatGPT and GPT-4) and\n",
    "then finetune a weaker student model (e.g., CODELLAMA) with the generated\n",
    "data to distill the knowledge from the teacher. For example, **Code\n",
    "Alpaca** consists of 20K automatically generated code instructions by applying\n",
    "SELF-INSTRUCT on ChatGPT using 21 seed tasks. To further enhance the coding abilities of LLMs, **Code Evol-Instruct** employs various heuristics to increase the\n",
    "complexity of seed code instructions (Code Alpaca in this case).\n",
    "\n",
    "While these data generation methods can effectively improve the instruction-following capability of\n",
    "an LLM, they rely on a narrow range of predefined tasks or heuristics under the hood.\n",
    "\n",
    "![](../images/magic1.png)\n",
    "\n",
    "OSS-INSTRUCT leverages a powerful LLM to automatically\n",
    "generate new coding problems by **drawing inspiration from any random code snippets collected from\n",
    "the open source**. In the end, we generate\n",
    "75K synthetic data to finetune CODELLAMA-PYTHON-7B, resulting in Magicoder-CL. While being\n",
    "simple and effective, OSS-INSTRUCT is orthogonal to existing data generation methods, and they\n",
    "can be combined to further push the boundaries of the models’ coding capabilities. Therefore, we\n",
    "continually finetune Magicoder-CL on an open-source Evol-Instruct with 110K entries, producing\n",
    "MagicoderS-CL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918ad1cd-e3fa-47b1-93ad-04a6f3745c32",
   "metadata": {},
   "source": [
    "## OSS-INSTRUCT: Instruction Tuning from Open Source\n",
    "\n",
    "OSS-INSTRUCT is powered by seed code snippets that can be easily collected from open source. In\n",
    "this work, we directly adopt `starcoderdata` as our seed corpus, a filtered version of The Stack dataset that StarCoder is trained on. \n",
    "\n",
    "For each code document from the corpus, we randomly\n",
    "extract 1–15 consecutive lines as the seed snippet for the model to gain inspiration from and produce\n",
    "coding problems. In total, we collected 80K initial seed snippets from 80K code documents. Then, each collected seed code snippet is applied to the prompt template shown below, which a\n",
    "teacher model takes as input and outputs both a coding problem and its solution.\n",
    "\n",
    "![](../images/magic2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2559a64c-9e8b-4d47-aab2-7cd1202b6c3a",
   "metadata": {},
   "source": [
    "## Implementation Details\n",
    "\n",
    "**Data generation** We use `gpt-3.5-turbo-1106` as the foundation model to do OSS-INSTRUCT. We perform greedy decoding to maximize\n",
    "the consistency between the generated problems and solutions.\n",
    "\n",
    "**Training** We employ CODELLAMA-PYTHON-7B and DeepSeek-Coder-Base 6.7B as the base\n",
    "LLMs. To obtain Magicoder series, we first finetune the base models on about 75K synthetic data\n",
    "generated through OSS-INSTRUCT. To obtain MagicoderS,\n",
    "we continue to finetune Magicoder models with the `evol-codealpaca-v1` dataset, an open-source\n",
    "Evol-Instruct implementation containing about 110K samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e781b34-dc15-4834-bc83-d42dc155b5bf",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Evaluation powered by `EvalPlus`.\n",
    "\n",
    "![](../images/magicoder1.png)\n",
    "\n",
    "![](../images/magicoder2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bf9c40-e0f9-4eca-8194-a60fa2279ed7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
