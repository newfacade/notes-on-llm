{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fae9d82e-ae6a-4978-88dc-3e19ae277f37",
   "metadata": {},
   "source": [
    "# WizardCoder\n",
    "\n",
    "```{note}\n",
    "WizardCoder empowers Code LLMs\n",
    "with complex instruction fine-tuning, by adapting the Evol-Instruct method to\n",
    "the domain of code.<br>\n",
    "Through comprehensive experiments on four prominent\n",
    "code generation benchmarks, namely HumanEval, HumanEval+, MBPP, and DS-\n",
    "1000, we unveil the exceptional capabilities of our model. It surpasses all other\n",
    "open-source Code LLMs by a substantial margin.\n",
    "```\n",
    "\n",
    "paper: https://arxiv.org/pdf/2306.08568"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b07276-a749-4acf-b57c-516080054eff",
   "metadata": {},
   "source": [
    "## Evol-Instruct\n",
    "\n",
    "```{tip}\n",
    "Evol-Instruct is a novel method using LLMs instead of humans to automatically mass-produce open-domain instructions of various difficulty levels.\n",
    "```\n",
    "\n",
    "![](../images/wizardlm1.png)\n",
    "\n",
    "Starting from a simple initial instruction “1+1=?”, our method randomly selects In-depth Evolving (blue direction line) or In-breadth Evolving (red direction line) to upgrade the simple instruction to a more complex one or create a new one (to increase diversity). The In-depth Evolving includes five types of operations: add constraints, deepening, concretizing, increase reasoning steps, and complicate input. The In-breadth Evolving generating a completely new instruction based on the given instruction. These six operations are implemented by prompting an LLM with specific prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7895f51e-14d3-4f7c-807d-ebaf8bc7d40f",
   "metadata": {},
   "source": [
    "## WizardLM\n",
    "\n",
    "We start the evolution from a given initial instruction dataset $D^{(0)} = (I^{(0)}_{k}, R^{(0)}_{k})_{1\\le k\\le N}$, where $I^{(0)}_{k}$ is the $k$-th instruction in $D^{(0)}$, $R^{(0)}_{k}$ is the corresponding response for the $k$-th instruction, and $N$ is the number of samples in $D^{(0)}$.\n",
    "\n",
    "In each evolution, we upgrade all the $I^{(t)}$ in $D^{(t)}$ to $I^{(t+1)}$ by applying a\n",
    "LLM instruction evolution prompt, and then use the LLM to generate corresponding responses $R^{(t+1)}$ for the newly evolved $I^{(t+1)}$. Thus, we obtain an evolved instruction dataset $D^{(t+1)}$\n",
    "\n",
    "![](../images/wizardllm2.png)\n",
    "\n",
    "**Prompts of In-Depth Evolving.**\n",
    "    \n",
    "**Prompts of In-Breadth Evolving.**\n",
    "    \n",
    "**Response Generation.** We use the same LLM as for evolving to generate the corresponding\n",
    "responses for the evolved instructions. The generation prompt is `<Here is instruction.>`.\n",
    "    \n",
    "**Elimination Evolving.**\n",
    "\n",
    "**Finetuning the LLM on the Evolved Instructions.** Once all evolutions are done, we will **merge** the initial instruction dataset with evolved instruction\n",
    "data from all epochs and randomly shuffle the samples to create the final fine-tuning dataset. This\n",
    "processing ensures even distribution of instructions of varying difficulty levels in the dataset, maximizing\n",
    "model fine-tuning smoothness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d030e62-4113-441f-b814-38b4c4b22451",
   "metadata": {},
   "source": [
    "## Evol-Instruct Prompts for Code\n",
    "\n",
    "To adapt Evol-Instruct to the realm of code, we made the following modifications to the\n",
    "evolutionary prompt:\n",
    "\n",
    "1. Streamlined the evolutionary instructions by removing deepening, complicating input, and\n",
    "In-Breadth Evolving.\n",
    "\n",
    "2. Simplified the form of evolutionary prompts by unifying the evolutionary prompt template.\n",
    "\n",
    "3. Addressing the specific characteristics of the code domain, we added two evolutionary\n",
    "instructions: code debugging and code time-space complexity constraints.\n",
    "\n",
    "The unified code evolutionary prompt template is as follows:\n",
    "\n",
    "![](../images/wizardcoder1.png)\n",
    "\n",
    "Here, {question} represents the current code instruction awaiting evolution, and {method} is the type\n",
    "of evolution. The five types we used are listed as follows:\n",
    "\n",
    "![](../images/wizardcoder2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ae039e-072b-46bd-bb92-582932a4e829",
   "metadata": {},
   "source": [
    "## Training WizardCoder\n",
    "\n",
    "Initially, we utilize **StarCoder 15B** as\n",
    "the foundation and proceed to fine-tune it using the code instruction-following training set, which\n",
    "was evolved through Evol-Instruct. The prompt format for fine-tuning is outlined as follows:\n",
    "\n",
    "![](../images/wizardcoder3.png)\n",
    "\n",
    "To construct the training dataset, we initialized it with the 20K instruction-following dataset called\n",
    "**Code Alpaca**. We iteratively employ the Evol-Instruct technique on this dataset, after each round of data evolution, we **merge** the evolved data from\n",
    "all previous rounds with the original dataset to finetune StarCoder and assess the pass@1 metric on\n",
    "HumanEval. Once we observe a decline in the pass@1 metric, we will discontinue the usage of\n",
    "Evol-Instruct and choose the model with the highest pass@1 as the ultimate model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1caf61-5989-4bef-8e4c-b0014b644550",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "![](../images/wizard-x.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3729f8d2-0c04-4d49-af80-76cd29ac2a30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
