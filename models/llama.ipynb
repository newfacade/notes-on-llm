{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d12b446-d2fd-4361-85a9-59c953fcae24",
   "metadata": {},
   "source": [
    "# Llama\n",
    "\n",
    "```{note}\n",
    "Llama is a collection of foundation language models ranging from 7B to 65B parameters, we show that it is possible to train state-of-the-art models using publicly available datasets exclusively.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d63d06-5df8-49ba-b65c-a19b8942ad1d",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "More parameters leads to better performance? Recent work shows that, for a given compute budget, the best performances are not achieved by the largest models, but smaller models trained on more data.\n",
    "\n",
    "The focus of this work is to train a series of\n",
    "language models that achieve the best possible performance\n",
    "at various inference budgets, by training\n",
    "on more tokens than what is typically used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c95038e-9710-4d0b-a262-676fd5971a16",
   "metadata": {},
   "source": [
    "## Approach\n",
    "\n",
    "### Pre-training data\n",
    "\n",
    "| Dataset       | Sampling prop | Epochs | Type     |\n",
    "|---------------|---------------|--------|----------|\n",
    "| CommonCrawl   | 67.0%         | 1.10   | Websites |\n",
    "| C4            | 15.0%         | 1.16   | Websites |\n",
    "| Github        | 4.5%          | 0.64   | Code     |\n",
    "| Wikipedia     | 4.5%          | 2.45   | Multi languages  Wiki |\n",
    "| Books         | 4.5%          | 2.23   | Books    |\n",
    "| ArXiv         | 2.5%          | 1.06   | Science  |\n",
    "| StackExchange | 2.0%          | 1.03   | QA       |\n",
    "\n",
    "We tokenize the data with **byte pair encoding (BPE)** algorithm, using the implementation from Sentence-\n",
    "Piece. The entire training dataset contains roughly 1.4T tokens after tokenization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56ccfbf-6bbb-4fe6-a222-fcb61b78ebb5",
   "metadata": {},
   "source": [
    "### Architecture\n",
    "\n",
    "Our network is based on the transformer architecture, here are the main difference with the original architecture:\n",
    "\n",
    "* **Pre-normalization**. To improve the training stability, we normalize the input of each transformer sub-layer, instead of normalizing the output. We use the **RMSNorm** normalizing function.\n",
    "\n",
    "![](../images/llama-2.png)\n",
    "\n",
    "* **SwiGLU activation function.** We replace the ReLU non-linearity by the SwiGLU activation function.\n",
    "\n",
    "* **Rotary Embeddings**. We remove the absolute positional embeddings, and instead, add rotary positional embeddings (RoPE), at each layer of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c714f56d-b501-4b7f-937d-b422d1102cb6",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "\n",
    "Our models are trained using the AdamW optimizer with the following\n",
    "hyper-parameters: $\\beta_{1}=0.9, \\beta_{2}=0.95$. We use a cosine learning rate schedule, such that\n",
    "the final learning rate is equal to 10% of the maximal\n",
    "learning rate. We use a weight decay of 0.1 and\n",
    "gradient clipping of 1.0. We use 2000 warmup steps, and vary the learning rate and batch size with\n",
    "the size of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff53ca7-08b9-4894-835a-73ce238bd1ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
