
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>DAPO &#8212; Notes-on-LLM</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=3ee479438cf8b5e0d341" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=3ee479438cf8b5e0d341" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=3ee479438cf8b5e0d341" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=3ee479438cf8b5e0d341" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=87e54e7c" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=3ee479438cf8b5e0d341" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=3ee479438cf8b5e0d341" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=3ee479438cf8b5e0d341"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'preference/dapo';</script>
    <link rel="icon" href="../_static/github.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Reasoning" href="../reasoning/0.html" />
    <link rel="prev" title="REINFORCE++" href="reinforce%2B%2B.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/Andromede.jpg" class="logo__image only-light" alt="Notes-on-LLM - Home"/>
    <script>document.write(`<img src="../_static/Andromede.jpg" class="logo__image only-dark" alt="Notes-on-LLM - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Introduction
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../base/0.html">Base</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../base/attention.html">Attention Is All You Need</a></li>
<li class="toctree-l2"><a class="reference internal" href="../base/gpt.html">GPT</a></li>
<li class="toctree-l2"><a class="reference internal" href="../base/gpt2.html">GPT2</a></li>
<li class="toctree-l2"><a class="reference internal" href="../base/gpt3.html">GPT3</a></li>
<li class="toctree-l2"><a class="reference internal" href="../base/instruct-gpt.html">InstructGPT</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../models/0.html">Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../models/llama3.html">Llama 3</a></li>
<li class="toctree-l2"><a class="reference internal" href="../models/llama3-source-code.html">Llama 3 Source Code</a></li>
<li class="toctree-l2"><a class="reference internal" href="../models/qwen25.html">Qwen 2.5</a></li>
<li class="toctree-l2"><a class="reference internal" href="../models/qwen25-coder.html">Qwen2.5-Coder</a></li>
<li class="toctree-l2"><a class="reference internal" href="../models/deepseek-v2.html">DeepSeek-V2</a></li>
<li class="toctree-l2"><a class="reference internal" href="../models/deepseek-coder-v2.html">DeepSeek-Coder-V2</a></li>
<li class="toctree-l2"><a class="reference internal" href="../models/deepseek-v3.html">DeepSeek V3</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../techniques/0.html">Techniques</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../techniques/norm.html">Normalization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../techniques/rope.html">RoPE</a></li>
<li class="toctree-l2"><a class="reference internal" href="../techniques/extending.html">Extending context window of LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../techniques/mla.html">Multi-Head Latent Attention</a></li>
<li class="toctree-l2"><a class="reference internal" href="../techniques/deepseek-moe.html">DeepSeekMoE</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../bench/0.html">Benchmarks</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../bench/humaneval.html">HumanEval</a></li>
<li class="toctree-l2"><a class="reference internal" href="../bench/mbpp.html">MBPP</a></li>
<li class="toctree-l2"><a class="reference internal" href="../bench/evalplus.html">EvalPlus</a></li>
<li class="toctree-l2"><a class="reference internal" href="../bench/livecodebench.html">LiveCodeBench</a></li>
<li class="toctree-l2"><a class="reference internal" href="../bench/cruxeval.html">CRUXEval</a></li>
<li class="toctree-l2"><a class="reference internal" href="../bench/bigcodebench.html">BigCodeBench</a></li>
<li class="toctree-l2"><a class="reference internal" href="../bench/swe.html">SWE-bench</a></li>
<li class="toctree-l2"><a class="reference internal" href="../bench/general.html">General Benchmarks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../bench/math-science.html">Math &amp; Science Benchmarks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../bench/alignment.html">Alignment Benchmarks</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../data/0.html">Data</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../data/apps.html">APPS</a></li>
<li class="toctree-l2"><a class="reference internal" href="../data/taco.html">TACO</a></li>
<li class="toctree-l2"><a class="reference internal" href="../data/alphacode.html">AlphaCode</a></li>
<li class="toctree-l2"><a class="reference internal" href="../data/self-instruct.html">SELF-INSTRUCT</a></li>
<li class="toctree-l2"><a class="reference internal" href="../data/code-alpaca.html">Code Alpaca</a></li>
<li class="toctree-l2"><a class="reference internal" href="../data/wizard.html">WizardCoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="../data/magic.html">Magicoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="../data/unicoder.html">UNICODER</a></li>
<li class="toctree-l2"><a class="reference internal" href="../data/opencoder.html">OpenCoder</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../sft/0.html">SFT</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../sft/rs.html">Scaling Relationship on Learning Mathematical Reasoning with Large Language Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../sft/lima.html">LIMA: Less Is More for Alignment</a></li>
</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="0.html">Preference Optimization</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="rlaif-1.html">Constitutional AI: Harmlessness from AI Feedback</a></li>
<li class="toctree-l2"><a class="reference internal" href="rlaif-2.html">RLAIF vs. RLHF</a></li>


<li class="toctree-l2"><a class="reference internal" href="rlcd.html">RLCD</a></li>
<li class="toctree-l2"><a class="reference internal" href="west-of-n.html">West-of-N</a></li>
<li class="toctree-l2"><a class="reference internal" href="ee.html">Efficient Exploration for LLMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="deepseek-grm.html">DeepSeek-GRM</a></li>
<li class="toctree-l2"><a class="reference internal" href="ppo.html">PPO</a></li>
<li class="toctree-l2"><a class="reference internal" href="dpo.html">DPO</a></li>
<li class="toctree-l2"><a class="reference internal" href="grpo.html">Group Relative Policy Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="reinforce%2B%2B.html">REINFORCE++</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">DAPO</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../reasoning/0.html">Reasoning</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../reasoning/cot.html">Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../reasoning/verify.html">Let’s Verify Step by Step</a></li>
<li class="toctree-l2"><a class="reference internal" href="../reasoning/self-correct-rl.html">Training Language Models to Self-Correct via Reinforcement Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../reasoning/deepseek-r1.html">DeepSeek-R1</a></li>
<li class="toctree-l2"><a class="reference internal" href="../reasoning/s1.html">s1: Simple test-time scaling</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../agent/0.html">Agent</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../agent/react.html">REACT</a></li>
<li class="toctree-l2"><a class="reference internal" href="../agent/reflexion.html">Reflexion</a></li>
<li class="toctree-l2"><a class="reference internal" href="../agent/api-bank.html">API-Bank</a></li>
<li class="toctree-l2"><a class="reference internal" href="../agent/code-act.html">CodeAct</a></li>
<li class="toctree-l2"><a class="reference internal" href="../agent/swe-agent.html">SWE-agent</a></li>
<li class="toctree-l2"><a class="reference internal" href="../agent/agentless.html">AGENTLESS</a></li>
<li class="toctree-l2"><a class="reference internal" href="../agent/search-r1.html">Search-R1</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../reference.html">References</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Fpreference/dapo.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/preference/dapo.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>DAPO</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#preliminary">Preliminary</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#removing-kl-divergence">Removing KL Divergence</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rule-based-reward-modeling">Rule-based Reward Modeling</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">DAPO</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#clip-higher">Clip-Higher</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dynamic-sampling">Dynamic Sampling</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#token-level-policy-gradient-loss">Token-Level Policy Gradient Loss</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#overlong-reward-shaping">Overlong Reward Shaping</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#experiments">Experiments</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dataset">Dataset</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#training-details">Training Details</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#main-results">Main Results</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="dapo">
<h1>DAPO<a class="headerlink" href="#dapo" title="Link to this heading">#</a></h1>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>We propose the Decoupled
Clip and Dynamic sAmpling Policy Optimization (<a class="reference external" href="https://arxiv.org/abs/2503.14476">DAPO</a>) algorithm, and fully open-source a
state-of-the-art large-scale RL system that achieves 50 points on AIME 2024 using Qwen2.5-32B
base model.</p>
</div>
<section id="preliminary">
<h2>Preliminary<a class="headerlink" href="#preliminary" title="Link to this heading">#</a></h2>
<p><a class="reference external" href="https://newfacade.github.io/notes-on-reinforcement-learning/17-ppo-trl.html">PPO</a></p>
<p><a class="reference internal" href="grpo.html#grpo"><span class="std std-ref">Group Relative Policy Optimization</span></a></p>
<section id="removing-kl-divergence">
<h3>Removing KL Divergence<a class="headerlink" href="#removing-kl-divergence" title="Link to this heading">#</a></h3>
<p>The KL penalty term is used to regulate the divergence between the online policy and the frozen reference
policy. In the RLHF scenario, the goal of RL is to align the model behavior without diverging too far
from the initial model. However, during training the long-CoT reasoning model, the model distribution can
diverge significantly from the initial model, thus this restriction is not necessary. Therefore, we will exclude
the KL term from our proposed algorithm.</p>
</section>
<section id="rule-based-reward-modeling">
<h3>Rule-based Reward Modeling<a class="headerlink" href="#rule-based-reward-modeling" title="Link to this heading">#</a></h3>
<p>The use of reward model usually suffers from the reward hacking problem. Instead, we directly use
the final accuracy of a verifiable task as the outcome reward:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
R(\hat{y},y) = 
\begin{cases}
1,\quad &amp;\text{isEquivalent}(\hat{y},y)\\
-1,&amp;\text{otherwise}
\end{cases}
\end{split}\]</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Reward hacking is when an AI system finds unexpected ways to maximize its reward function without achieving the intended goal. Different from lack of generalization.</p>
</div>
</section>
</section>
<section id="id1">
<h2>DAPO<a class="headerlink" href="#id1" title="Link to this heading">#</a></h2>
<p>We propose the Decouple Clip and Dynamic sAmpling Policy Optimization (DAPO) algorithm. DAPO samples
a group of outputs <span class="math notranslate nohighlight">\(\{o_i\}_{i=1}^{G}\)</span> for each question <span class="math notranslate nohighlight">\(q\)</span> paired with the answer <span class="math notranslate nohighlight">\(a\)</span> (ground truth), and optimizes the policy via the
following objective:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathcal{J}_{\text{DAPO}}(\theta) = &amp;\mathbb{E}_{(q,a)\sim\mathcal{D},\{o_i\}_{i=1}^{G}\sim \pi_{\theta_{\text{old}}}(\cdot|q)}\\
&amp;\left[\frac{1}{\sum_{i=1}^{G}|o_i|}\sum_{i=1}^{G}\sum_{t=1}^{|o_i|}\min\left(r_{i,t}(\theta)\hat{A}_{i,t},\text{clip}(r_{i,t}(\theta), 1-\epsilon_{\text{low}}, 1+\epsilon_{\text{high}})\hat{A}_{i,t}\right)\right]\\
\text{s.t.}\quad&amp;0&lt;\left|\{o_i|\text{isEquivalent}(a,o_i)\}\right|&lt;G
\end{aligned}
\end{split}\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[
r_{i,t}(\theta) = \frac{\pi_{\theta}(o_{i,t}|q, o_{i,&lt;t})}{\pi_{\theta_{\text{old}}}(o_{i,t}|q, o_{i,&lt;t})},\quad\hat{A}_{i,t} = \frac{R_i -\text{mean}(\{R_i\})_{i=1}^{G}}{\text{std}(\{R_i\})_{i=1}^{G})}
\]</div>
<p>In this section, we will introduce the key techniques associated
with DAPO.</p>
<section id="clip-higher">
<h3>Clip-Higher<a class="headerlink" href="#clip-higher" title="Link to this heading">#</a></h3>
<p>In our initial experiments using naive PPO or GRPO, we observed the entropy collapse phenomenon:
the entropy of the policy decreases quickly as training progresses. The sampled responses of
certain groups tend to be nearly identical.</p>
<p>We identify that the <code class="docutils literal notranslate"><span class="pre">upper</span> <span class="pre">clip</span> <span class="pre">can</span> <span class="pre">restrict</span> <span class="pre">the</span> <span class="pre">exploration</span> <span class="pre">of</span> <span class="pre">the</span> <span class="pre">policy</span></code>. In this case, it
is much easier to make an ‘exploitation token’ more probable, than to uplift the probability of an unlikely
‘exploration token’. Concretely, when <span class="math notranslate nohighlight">\(\epsilon=0.2\)</span>, consider two actions with probabilities <span class="math notranslate nohighlight">\(\pi_{\theta_{\text{old}}}(o_i|q)=0.01\)</span> and <span class="math notranslate nohighlight">\(0.9\)</span>. The maximum possible updated probabilities <span class="math notranslate nohighlight">\(\pi_{\theta_{\text{old}}}(o_i|q)=0.012\)</span> and <span class="math notranslate nohighlight">\(1.08\)</span>, respectively. This implies that for tokens with a higher probability (e.g., <span class="math notranslate nohighlight">\(0.9\)</span>) is less constrained. Conversely,
for low-probability tokens, achieving a non-trivial increase in probability is considerably more challenging.</p>
<p>Adhering to the Clip-Higher strategy, we decouple the lower and higher clipping range as <span class="math notranslate nohighlight">\(\epsilon_{\text{low}}\)</span> and <span class="math notranslate nohighlight">\(\epsilon_{\text{high}}\)</span>.</p>
<figure class="align-default">
<img alt="../_images/dapo1.png" src="../_images/dapo1.png" />
</figure>
</section>
<section id="dynamic-sampling">
<h3>Dynamic Sampling<a class="headerlink" href="#dynamic-sampling" title="Link to this heading">#</a></h3>
<p>We propose to over-sample and filter out prompts with the accuracy equal to 1 and 0, leaving all prompts in the batch with effective gradients and keeping a consistent number
of prompts. Besides, we find that with dynamic sampling the experiment achieves the same performance
faster.</p>
</section>
<section id="token-level-policy-gradient-loss">
<h3>Token-Level Policy Gradient Loss<a class="headerlink" href="#token-level-policy-gradient-loss" title="Link to this heading">#</a></h3>
<p>In the original GRPO algorithm, all samples are assigned the same weight in the loss calculation, tokens within longer responses (which
contain more tokens) may have a disproportionately lower contribution to the overall loss, which can lead to:</p>
<ol class="arabic simple">
<li><p>For high-quality long samples, this effect can impede the model’s ability to learn
reasoning-relevant patterns within them.</p></li>
<li><p>We observe that excessively long samples often exhibit
low-quality patterns such as gibberish and repetitive words.</p></li>
</ol>
<p>Thus, sample-level loss calculation, due to its
inability to effectively penalize those undesirable patterns in long samples, leads to an unhealthy increase in
entropy and response length.</p>
<p>We introduce a <code class="docutils literal notranslate"><span class="pre">Token-level</span> <span class="pre">Policy</span> <span class="pre">Gradient</span> <span class="pre">Loss</span></code> in the long-CoT RL scenario to address the above limitations. In this setting, longer sequences can have more influence on the overall gradient update compared to shorter
sequences. Moreover, from the perspective of individual tokens, if a particular generation pattern can lead to
an increase or decrease in reward, it will be equally prompted or suppressed, regardless of the length of the
response in which it appears.</p>
</section>
<section id="overlong-reward-shaping">
<h3>Overlong Reward Shaping<a class="headerlink" href="#overlong-reward-shaping" title="Link to this heading">#</a></h3>
<p>In RL training, we typically set a maximum length for generation, with overlong samples truncated accordingly. By default, we assign a punitive reward to truncated samples. This approach may introduce noise into the
training process, as a sound reasoning process can be penalized solely due to its excessive length. Such
penalties can potentially confuse the model regarding the validity of its reasoning process.</p>
<p>To investigate the impact of this reward noise, we first apply an Overlong Filtering strategy which <code class="docutils literal notranslate"><span class="pre">masks</span> <span class="pre">the</span> <span class="pre">loss</span> <span class="pre">of</span> <span class="pre">truncated</span> <span class="pre">samples</span></code>. We find that this approach significantly stabilizes training and enhances
performance.</p>
<figure class="align-default">
<img alt="../_images/dapo2.png" src="../_images/dapo2.png" />
</figure>
<p>Furthermore, we propose <code class="docutils literal notranslate"><span class="pre">Soft</span> <span class="pre">Overlong</span> <span class="pre">Punishment</span></code>, this penalty is added to the original rule-based correctness reward, thereby signaling
to the model to avoid excessively long responses.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
R_{\text{length}}(y) = 
\begin{cases}
0, &amp;|y|\le L_{\text{max}} - L_{\text{cache}}\\
\frac{(L_{\text{max}} - L_{\text{cache}}) - |y|}{L_{\text{cache}}},\   &amp;L_{\text{max}} - L_{\text{cache}} &lt; |y| \le L_{\text{max}}\\
-1, &amp;L_{\text{max}}&lt; |y|\\
\end{cases}
\end{split}\]</div>
</section>
</section>
<section id="experiments">
<h2>Experiments<a class="headerlink" href="#experiments" title="Link to this heading">#</a></h2>
<section id="dataset">
<h3>Dataset<a class="headerlink" href="#dataset" title="Link to this heading">#</a></h3>
<p>Our dataset is sourced from the AoPS website and official competition homepages through a combination of
web scraping and manual annotation. After selection and transformation, we obtained the DAPO-Math-17K
dataset, which consists of 17K prompts, each paired with an integer as the answer.</p>
</section>
<section id="training-details">
<h3>Training Details<a class="headerlink" href="#training-details" title="Link to this heading">#</a></h3>
<p>We use naive GRPO as our
baseline algorithm.</p>
<p>For hyper-parameters, we utilize the AdamW ptimizer with a constant learning rate of <span class="math notranslate nohighlight">\(1 × 10^{−6}\)</span>,
incorporating a linear warm-up over 20 rollout steps. For rollout, the prompt batch size is 512 and we sample
16 responses for each prompt. For training, the mini-batch size is set to 512, i.e., 16 gradient updates for
each rollout step. For Overlong Reward Shaping, we set the expected maximum length as 16,384 tokens
and allocate additional 4,096 tokens as the soft punish cache. As for the Clip-Higher mechanism, we set the clipping parameter <span class="math notranslate nohighlight">\(\epsilon_{\text{low}}\)</span>
to 0.2 and <span class="math notranslate nohighlight">\(\epsilon_{\text{high}}\)</span> to 0.28, which effectively balance the trade-off between exploration and exploitation. For
evaluation on AIME, we repeat the evaluation set for 32 times and report avg&#64;32 for results stability. The
inference hyperparameters of evaluation are set to temperature 1.0 and topp 0.7.</p>
</section>
<section id="main-results">
<h3>Main Results<a class="headerlink" href="#main-results" title="Link to this heading">#</a></h3>
<p>Experiments on AIME 2024 demonstrate that DAPO has successfully trained the Qwen-32B Base model into
a powerful reasoning model, achieving performance superior to DeepSeek’s experiments on Qwen2.5-32B
using the R1 approach.</p>
<figure class="align-default">
<img alt="../_images/dapo3.png" src="../_images/dapo3.png" />
</figure>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./preference"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="reinforce%2B%2B.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">REINFORCE++</p>
      </div>
    </a>
    <a class="right-next"
       href="../reasoning/0.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Reasoning</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#preliminary">Preliminary</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#removing-kl-divergence">Removing KL Divergence</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rule-based-reward-modeling">Rule-based Reward Modeling</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">DAPO</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#clip-higher">Clip-Higher</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dynamic-sampling">Dynamic Sampling</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#token-level-policy-gradient-loss">Token-Level Policy Gradient Loss</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#overlong-reward-shaping">Overlong Reward Shaping</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#experiments">Experiments</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dataset">Dataset</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#training-details">Training Details</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#main-results">Main Results</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By newfacade
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=3ee479438cf8b5e0d341"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=3ee479438cf8b5e0d341"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>