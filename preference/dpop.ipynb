{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90215ec3-1e07-4a06-b565-e1d76e4503a1",
   "metadata": {},
   "source": [
    "# DPOP\n",
    "\n",
    "```{note}\n",
    "In this work, first we show theoretically that the standard DPO loss can lead to a reduction\n",
    "of the model’s likelihood of the preferred examples, as long as the relative probability between the\n",
    "preferred and dispreferred classes increases.<br>\n",
    "We design DPO-Positive (DPOP), a new loss function and\n",
    "training procedure which avoids this failure mode.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e501d840-87e8-4f22-b12a-2894ff6e6302",
   "metadata": {},
   "source": [
    "## Failure Mode of DPO\n",
    "\n",
    "The DPO loss\n",
    "\n",
    "$$\\mathcal{L}_{\\text{DPO}}(\\pi_{\\theta};\\pi_{\\text{ref}}) = -\\mathbb{E}_{(x, y_{w},y_{l})\\sim\\mathcal{D}}\\left[\\log\\sigma\\left(\\beta\\log\\frac{\\pi_{\\theta}(y_{w}|x)}{\\pi_{\\text{ref}}(y_{w}|x)} -  \\beta\\log\\frac{\\pi_{\\theta}(y_{l}|x)}{\\pi_{\\text{ref}}(y_{l}|x)}\\right)\\right]$$\n",
    "\n",
    "is a function only of the difference\n",
    "in the log-ratios. which means that we can achieve a low loss value even if $\\pi_{\\text{ratio}}(y_w|x)$ is lowered below\n",
    "1, as long as $\\pi_{\\text{ratio}}(y_l|x)$ is also lowered sufficiently.\n",
    "\n",
    "**Edit Distance 1** Now we provide a specific case in\n",
    "which DPO may cause a decrease in the probability of the better completion. Consider the case of trying\n",
    "to improve a model’s math or reasoning abilities by comparing a completion of “2+2=4” to “2+2=5” which have an edit (Hamming) distance\n",
    "of 1, i.e., all tokens in the completion are the same except for one.\n",
    "\n",
    "Consider two completions with an edit distance of 1 which differ at token $m$, i.e. consider $y_{w}=(t_{1},\\dots,t_{K})$ and $y_{l}=(t_{1},\\dots,t_{m-1},t_{m}',t_{m+1},\\dots, t_{K})$. Denote $y^{<r} = (t_{1},\\dots,t_{r-1})$ and $y^{\\ge r} = (t_{r},\\dots, t_{K})$. Let $s_{i}^{\\{x\\}}$ represent the probability of the $i-$th token in the model’s vocabulary given the input $x$. \n",
    "While the LLM model parameters $\\theta$ are numerous, we restrict our attention to the logits $\\theta_{j}$ where $1\\le j\\le \\text{vocab length}$. The gradient of DPO loss with respect to $\\theta$ is proportional to the following:\n",
    "\n",
    "$$\\nabla_{\\theta}\\mathcal{L}_{DPO}(\\pi_{\\theta};\\pi_{\\text{ref}}) \\propto -\\left[\\nabla_{\\theta}\\log\\pi_{\\theta}(y_{w}|x) - \\nabla_{\\theta}\\log\\pi_{\\theta}(y_{l}|x)\\right]$$\n",
    "\n",
    "We note first that for all tokens from 1 to $m-1$ have no effect on the gradient. Therefore, without loss of generality, assume $m=1$, i.e. $y_{w}$ and $y_{l}$ differ only at the first token. Without loss of generality, we also assume that $t_{k}$ takes vocabulary position 1. Then for each $k>1$:\n",
    "\n",
    "$$\\nabla_{\\theta_{j}}\\log\\pi_{\\theta}(t_{k}|y_{w}^{<k},x) - \\nabla_{\\theta_{j}}\\log\\pi_{\\theta}(t_{k}|y_{l}^{<k},x) = s_{j}^{\\{y_{l}^{<k},x\\}} - s_{j}^{\\{y_{w}^{<k},x\\}}$$\n",
    "\n",
    "As we typically run DPO after SFT, the model is likely to be reasonably well optimised, so we have $s_{j}^{\\{y_{w}^{<k},x\\}} \\le s_{j}^{\\{y_{l}^{<k},x\\}}$ for $j\\ne 1$ and $s_{1}^{\\{y_{w}^{<k},x\\}} \\ge s_{1}^{\\{y_{l}^{<k},x\\}}$, so $\\nabla_{\\theta_{j}}\\mathcal{L}_{DPO}(\\pi_{\\theta};\\pi_{\\text{ref}}) \\le 0$ for $j\\ne 1$ and $\\nabla_{\\theta_{1}}\\mathcal{L}_{DPO}(\\pi_{\\theta};\\pi_{\\text{ref}}) \\ge 0$. We see that the gradient vector is decreasing in the correct logit dimension\n",
    "and increasing in the wrong logit dimensions. Surprisingly, this suggests that under DPO, all tokens that\n",
    "follow a mismatched token should have reduced probability of emitting the correct token when compared to $\\pi_{\\text{ref}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc18d8f8-c73a-4580-8d7e-b28fc0492e14",
   "metadata": {},
   "source": [
    "## DPOP\n",
    "\n",
    "The DPOP loss function:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathcal{L}_{\\text{DPO}}(\\pi_{\\theta};\\pi_{\\text{ref}}) = -\\mathbb{E}_{(x, y_{w},y_{l})\\sim\\mathcal{D}}\\left[\\log\\sigma\\left(\\beta\\log\\frac{\\pi_{\\theta}(y_{w}|x)}{\\pi_{\\text{ref}}(y_{w}|x)} -  \\beta\\log\\frac{\\pi_{\\theta}(y_{l}|x)}{\\pi_{\\text{ref}}(y_{l}|x)}\\right) \\\\\n",
    "- \\lambda\\max\\left(0, \\log\\frac{\\pi_{\\text{ref}}(y_{w}|x)}{\\pi_{\\theta}(y_{w}|x)}\\right)\\right]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $\\lambda > 0$ is a hyperparameter that can be tuned. The model can no longer minimise the loss by significantly reducing\n",
    "the log-likelihood of the dispreferred examples more than it reduces the log-likelihood of the preferred\n",
    "examples; it must also ensure that the log-likelihood of the preferred examples remains high relative to the\n",
    "log-likelihood under the reference model.\n",
    "\n",
    "For our example, if $\\pi_{\\text{ratio}} < 1$, the DPOP gradients become:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\nabla_{\\theta_{j}}&\\left[\\log\\pi_{\\theta}(t_{k}|y_{w}^{<k},x) - \\log\\pi_{\\theta}(t_{k}|y_{l}^{<k},x) -\\lambda\\cdot\\log\\pi_{\\theta}(t_{k}|y_{w}^{<k},x)\\right]\\\\\n",
    "=&\n",
    "\\begin{cases}\n",
    "\\lambda(1 - s_{j}^{\\{y_{w}^{<k},x\\}}) + s_{j}^{\\{y_{l}^{<k},x\\}} - s_{j}^{\\{y_{w}^{<k},x\\}}& j=1\\\\\n",
    "-(\\lambda+1)s_{j}^{\\{y_{w}^{<k},x\\}} + s_{j}^{\\{y_{l}^{<k},x\\}}&j\\ne 1\\\\\n",
    "\\end{cases}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Since $s_{j}^{\\{y_{w}^{<k},x\\}} \\le 1$, for the case $j=1$, the gradient is guaranteed\n",
    "to be positive for a large enough choice of $\\lambda$. Similarly, for the case $j\\ne 1$, the gradient is guaranteed to be\n",
    "negative for a large enough $\\lambda$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8b4dc0-977b-4022-a828-f93c48901a28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
