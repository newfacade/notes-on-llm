Search.setIndex({"alltitles": {"2D case": [[98, "d-case"], [99, "d-case"]], "A Recipe for Instruction Data": [[54, "a-recipe-for-instruction-data"]], "APPS": [[25, "apps"]], "ASCII": [[102, "ascii"]], "ASCII,UNICODE,UTF8": [[102, "ascii-unicode-utf8"]], "Ablation of Attention Mechanisms": [[96, "ablation-of-attention-mechanisms"]], "Ablation of MHA, GQA, and MQA": [[96, "ablation-of-mha-gqa-and-mqa"]], "Ablations on Data Diversity, Quality, and Quantity": [[87, "ablations-on-data-diversity-quality-and-quantity"]], "Absolute position embedding": [[99, "absolute-position-embedding"]], "Active Exploration with a Point Estimate": [[61, "active-exploration-with-a-point-estimate"]], "Active Exploration with an ENN": [[61, "active-exploration-with-an-enn"]], "Active Learning": [[84, "active-learning"]], "Adaptive Margin": [[73, "adaptive-margin"]], "Additional insights": [[43, "additional-insights"]], "Advantage Normalization": [[67, "advantage-normalization"]], "Aligning Language Models with Judgments": [[64, "aligning-language-models-with-judgments"]], "Alignment": [[45, "alignment"], [46, "alignment"]], "Alignment Benchmarks": [[7, "alignment-benchmarks"]], "Alignment Data": [[87, "alignment-data"]], "Alignment Effect on Non-Determinism": [[37, "alignment-effect-on-non-determinism"]], "AlphaCode": [[24, "alphacode"], [41, "alphacode"]], "AlphaCode 2": [[42, "alphacode-2"]], "AlphaCodium": [[43, "alphacodium"]], "An example cell": [[35, "an-example-cell"]], "Analyze and Leverage Diverse Data to its Fullest Potential": [[73, "analyze-and-leverage-diverse-data-to-its-fullest-potential"]], "Appendix": [[59, "appendix"]], "Approach": [[3, "approach"], [4, "approach"], [41, "approach"], [48, "approach"]], "Architecture": [[46, "architecture"], [47, "architecture"], [48, "architecture"]], "Architecture & Tokenizer": [[53, "architecture-tokenizer"]], "Arena-Hard": [[7, "arena-hard"]], "ArmoRM-MoE": [[56, "armorm-moe"]], "Assessment Pipeline": [[61, "assessment-pipeline"]], "Attention": [[1, "attention"], [51, "attention"], [51, "id1"], [52, "attention"], [52, "id1"]], "Attention Is All You Need": [[1, "attention-is-all-you-need"]], "Background": [[99, "background"]], "Background: Rotary Position Embedding (RoPE)": [[94, "background-rotary-position-embedding-rope"], [95, "background-rotary-position-embedding-rope"]], "Background: The REINFORCE Algorithm": [[67, "background-the-reinforce-algorithm"]], "Base": [[0, "base"]], "Base Models": [[53, "base-models"], [54, "base-models"], [84, "base-models"]], "Basic Architecture": [[47, "basic-architecture"]], "Batch Normalization": [[97, "batch-normalization"]], "BatchNorm": [[51, "batchnorm"]], "Benchmarks": [[6, "benchmarks"]], "Boosting Reward Quality with Principles": [[58, "boosting-reward-quality-with-principles"]], "Byte Pair Encoding (BPE)": [[92, "byte-pair-encoding-bpe"]], "CRUXEval": [[9, "cruxeval"]], "Capabilities": [[50, "capabilities"]], "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models": [[78, "chain-of-thought-prompting-elicits-reasoning-in-large-language-models"]], "Charting the Infinite Data Limit and Overfitting": [[38, "charting-the-infinite-data-limit-and-overfitting"]], "Chat Dialog Format": [[50, "chat-dialog-format"]], "ChatFormat": [[51, "chatformat"]], "Citations": [[34, "citations"]], "Classification Task Identification": [[18, "classification-task-identification"], [29, "classification-task-identification"]], "Clip-Higher": [[57, "clip-higher"]], "Clustering": [[41, "clustering"], [42, "clustering"]], "Code": [[50, "code"]], "Code Alpaca": [[8, "code-alpaca"], [8, "id1"], [26, "code-alpaca"], [26, "id1"]], "Code Correctness Evaluation: HumanEval(+) or MBPP(+)": [[10, "code-correctness-evaluation-humaneval-or-mbpp"]], "Code Llama": [[44, "code-llama"]], "Code Llama: Specializing Llama 2 for code": [[44, "code-llama-specializing-llama-2-for-code"]], "Code-Oriented Design Concepts": [[43, "code-oriented-design-concepts"]], "CodeContests fine-tuning dataset": [[24, "codecontests-fine-tuning-dataset"]], "Cold Start": [[80, "cold-start"]], "Communication Balance Loss": [[93, "communication-balance-loss"]], "Comparison Between MLA and MHA": [[96, "comparison-between-mla-and-mha"]], "Comparison of Key-Value Cache": [[96, "comparison-of-key-value-cache"]], "Comparisons of Different RM approaches": [[58, "comparisons-of-different-rm-approaches"]], "Constitutional AI: Harmlessness from AI Feedback": [[68, "constitutional-ai-harmlessness-from-ai-feedback"]], "Contents": [[21, "contents"], [22, "contents"]], "Create a notebook with MyST Markdown": [[35, "create-a-notebook-with-myst-markdown"]], "Critiques, Revisions, and Supervised Learning": [[68, "critiques-revisions-and-supervised-learning"]], "DAPO": [[57, "dapo"], [57, "id1"]], "DPO": [[59, "dpo"]], "DPOP": [[60, "dpop"], [60, "id1"]], "Data": [[8, "data"], [23, "data"], [26, "data"]], "Data Collection": [[45, "data-collection"], [84, "data-collection"]], "Data Composition": [[28, "data-composition"], [54, "data-composition"]], "Data Construction": [[46, "data-construction"], [47, "data-construction"]], "Data Generation": [[18, "data-generation"], [29, "data-generation"]], "Data Mixture": [[54, "data-mixture"]], "Data Processing and Quality Control": [[50, "data-processing-and-quality-control"]], "Dataset": [[5, "dataset"], [44, "dataset"], [57, "dataset"], [63, "dataset"]], "Datasets": [[41, "datasets"]], "Decontamination": [[28, "decontamination"], [54, "decontamination"]], "Decoupled Rotary Position Embedding": [[96, "decoupled-rotary-position-embedding"]], "DeepCoder": [[79, "deepcoder"]], "DeepSeek V3": [[47, "deepseek-v3"]], "DeepSeek-Coder-V2": [[45, "deepseek-coder-v2"]], "DeepSeek-GRM": [[58, "deepseek-grm"]], "DeepSeek-R1": [[80, "deepseek-r1"]], "DeepSeek-R1-Zero: Reinforcement Learning on the Base Model": [[80, "deepseek-r1-zero-reinforcement-learning-on-the-base-model"]], "DeepSeek-R1: Reinforcement Learning with Cold Start": [[80, "deepseek-r1-reinforcement-learning-with-cold-start"]], "DeepSeek-V2": [[46, "deepseek-v2"]], "DeepSeekMoE": [[93, "deepseekmoe"]], "Derivation of \\pi_{r}": [[59, "derivation-of-pi-r"]], "Device-Level Balance Loss": [[93, "device-level-balance-loss"]], "Direct Preference Optimization": [[50, "direct-preference-optimization"], [59, "direct-preference-optimization"]], "Direct extrapolation": [[95, "direct-extrapolation"]], "Discussion": [[46, "discussion"]], "Dynamic Sampling": [[57, "dynamic-sampling"]], "Dynamic Scaling - \u201cDynamic NTK\u201d interpolation": [[94, "dynamic-scaling-dynamic-ntk-interpolation"]], "Efficient Exploration for LLMs": [[61, "efficient-exploration-for-llms"]], "Embeddings and Softmax": [[1, "embeddings-and-softmax"]], "Empirical Results": [[61, "empirical-results"]], "Empirical Results and Basic Power Laws": [[38, "empirical-results-and-basic-power-laws"]], "Encoder and Decoder Stacks": [[1, "encoder-and-decoder-stacks"]], "Epistemic Neural Network": [[61, "epistemic-neural-network"]], "EvalPlus": [[10, "evalplus"]], "Evaluation": [[4, "evaluation"], [14, "evaluation"], [20, "evaluation"], [27, "evaluation"], [32, "evaluation"], [41, "evaluation"], [42, "evaluation"], [53, "evaluation"], [54, "evaluation"], [69, "evaluation"]], "Evaluation Results": [[46, "evaluation-results"], [46, "id3"], [47, "evaluation-results"], [47, "id4"]], "Evaluation of LLMs Should Not Ignore Non-Determinism": [[37, "evaluation-of-llms-should-not-ignore-non-determinism"]], "Evol-Instruct": [[20, "evol-instruct"], [32, "evol-instruct"]], "Evol-Instruct Prompts for Code": [[20, "evol-instruct-prompts-for-code"], [32, "evol-instruct-prompts-for-code"]], "Experimental Results": [[37, "experimental-results"], [45, "experimental-results"]], "Experimental Setup": [[67, "experimental-setup"], [74, "experimental-setup"]], "Experimentation Pipeline": [[61, "experimentation-pipeline"]], "Experiments": [[3, "experiments"], [57, "experiments"], [74, "experiments"], [95, "experiments"]], "Expert-Level Balance Loss": [[93, "expert-level-balance-loss"]], "Exploration Algorithms": [[61, "exploration-algorithms"]], "Extending context window of LLMs": [[94, "extending-context-window-of-llms"]], "Extending context window of large language models via position interpolation": [[95, "extending-context-window-of-large-language-models-via-position-interpolation"]], "FFN": [[100, "ffn"]], "Failure Mode of DPO": [[60, "failure-mode-of-dpo"]], "FeedForward": [[51, "feedforward"], [52, "feedforward"]], "Filtering": [[41, "filtering"], [42, "filtering"]], "Filtering and Postprocessing": [[18, "filtering-and-postprocessing"], [29, "filtering-and-postprocessing"]], "Fine-Grained Expert Segmentation": [[93, "fine-grained-expert-segmentation"]], "Fine-tuning": [[41, "fine-tuning"]], "Finetuning the LM to Follow Instructions": [[18, "finetuning-the-lm-to-follow-instructions"], [29, "finetuning-the-lm-to-follow-instructions"]], "Flipping the Labels": [[73, "flipping-the-labels"]], "Flow stages": [[43, "flow-stages"]], "Formulation": [[98, "formulation"], [99, "formulation"]], "Framework": [[2, "framework"]], "From MLE to RL framework": [[36, "from-mle-to-rl-framework"]], "Functional Correctness": [[12, "functional-correctness"]], "GB2312\u3001GBK\u7b49\u5176\u4ed6\u7f16\u7801": [[102, "gb2312gbk"]], "GOLD": [[36, "gold"]], "GPQA": [[15, "gpqa"]], "GPT": [[2, "gpt"]], "GPT2": [[3, "gpt2"]], "GPT3": [[4, "gpt3"]], "GRPO": [[62, "id3"]], "GSM8K": [[15, "gsm8k"]], "Gated Linear Units (GLU) and Variants": [[100, "gated-linear-units-glu-and-variants"]], "General Benchmarks": [[11, "general-benchmarks"]], "General form": [[98, "general-form"], [99, "general-form"]], "Generation": [[51, "generation"]], "Generator": [[84, "generator"]], "Gradient of DPO Loss": [[59, "gradient-of-dpo-loss"]], "Group Relative Policy Optimization": [[62, "group-relative-policy-optimization"]], "High-level methodology": [[5, "high-level-methodology"]], "How Various Factors Influence Non-Determinism?": [[37, "how-various-factors-influence-non-determinism"]], "How to Better Model Human Preference?": [[73, "how-to-better-model-human-preference"]], "Human Evaluation": [[87, "human-evaluation"]], "Human Preference Data Collection": [[49, "human-preference-data-collection"]], "HumanEval": [[12, "humaneval"]], "Hyper-Parameters": [[46, "hyper-parameters"]], "IFEval": [[7, "ifeval"]], "Impacts of Different Data on RM Performance": [[73, "impacts-of-different-data-on-rm-performance"]], "Implementation Details": [[14, "implementation-details"], [27, "implementation-details"]], "Incorporating Judgments for Alignment": [[64, "incorporating-judgments-for-alignment"]], "Inference-Time Scaling with SPCT": [[58, "inference-time-scaling-with-spct"]], "Infilling": [[44, "infilling"]], "Initialization": [[74, "initialization"], [75, "initialization"]], "Input Representation": [[3, "input-representation"]], "Installation": [[13, "installation"]], "Instance Generation": [[18, "instance-generation"], [29, "instance-generation"]], "Instruct GPT": [[63, "instruct-gpt"]], "Instruct Models": [[54, "instruct-models"]], "InstructGPT": [[5, "instructgpt"]], "Instruction Following Ability Results": [[74, "instruction-following-ability-results"]], "Instruction Following Training": [[74, "instruction-following-training"]], "Instruction Generation": [[18, "instruction-generation"], [29, "instruction-generation"]], "Instruction Selection": [[75, "instruction-selection"]], "Instruction fine-tuning": [[44, "instruction-fine-tuning"]], "Instruction-tuned Model": [[53, "instruction-tuned-model"]], "Introduction": [[14, "introduction"], [27, "introduction"], [33, "introduction"], [39, "introduction"], [44, "introduction"], [48, "introduction"], [64, "introduction"], [70, "introduction"], [95, "introduction"]], "Iterative Fine-Tuning": [[49, "iterative-fine-tuning"]], "Iterative RL with GRPO": [[62, "iterative-rl-with-grpo"]], "Iterative Rounds": [[50, "iterative-rounds"]], "Iterative Training": [[75, "iterative-training"]], "Judgment Annotation": [[75, "judgment-annotation"]], "LIMA: Less Is More for Alignment": [[87, "lima-less-is-more-for-alignment"]], "LORA": [[104, "lora"]], "Label Smoothing": [[73, "label-smoothing"]], "Large scale sampling": [[41, "large-scale-sampling"]], "Large-scale Supervision": [[84, "large-scale-supervision"]], "Layer Normalization": [[97, "layer-normalization"]], "LayerNorm": [[51, "layernorm"]], "Learn more": [[34, "learn-more"]], "Learning Pipeline": [[61, "learning-pipeline"]], "Learning from Contrasting": [[64, "learning-from-contrasting"]], "Let\u2019s Verify Step by Step": [[84, "lets-verify-step-by-step"]], "LiveCodeBench": [[13, "livecodebench"]], "Llama": [[48, "llama"]], "Llama 2": [[49, "llama-2"]], "Llama 3": [[50, "llama-3"]], "Llama 3 Source Code": [[52, "llama-3-source-code"]], "Llama Factory": [[103, "llama-factory"]], "Llama implementation": [[95, "llama-implementation"]], "Llama3": [[51, "llama3"]], "Load Balance Consideration": [[93, "load-balance-consideration"]], "Logic-RL": [[81, "logic-rl"]], "Long Context Extension": [[47, "long-context-extension"]], "Long context fine-tuning": [[44, "long-context-fine-tuning"]], "Loss": [[59, "loss"]], "Loss of High Frequency information - \u201cNTK-aware\u201d interpolation": [[94, "loss-of-high-frequency-information-ntk-aware-interpolation"]], "Loss of Relative Local Distances - \u201cNTK-by-parts\u201d interpolation": [[94, "loss-of-relative-local-distances-ntk-by-parts-interpolation"]], "Low-Rank Compression for Queries": [[96, "low-rank-compression-for-queries"]], "Low-Rank Key-Value Joint Compression": [[96, "low-rank-key-value-joint-compression"]], "MATH": [[15, "math"]], "MATH 500": [[15, "math-500"]], "MBPP": [[16, "mbpp"]], "MMLU": [[11, "mmlu"]], "MMLU-Pro": [[11, "mmlu-pro"]], "MMLU-Redux": [[11, "mmlu-redux"]], "Magicoder": [[14, "magicoder"], [27, "magicoder"]], "Main Result": [[68, "main-result"]], "Main Results": [[57, "main-results"], [68, "main-results"]], "Markdown Files": [[34, "markdown-files"]], "Math & Science Benchmarks": [[15, "math-science-benchmarks"]], "Measuring the Strength of Preferences": [[73, "measuring-the-strength-of-preferences"]], "Method": [[68, "method"], [68, "id1"], [75, "method"]], "Methodology": [[39, "methodology"], [69, "methodology"]], "Methods": [[84, "methods"]], "Methods and experimental details": [[5, "methods-and-experimental-details"]], "Mini-Batch Updates": [[67, "mini-batch-updates"]], "Model": [[3, "model"], [51, "model"], [52, "model"]], "Model Accuracy VS. Augmented Data Count": [[90, "model-accuracy-vs-augmented-data-count"]], "Model Accuracy VS. Pre-training Loss": [[90, "model-accuracy-vs-pre-training-loss"]], "Model Accuracy VS. Supervised Data Count": [[90, "model-accuracy-vs-supervised-data-count"]], "Model Architecture": [[1, "model-architecture"]], "Model Averaging": [[50, "model-averaging"]], "Model Fine-tuning (Iterative Training)": [[75, "model-fine-tuning-iterative-training"]], "Model and Architectures": [[4, "model-and-architectures"]], "Modeling": [[50, "modeling"]], "Models": [[5, "models"], [40, "models"]], "Multi-Head Attention": [[1, "multi-head-attention"]], "Multi-Head Latent Attention": [[96, "multi-head-latent-attention"]], "Multi-Token Prediction": [[47, "multi-token-prediction"]], "NLP\u5b9e\u4f8b": [[92, "nlp"]], "Normalization": [[51, "normalization"], [97, "normalization"]], "Notebooks with MyST Markdown": [[35, "notebooks-with-myst-markdown"]], "OOD Generalization": [[84, "ood-generalization"]], "OSS-INSTRUCT: Instruction Tuning from Open Source": [[14, "oss-instruct-instruction-tuning-from-open-source"], [27, "oss-instruct-instruction-tuning-from-open-source"]], "Off-policy policy gradient": [[36, "off-policy-policy-gradient"]], "Offline Reinforcement Learning": [[53, "offline-reinforcement-learning"]], "Online Reinforcement Learning": [[53, "online-reinforcement-learning"]], "OpenCoder": [[28, "opencoder"]], "OpenRLHF": [[65, "openrlhf"], [105, "openrlhf"]], "Optimizer": [[48, "optimizer"]], "Outcome Supervision RL with GRPO": [[62, "outcome-supervision-rl-with-grpo"]], "Outcome-supervised Reward Models (ORMs)": [[84, "outcome-supervised-reward-models-orms"]], "Overall Self-Alignment Algorithm": [[74, "overall-self-alignment-algorithm"]], "Overall System": [[42, "overall-system"]], "Overlong Reward Shaping": [[57, "overlong-reward-shaping"]], "Overview": [[43, "overview"]], "PPO": [[66, "ppo"]], "PPO Review": [[62, "ppo-review"]], "PPO implementation detail": [[65, "ppo-implementation-detail"]], "PPO-Clip Integration": [[67, "ppo-clip-integration"]], "Parameter and Compute Scaling of Transformers": [[38, "parameter-and-compute-scaling-of-transformers"]], "Passive Exploration": [[61, "passive-exploration"]], "Performance with Dataset Size and Compute": [[38, "performance-with-dataset-size-and-compute"]], "Performance with Non-Embedding Parameter Count N": [[38, "performance-with-non-embedding-parameter-count-n"]], "Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero": [[80, "performance-self-evolution-process-and-aha-moment-of-deepseek-r1-zero"]], "Point Estimate": [[61, "point-estimate"]], "Policy and Fine-Tuning": [[42, "policy-and-fine-tuning"]], "Position interpolation": [[94, "position-interpolation"]], "Position-wise Feed-Forward Networks": [[1, "position-wise-feed-forward-networks"]], "Positional Encoding": [[1, "positional-encoding"]], "Positional Interpolation": [[95, "positional-interpolation"]], "Post Training": [[28, "post-training"]], "Post-Training": [[47, "post-training"], [50, "post-training"]], "Post-trained Language Model": [[50, "post-trained-language-model"]], "Post-training": [[53, "post-training"], [54, "post-training"]], "Post-training Data": [[50, "post-training-data"]], "Pre-Training": [[46, "pre-training"], [47, "pre-training"]], "Pre-trained Language Model": [[50, "pre-trained-language-model"]], "Pre-training": [[53, "pre-training"], [54, "pre-training"]], "Pre-training data": [[48, "pre-training-data"]], "Preference Data": [[50, "preference-data"]], "Preference Labeling with LLMs": [[69, "preference-labeling-with-llms"]], "Preference Optimization": [[55, "preference-optimization"]], "Preliminaries": [[59, "preliminaries"], [72, "preliminaries"], [73, "preliminaries"]], "Preliminaries and Problem Setup": [[83, "preliminaries-and-problem-setup"]], "Preliminaries: Mixture-of-Experts for Transformers": [[93, "preliminaries-mixture-of-experts-for-transformers"]], "Preliminaries: Standard Multi-Head Attention": [[96, "preliminaries-standard-multi-head-attention"]], "Preliminary": [[57, "preliminary"], [99, "preliminary"]], "Pretrain": [[49, "pretrain"]], "Pretraining": [[28, "pretraining"]], "Pretraining Data": [[28, "pretraining-data"]], "Problem Setting": [[64, "problem-setting"]], "Process Supervision RL with GRPO": [[62, "process-supervision-rl-with-grpo"]], "Process vs Outcome Supervision": [[84, "process-vs-outcome-supervision"]], "Process-supervised Reward Models (PRMs)": [[84, "process-supervised-reward-models-prms"]], "Prompting Techniques": [[69, "prompting-techniques"]], "Properties of RoPE": [[99, "properties-of-rope"]], "Proposed approach": [[99, "proposed-approach"]], "Quick Start": [[10, "quick-start"]], "Quickly add YAML metadata for MyST Notebooks": [[35, "quickly-add-yaml-metadata-for-myst-notebooks"]], "Qwen 2.5": [[53, "qwen-2-5"]], "Qwen2.5-Coder": [[54, "qwen2-5-coder"]], "REINFORCE++": [[67, "reinforce"]], "REINFORCE++ Enhancements": [[67, "reinforce-enhancements"]], "RETHINKING DATA SELECTION AT SCALE: RANDOM SELECTION IS ALMOST ALL YOU NEED": [[88, "rethinking-data-selection-at-scale-random-selection-is-almost-all-you-need"]], "RL on Incorrect Synthetic Data Scales the Efficiency of LLM Math Reasoning by Eight-Fold": [[89, "rl-on-incorrect-synthetic-data-scales-the-efficiency-of-llm-math-reasoning-by-eight-fold"]], "RLAIF vs. RLHF": [[69, "rlaif-vs-rlhf"], [69, "id1"]], "RLCD": [[70, "rlcd"], [70, "id1"]], "RLHF": [[49, "rlhf"]], "RMSNorm": [[51, "rmsnorm"], [52, "rmsnorm"], [97, "rmsnorm"]], "RS-DPO": [[71, "rs-dpo"]], "RSO": [[72, "rso"]], "RSO APPROACH": [[72, "rso-approach"]], "Reasoning": [[77, "reasoning"]], "Reasoning data curation to create s1K": [[82, "reasoning-data-curation-to-create-s1k"]], "Reasoning-oriented Reinforcement Learning": [[80, "reasoning-oriented-reinforcement-learning"]], "References": [[85, "references"]], "Reinforcement Learning": [[45, "reinforcement-learning"], [46, "reinforcement-learning"], [47, "reinforcement-learning"]], "Reinforcement Learning Algorithm": [[80, "reinforcement-learning-algorithm"]], "Reinforcement Learning for all Scenarios": [[80, "reinforcement-learning-for-all-scenarios"]], "Reinforcement Learning from AI Feedback": [[68, "reinforcement-learning-from-ai-feedback"], [69, "reinforcement-learning-from-ai-feedback"]], "Reinforcement learning (RL)": [[63, "reinforcement-learning-rl"]], "Rejection Sampling and Supervised Fine-Tuning": [[80, "rejection-sampling-and-supervised-fine-tuning"]], "Rejective Fine-Tuning (Cold Start)": [[58, "rejective-fine-tuning-cold-start"]], "Related Work": [[76, "related-work"]], "Removing KL Divergence": [[57, "removing-kl-divergence"]], "Response Pair Construction": [[75, "response-pair-construction"]], "Results": [[5, "results"], [50, "results"], [69, "results"], [82, "results"]], "Results on Reward Modeling Benchmarks": [[58, "results-on-reward-modeling-benchmarks"]], "Reward": [[36, "reward"]], "Reward Model Architectures and Training": [[61, "reward-model-architectures-and-training"]], "Reward Modeling": [[49, "reward-modeling"], [50, "reward-modeling"], [80, "reward-modeling"]], "Reward Modeling Ability Results": [[74, "reward-modeling-ability-results"]], "Reward Normalization and Clipping": [[67, "reward-normalization-and-clipping"]], "Reward modeling (RM)": [[63, "reward-modeling-rm"]], "RewardBench": [[17, "rewardbench"]], "RoPE": [[51, "rope"], [52, "rope"], [98, "rope"]], "RoPE \u7684\u8fdc\u7a0b\u8870\u51cf": [[94, "rope"]], "Rotary Positional Embeddings (RoPE)": [[99, "rotary-positional-embeddings-rope"]], "Rule-Based RL": [[58, "rule-based-rl"]], "Rule-based Reward Modeling": [[57, "rule-based-reward-modeling"]], "SCoRe: Self-Correction via Multi-Turn Reinforcement Learning": [[83, "score-self-correction-via-multi-turn-reinforcement-learning"]], "SELF-INSTRUCT": [[18, "self-instruct"], [29, "self-instruct"]], "SFT": [[49, "sft"], [86, "sft"], [103, "sft"], [105, "sft"]], "SFT Data": [[50, "sft-data"]], "STATISTICAL REJECTION SAMPLING ALGORITHM": [[72, "statistical-rejection-sampling-algorithm"]], "Sample Roles and Directives": [[34, "sample-roles-and-directives"]], "Sampling": [[42, "sampling"]], "Scaled Dot-Product Attention": [[1, "scaled-dot-product-attention"]], "Scaling Effect on Non-Determinism": [[37, "scaling-effect-on-non-determinism"]], "Scaling Laws for Neural Language Models": [[38, "scaling-laws-for-neural-language-models"]], "Scaling Laws with Model Size and Training Time": [[38, "scaling-laws-with-model-size-and-training-time"]], "Scaling Relationship on Learning Mathematical Reasoning with Large Language Models": [[90, "scaling-relationship-on-learning-mathematical-reasoning-with-large-language-models"]], "Scoring Model": [[42, "scoring-model"]], "Secrets of RLHF in Large Language Models: Reward Modeling": [[73, "secrets-of-rlhf-in-large-language-models-reward-modeling"]], "Self-Instruction Creation": [[74, "self-instruction-creation"]], "Self-Principled Critique Tuning (SPCT)": [[58, "self-principled-critique-tuning-spct"]], "Self-Rewarding Language Models": [[74, "self-rewarding-language-models"]], "Self-Taught Evaluators": [[75, "self-taught-evaluators"]], "Self-Training for Preference Modeling": [[76, "self-training-for-preference-modeling"]], "SentencePiece": [[101, "sentencepiece"]], "Shared Expert Isolation": [[93, "shared-expert-isolation"]], "Small-scale Synthetic Supervision": [[84, "small-scale-synthetic-supervision"]], "Stage I: Training a Model Initialization to Prevent Collapse": [[83, "stage-i-training-a-model-initialization-to-prevent-collapse"]], "Stage II: Multi-Turn RL with Reward Shaping": [[83, "stage-ii-multi-turn-rl-with-reward-shaping"]], "Stage-1: Multi-Objective Reward Modeling": [[56, "stage-1-multi-objective-reward-modeling"]], "Stage-2: Mixture-of-Experts Aggregation of Reward Objectives": [[56, "stage-2-mixture-of-experts-aggregation-of-reward-objectives"]], "Stanford Alpaca": [[8, "stanford-alpaca"], [26, "stanford-alpaca"]], "Summarization": [[3, "summarization"]], "Supervised Fine-Tuning": [[45, "supervised-fine-tuning"], [46, "supervised-fine-tuning"], [47, "supervised-fine-tuning"]], "Supervised Fine-tuning": [[53, "supervised-fine-tuning"]], "Supervised Finetuning": [[50, "supervised-finetuning"]], "Supervised fine-tuning": [[2, "supervised-fine-tuning"]], "Supervised fine-tuning (SFT)": [[63, "supervised-fine-tuning-sft"]], "Surface Patterns in Non-Determinism Generation?": [[37, "surface-patterns-in-non-determinism-generation"]], "SwiGLU": [[100, "swiglu"]], "SwiGLU activation function": [[51, "swiglu-activation-function"], [52, "swiglu-activation-function"]], "Swish": [[100, "swish"]], "TACO": [[19, "taco"], [30, "taco"]], "Takeaway": [[45, "takeaway"], [46, "takeaway"], [47, "takeaway"], [50, "takeaway"], [53, "takeaway"], [54, "takeaway"], [98, "takeaway"]], "Takeaways": [[73, "takeaways"], [84, "takeaways"]], "Task-specific input transformations": [[2, "task-specific-input-transformations"]], "Techniques": [[91, "techniques"]], "Temperature Effect on Non-Determinism": [[37, "temperature-effect-on-non-determinism"]], "Test-time scaling": [[82, "test-time-scaling"]], "The Factors of Math Reasoning Ability in Supervised LLM": [[90, "the-factors-of-math-reasoning-ability-in-supervised-llm"]], "The GOLD algorithm": [[36, "the-gold-algorithm"]], "The Need for Interpretable Reward Models": [[56, "the-need-for-interpretable-reward-models"]], "The Proposed Flow": [[43, "the-proposed-flow"]], "Tiktoken": [[51, "tiktoken"]], "Token-Dropping Strategy": [[93, "token-dropping-strategy"]], "Token-Level KL Penalty": [[67, "token-level-kl-penalty"]], "Token-Level Policy Gradient Loss": [[57, "token-level-policy-gradient-loss"]], "Tokenizer": [[51, "tokenizer"], [51, "id2"]], "Training Dataset": [[3, "training-dataset"], [4, "training-dataset"]], "Training Details": [[28, "training-details"], [57, "training-details"]], "Training LIMA": [[87, "training-lima"]], "Training Language Models to Self-Correct via Reinforcement Learning": [[83, "training-language-models-to-self-correct-via-reinforcement-learning"]], "Training Policy": [[54, "training-policy"], [54, "id8"]], "Training Template": [[80, "training-template"]], "Training WizardCoder": [[20, "training-wizardcoder"], [32, "training-wizardcoder"]], "Transformer": [[51, "transformer"], [52, "transformer"]], "Two-stage Instruction-Tuning": [[28, "two-stage-instruction-tuning"]], "UNICODER": [[31, "unicoder"], [31, "id2"]], "UNICODER-INSTRUCT": [[31, "unicoder-instruct"]], "UTF-16\u3001UTF-32\u7b49": [[102, "utf-16utf-32"]], "UTF-8": [[102, "utf-8"]], "Unigram Language Model (ULM)": [[101, "unigram-language-model-ulm"]], "Unpinning Principles from Understanding to Generation": [[58, "unpinning-principles-from-understanding-to-generation"]], "Unsupervised pre-training": [[2, "unsupervised-pre-training"]], "Weak to Strong Generalization": [[39, "weak-to-strong-generalization"]], "West-of-N": [[76, "west-of-n"]], "West-of-N Self-Training": [[76, "west-of-n-self-training"]], "What is MyST?": [[34, "what-is-myst"]], "What is the Full Potential of Non-Determinism?": [[37, "what-is-the-full-potential-of-non-determinism"]], "Why KV cache": [[96, "why-kv-cache"]], "Why Layer Normalization": [[97, "why-layer-normalization"]], "Why decoder-only": [[2, "why-decoder-only"]], "WizardCoder": [[20, "wizardcoder"], [32, "wizardcoder"]], "WizardLM": [[20, "wizardlm"], [32, "wizardlm"]], "WordPiece": [[101, "wordpiece"]], "WordPiece, ULM and SentencePiece": [[101, "wordpiece-ulm-and-sentencepiece"]], "YaRN": [[94, "id5"]], "YaRN: Efficient ContextWindow Extension of Large Language Models": [[94, "yarn-efficient-contextwindow-extension-of-large-language-models"]], "methodology": [[63, "methodology"]], "s1: Simple test-time scaling": [[82, "s1-simple-test-time-scaling"]], "unicode": [[102, "unicode"]], "\u4e3a\u4ec0\u4e48\u4f1a\u4e71\u7801": [[102, "id1"]], "\u521d\u8bc6BPE": [[92, "bpe"]], "\u603b\u7ed3": [[102, "id2"]], "\u662f\u4e0d\u662f\u8d2a\u5fc3\u7b97\u6cd5\uff1f": [[92, "id2"]], "\u672c\u5730 Evaluate": [[10, "evaluate"], [13, "evaluate"]], "\u7f16\u7801\u548c\u89e3\u7801": [[92, "id1"]], "\u9ad8\u9891\u5916\u63a8\u4f4e\u9891\u5185\u63d2": [[94, "id4"]]}, "docnames": ["base/0", "base/attention", "base/gpt", "base/gpt2", "base/gpt3", "base/instruct-gpt", "bench/0", "bench/alignment", "bench/code-alpaca", "bench/cruxeval", "bench/evalplus", "bench/general", "bench/humaneval", "bench/livecodebench", "bench/magic", "bench/math-science", "bench/mbpp", "bench/reward-bench", "bench/self-instruct", "bench/taco", "bench/wizard", "content", "content-Copy1", "data/0", "data/alphacode", "data/apps", "data/code-alpaca", "data/magic", "data/opencoder", "data/self-instruct", "data/taco", "data/unicoder", "data/wizard", "intro", "markdown", "markdown-notebooks", "methodology/gold", "methodology/non-determin", "methodology/scaling-law", "methodology/weak-to-strong", "models/0", "models/alphacode", "models/alphacode2", "models/alphacodium", "models/codellama", "models/deepseek-coder-v2", "models/deepseek-v2", "models/deepseek-v3", "models/llama", "models/llama2", "models/llama3", "models/llama3-code copy", "models/llama3-source-code", "models/qwen25", "models/qwen25-coder", "preference/0", "preference/armo", "preference/dapo", "preference/deepseek-grm", "preference/dpo", "preference/dpop", "preference/ee", "preference/grpo", "preference/instruct-gpt", "preference/judge", "preference/openrlhf", "preference/ppo", "preference/reinforce++", "preference/rlaif-1", "preference/rlaif-2", "preference/rlcd", "preference/rs-dpo", "preference/rso", "preference/secret-rm", "preference/self-reward", "preference/self-taught", "preference/west-of-n", "reasoning/0", "reasoning/cot", "reasoning/deepcoder", "reasoning/deepseek-r1", "reasoning/logic-rl", "reasoning/s1", "reasoning/self-correct-rl", "reasoning/verify", "reference", "sft/0", "sft/lima", "sft/random", "sft/rl-eight-fold", "sft/rs", "techniques/0", "techniques/bpe", "techniques/deepseek-moe", "techniques/extending", "techniques/extending-old", "techniques/mla", "techniques/norm", "techniques/rope", "techniques/rope-old", "techniques/swiglu", "techniques/tokenize", "techniques/unicode-utf8", "tools/llama-factory", "tools/lora", "tools/openrlhf"], "envversion": {"sphinx": 61, "sphinx.domains.c": 3, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 9, "sphinx.domains.index": 1, "sphinx.domains.javascript": 3, "sphinx.domains.math": 2, "sphinx.domains.python": 4, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.intersphinx": 1, "sphinxcontrib.bibtex": 9}, "filenames": ["base/0.ipynb", "base/attention.ipynb", "base/gpt.ipynb", "base/gpt2.ipynb", "base/gpt3.ipynb", "base/instruct-gpt.ipynb", "bench/0.ipynb", "bench/alignment.ipynb", "bench/code-alpaca.ipynb", "bench/cruxeval.ipynb", "bench/evalplus.ipynb", "bench/general.ipynb", "bench/humaneval.ipynb", "bench/livecodebench.ipynb", "bench/magic.ipynb", "bench/math-science.ipynb", "bench/mbpp.ipynb", "bench/reward-bench.ipynb", "bench/self-instruct.ipynb", "bench/taco.ipynb", "bench/wizard.ipynb", "content.ipynb", "content-Copy1.ipynb", "data/0.ipynb", "data/alphacode.ipynb", "data/apps.ipynb", "data/code-alpaca.ipynb", "data/magic.ipynb", "data/opencoder.ipynb", "data/self-instruct.ipynb", "data/taco.ipynb", "data/unicoder.ipynb", "data/wizard.ipynb", "intro.md", "markdown.md", "markdown-notebooks.md", "methodology/gold.ipynb", "methodology/non-determin.ipynb", "methodology/scaling-law.ipynb", "methodology/weak-to-strong.ipynb", "models/0.ipynb", "models/alphacode.ipynb", "models/alphacode2.ipynb", "models/alphacodium.ipynb", "models/codellama.ipynb", "models/deepseek-coder-v2.ipynb", "models/deepseek-v2.ipynb", "models/deepseek-v3.ipynb", "models/llama.ipynb", "models/llama2.ipynb", "models/llama3.ipynb", "models/llama3-code copy.ipynb", "models/llama3-source-code.ipynb", "models/qwen25.ipynb", "models/qwen25-coder.ipynb", "preference/0.ipynb", "preference/armo.ipynb", "preference/dapo.ipynb", "preference/deepseek-grm.ipynb", "preference/dpo.ipynb", "preference/dpop.ipynb", "preference/ee.ipynb", "preference/grpo.ipynb", "preference/instruct-gpt.ipynb", "preference/judge.ipynb", "preference/openrlhf.ipynb", "preference/ppo.ipynb", "preference/reinforce++.ipynb", "preference/rlaif-1.ipynb", "preference/rlaif-2.ipynb", "preference/rlcd.ipynb", "preference/rs-dpo.ipynb", "preference/rso.ipynb", "preference/secret-rm.ipynb", "preference/self-reward.ipynb", "preference/self-taught.ipynb", "preference/west-of-n.ipynb", "reasoning/0.ipynb", "reasoning/cot.ipynb", "reasoning/deepcoder.ipynb", "reasoning/deepseek-r1.ipynb", "reasoning/logic-rl.ipynb", "reasoning/s1.ipynb", "reasoning/self-correct-rl.ipynb", "reasoning/verify.ipynb", "reference.ipynb", "sft/0.ipynb", "sft/lima.ipynb", "sft/random.ipynb", "sft/rl-eight-fold.ipynb", "sft/rs.ipynb", "techniques/0.ipynb", "techniques/bpe.ipynb", "techniques/deepseek-moe.ipynb", "techniques/extending.ipynb", "techniques/extending-old.ipynb", "techniques/mla.ipynb", "techniques/norm.ipynb", "techniques/rope.ipynb", "techniques/rope-old.ipynb", "techniques/swiglu.ipynb", "techniques/tokenize.ipynb", "techniques/unicode-utf8.ipynb", "tools/llama-factory.ipynb", "tools/lora.ipynb", "tools/openrlhf.ipynb"], "indexentries": {}, "objects": {}, "objnames": {}, "objtypes": {}, "terms": {"": [2, 3, 5, 10, 11, 15, 28, 34, 35, 36, 38, 39, 43, 44, 46, 49, 50, 51, 53, 54, 56, 57, 59, 60, 61, 62, 63, 67, 68, 69, 70, 72, 73, 75, 82, 83, 85, 90, 92, 93, 94, 95, 101], "0": [1, 2, 10, 12, 13, 18, 20, 28, 29, 32, 36, 37, 38, 41, 42, 45, 46, 47, 48, 49, 50, 51, 52, 56, 57, 59, 60, 62, 64, 65, 68, 69, 72, 73, 74, 76, 87, 90, 93, 94, 95, 97, 98, 99, 100, 102, 103, 104], "000": [11, 15, 19, 25, 30, 41, 44, 82, 87], "0000": [95, 102], "0000j": 95, "0001": 102, "0010": 102, "003": [8, 26], "0041": 102, "005": 12, "007f": 102, "0080": 102, "01": [57, 65], "0100j": 95, "012": 57, "01825": [34, 85], "02120": [14, 27, 34, 85], "02155": [34, 85], "02954": [34, 85], "03": 48, "03065": [34, 85], "0314": 7, "03300": [34, 85], "03341": [34, 85], "03374": [34, 85], "03762": [34, 85], "04434": [34, 85], "0461": 95, "04805": [34, 85], "0596": 95, "0596j": 95, "06": 48, "0674": 95, "0674j": 95, "07436": [34, 85], "076": 38, "07911": [34, 85], "07974": [34, 85], "07ff": 102, "08": 57, "0800": 102, "08083": [34, 85], "08361": [34, 85], "08568": [20, 32], "096": [44, 57], "09864": [34, 85], "0xxxxxxx": 102, "1": [1, 2, 5, 7, 8, 10, 12, 13, 14, 18, 19, 20, 22, 26, 27, 28, 29, 30, 31, 32, 34, 36, 37, 38, 41, 42, 44, 45, 46, 47, 48, 50, 51, 52, 53, 54, 57, 58, 59, 60, 61, 62, 63, 64, 65, 67, 68, 69, 72, 73, 74, 75, 76, 82, 83, 84, 85, 87, 90, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103], "10": [10, 13, 15, 25, 28, 37, 38, 42, 44, 45, 46, 48, 50, 54, 57, 62, 69, 73, 92, 93, 102, 103], "100": [3, 8, 11, 12, 26, 28, 42, 44, 54, 56, 61, 84], "1000": [20, 32, 47, 84, 94, 95], "10000": [1, 42, 51, 52, 94, 95, 98, 99], "100000": 65, "10000000": 102, "100k": 46, "10111000": 102, "1024": [54, 65], "102400": 46, "1048576000": 46, "105": 13, "10509": [34, 85], "10560": [18, 29], "106": 13, "107": 13, "10k": 46, "10x": 4, "10xxxxxx": 102, "11": [13, 99], "1106": [14, 27], "110k": [14, 27], "110xxxxx": 102, "110\u8868\u793a\u9700\u8981\u4e24\u4e2a\u5b57\u8282\u8868\u793a\u5f53\u524d\u5b57\u7b26": 102, "1110": 102, "1110xxxx": 102, "1110\u8868\u793a\u9700\u8981\u4e09\u4e2a\u5b57\u8282": 102, "11110xxx": 102, "11110\u8868\u793a\u9700\u8981\u56db\u4e2a\u5b57\u8282": 102, "117": 13, "12": [11, 13, 15, 38, 42, 46, 92, 99], "12000": 103, "12122": [34, 85], "12186": [34, 85], "12288": 46, "123abc\u4e00\u4e8c\u4e09": 102, "125": [4, 13], "128": [46, 65], "128k": [46, 47, 50], "128\u4f4dascii\u7801\u662f\u6570\u5b57": 102, "12b": 12, "12k": 84, "12n_": 38, "13": [13, 38, 51, 92], "131": 25, "13245": [34, 85], "13b": 44, "13k": 5, "14": [11, 13, 44, 47, 96], "14165": [34, 85], "14168": [34, 85], "14187": [34, 85], "14858": [34, 85], "149225472": 46, "15": [13, 14, 27, 48, 49, 87], "151": 53, "15115": [34, 85], "1536": 46, "15b": [20, 32], "16": [11, 13, 48, 57, 63, 65, 68, 82, 90], "160": 46, "1609": 51, "1612": [34, 85], "164": 12, "16441": [34, 85], "16609": [34, 85], "16k": 50, "17": [12, 13], "1706": [34, 85], "175": [4, 18, 29], "17k": 57, "18": [13, 53], "1810": [34, 85], "18290": [34, 85], "185b": 45, "188743680": 46, "19": [3, 4, 34, 85], "1904": [34, 85], "1909": [34, 85], "198": 15, "1990\u5e74\u5f00\u59cb\u7814\u53d1": 102, "1994": 92, "1_gnu": 13, "1e": [51, 52, 87, 97, 103], "1k": 15, "1l": 71, "1m": 28, "1qvx610cu7": [34, 85], "1t": [46, 50], "1w": 71, "1\u4f4d\u4e3a": 102, "2": [1, 2, 3, 4, 5, 8, 10, 11, 13, 15, 18, 22, 26, 29, 35, 38, 39, 41, 43, 45, 46, 47, 48, 50, 51, 52, 54, 57, 58, 59, 60, 61, 62, 63, 64, 65, 68, 69, 72, 73, 74, 76, 82, 83, 84, 90, 92, 93, 94, 95, 96, 97, 98, 99, 100, 102], "20": [4, 5, 8, 13, 26, 34, 57, 73, 84, 85], "200": [5, 8, 12, 26, 84], "2000": 48, "2001": [34, 85], "2005": [34, 85], "20050": [34, 85], "2009": [34, 85], "2017": [34, 85], "2019": [34, 85], "2020": [34, 85], "2021": [34, 85], "2022": [34, 85], "2023": [34, 45, 85], "2024": [13, 34, 54, 57, 85], "20240602": 13, "2025": [34, 85], "2048": [51, 52, 87, 94, 95], "20k": [8, 14, 20, 26, 27, 32, 45], "21": [11, 12, 13, 14, 15, 27, 34, 85, 90, 99], "2104": [34, 85], "2107": [34, 85], "2110": [34, 85], "21326725120": 46, "21b": 46, "22": [5, 13, 34, 85, 99], "2203": [34, 85], "2212": [18, 29], "2294": 51, "23": [1, 2, 7, 13, 15, 34, 47, 48, 53, 85, 90, 92, 94, 96, 98], "2305": [34, 85], "2306": [20, 32], "2308": [34, 85], "2309": [34, 85], "2311": [34, 85], "2312": [14, 27, 34, 85], "232": 25, "235692359680": 46, "236b": 46, "24": [9, 11, 13, 28, 31, 34, 45, 46, 47, 50, 53, 54, 85, 93, 96], "2401": [34, 85], "2403": [34, 85], "2405": [34, 85], "2406": [34, 85], "2409": [34, 85], "2412": [34, 85], "25": [7, 13, 19, 30, 34, 53, 54, 73, 85, 96], "250": 87, "256": [51, 52, 74], "256\u4f4dascii\u6269\u5c55\u7801\u662fascii": 102, "26": [13, 82], "27": [49, 82], "28": 57, "29": 13, "2900": 95, "290k": 28, "2d": 94, "2d_": 38, "2e": 28, "2i": [1, 44, 99], "2j": [51, 52, 94, 95], "2m": 46, "2n": 38, "2n_": [38, 96], "2t": 99, "2\u62164\u5b57\u8282\u53d8\u957f": 102, "3": [2, 3, 4, 5, 7, 13, 14, 27, 37, 38, 41, 43, 44, 45, 46, 47, 51, 63, 65, 68, 74, 80, 82, 90, 92, 93, 94, 95, 97, 100], "30": [11, 45, 50], "3000": 95, "300m": 45, "30k": 45, "31k": 5, "32": [13, 51, 52, 57, 61, 87, 90, 102], "3200": 61, "32768": [94, 95], "32b": [57, 82], "32k": 47, "33": 11, "338": 45, "33k": 5, "33t": 96, "34": [13, 15], "34b": [9, 44, 65], "35x": 10, "374": 51, "37b": 47, "38": 13, "3822059520": 46, "384": 57, "39": 15, "3m": 46, "4": [5, 7, 11, 13, 14, 15, 27, 38, 39, 44, 48, 50, 51, 52, 57, 60, 61, 63, 65, 69, 74, 84, 90, 92, 95, 96, 97, 102, 103], "40": [56, 68, 73], "400": 7, "405b": 50, "4096": [28, 51, 52, 103], "40k": 45, "41": 13, "421": 25, "426": 16, "43": 13, "443": [19, 30], "448": 15, "45": [3, 13, 48], "4d": [51, 52], "4e00": 102, "4e00\u57280800": 102, "4e00\u5bf9\u5e94\u7684\u4e8c\u8fdb\u5236\u4e3a100": 102, "4k": 47, "4t": 48, "4\u4e2a\u5b57\u8282\u8868\u793a\u4e00\u4e2a\u5b57\u7b26": 102, "4\u5b57\u8282\u53d8\u957f": 102, "4\u5b57\u8282\u8868\u793a": 102, "5": [11, 13, 14, 15, 21, 22, 25, 27, 28, 34, 41, 44, 45, 46, 47, 48, 51, 52, 56, 57, 59, 60, 61, 73, 74, 76, 82, 85, 87, 92, 95, 103], "50": [42, 44, 57, 82], "500": 7, "500000": [51, 52], "500b": 44, "512": [1, 28, 46, 57], "5120": 46, "52": 44, "52k": [8, 18, 26, 29], "54": [13, 41], "540": 49, "5403": 95, "55m": [19, 30], "57": 11, "5963": 51, "59k": 82, "5b": 84, "5e": [28, 65], "5k": 15, "5m": 46, "5pm": [8, 26], "6": [1, 10, 13, 14, 18, 27, 29, 38, 43, 44, 45, 46, 51, 52, 57, 65, 69, 84, 87, 95, 97, 103], "60": [45, 46, 56, 68], "62": [13, 44], "63": 13, "64": [13, 46, 48, 87], "643": 53, "65": 15, "65b": [48, 87], "66": 13, "67": 48, "671b": 47, "67b": 46, "6n": 38, "6nb": 38, "6w": 102, "7": [12, 13, 15, 18, 29, 39, 42, 49, 53, 54, 57, 65, 90, 92, 95], "70": 13, "70b": [44, 49, 50, 65, 74], "72": 13, "750": 87, "75k": [14, 27, 84], "77": 42, "777": 25, "788m": 47, "7b": [8, 10, 14, 26, 27, 44, 48, 49, 65, 82, 96, 103], "8": [4, 13, 15, 18, 29, 34, 38, 43, 44, 46, 47, 53, 65, 68, 85, 90, 95, 96], "80": [39, 84], "800": 9, "8000": 42, "800k": 84, "80gb": 65, "80k": [14, 27], "80x": 10, "821b": 45, "82k": [18, 29], "83": 51, "8415j": 95, "85": [42, 50], "8b": [37, 50, 65], "8binstruct": 37, "8\u4e2abit\u4f4d\u4e2d\u9ad8\u4f4d\u5fc5\u987b\u7b49\u4e8e0": 102, "8\u4e2abit\u4f4d\u662f\u4e00\u4e2a\u5b57\u8282": 102, "8\u4e3a11100100": 102, "8\u4e3a\u4e09\u5b57\u8282": 102, "8\u4f4d\u53ef\u8868\u793a256\u4e2a\u5b57\u7b26": 102, "8\u548cgbk\u7f16\u7801": 102, "8\u6765\u5b9e\u73b0\u7f16\u7801": 102, "8\u7684\u65b9\u5f0f\u6765\u7f16\u7801\u89e3\u7801\u4f7f\u7528": 102, "8\u7684\u6620\u5c04\u5173\u7cfb\u5982\u4e0b": 102, "8\u7684\u7f16\u7801\u65b9\u5f0f": 102, "8\u7684\u89c4\u5219\u5f88\u7b80\u5355": 102, "8\u7b49": 102, "8\u7f16\u7801": 102, "9": [5, 12, 13, 34, 48, 51, 57, 63, 70, 85, 87, 92, 96], "92": 54, "9297": 95, "95": [13, 48, 87], "974": 16, "9901": 103, "9999": 95, "9e": 65, "A": [3, 8, 15, 18, 26, 28, 29, 34, 36, 37, 42, 49, 51, 52, 53, 56, 57, 61, 62, 67, 68, 69, 72, 75, 76, 80, 82, 85, 92, 93, 94, 99, 100, 104], "And": [62, 90], "As": [36, 39, 42, 43, 45, 49, 57, 60, 61, 62, 76, 80, 84, 93, 96], "At": [1, 3, 18, 29, 36, 41, 47, 49, 75, 84, 94, 95], "By": [4, 45, 47, 50, 51, 57, 61, 73, 94], "FOR": 68, "For": [2, 5, 8, 14, 16, 18, 26, 27, 28, 29, 34, 37, 38, 39, 42, 43, 44, 46, 47, 49, 50, 53, 54, 56, 57, 58, 60, 61, 62, 64, 68, 70, 71, 72, 73, 80, 82, 84, 90, 93, 94, 95, 96, 104], "If": [7, 35, 36, 39, 43, 51, 52, 54, 61, 62, 68, 72, 75, 93, 95, 100], "In": [1, 2, 5, 8, 14, 18, 20, 26, 27, 28, 29, 31, 32, 34, 36, 37, 41, 43, 44, 45, 46, 47, 49, 50, 51, 53, 54, 57, 58, 59, 60, 61, 62, 63, 64, 68, 69, 72, 73, 74, 75, 76, 78, 80, 85, 87, 90, 93, 94, 95, 96, 98, 99], "It": [5, 7, 8, 11, 20, 26, 32, 34, 38, 39, 43, 44, 46, 49, 50, 51, 59, 62, 68, 82, 83, 93, 95], "Its": 42, "No": [37, 47, 51, 54], "Not": [8, 26], "OF": 68, "Of": [18, 29], "On": [1, 39, 47, 61], "One": [7, 36, 41, 50, 61, 64, 68, 69, 72, 94, 95, 99], "Or": [10, 50], "Such": [34, 57, 75, 85], "That": [1, 35], "The": [1, 2, 3, 4, 5, 8, 9, 11, 12, 14, 15, 16, 18, 20, 24, 25, 26, 27, 28, 29, 31, 32, 34, 35, 37, 38, 41, 42, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 57, 58, 59, 60, 61, 62, 63, 64, 65, 68, 69, 70, 72, 73, 74, 75, 80, 84, 87, 93, 94, 95, 96, 97, 99, 100], "Their": 84, "Then": [5, 14, 27, 50, 60, 61, 62, 68, 69, 72, 82, 93, 96, 98], "There": 94, "These": [1, 2, 8, 15, 19, 20, 26, 28, 32, 50, 53, 68, 72, 74, 75], "To": [1, 3, 4, 5, 11, 14, 15, 18, 20, 24, 27, 28, 29, 31, 32, 36, 37, 38, 41, 42, 44, 45, 47, 48, 49, 50, 51, 53, 54, 56, 57, 58, 62, 63, 64, 67, 69, 72, 74, 76, 80, 82, 84, 93, 94, 97, 100], "With": [11, 15, 35, 43, 58, 94], "_": [1, 5, 18, 20, 29, 31, 32, 36, 44, 47, 49, 51, 52, 56, 57, 58, 59, 60, 61, 62, 63, 64, 67, 71, 72, 73, 76, 83, 84, 90, 93, 94, 96, 98, 99, 100], "_1": [1, 83], "__init__": [46, 51, 52, 97], "__main__": 10, "__name__": 10, "_bsz": [51, 52], "_h": 1, "_i": 90, "_libgcc_mutex": 13, "_mergeable_rank": 51, "_norm": [51, 52, 97], "_openmp_mutex": 13, "_t": [67, 96], "a100": 65, "a_": [36, 62, 67, 82, 95, 99], "a_i": [31, 90], "a_t": 67, "aa": 92, "aaabdaaabac": 92, "ab": [34, 85, 92, 95], "abbrevi": [15, 25, 102], "abil": [3, 11, 14, 15, 27, 39, 45, 46, 53, 57, 60, 62, 72, 78, 80, 84], "abl": [8, 9, 15, 26, 39, 49], "ablat": [28, 82, 84], "about": [1, 5, 8, 14, 26, 27, 34, 35, 39, 42, 43, 44, 46, 61, 73, 82, 93, 95], "abov": [2, 39, 42, 44, 50, 57, 58, 67, 72, 73, 75, 76, 80, 83, 93, 94], "absenc": 47, "absolut": [1, 48, 56, 98], "absorb": 96, "abstract": 3, "abstractset": 51, "ac": 92, "academ": 11, "acceler": [2, 96], "accept": [34, 72], "access": [25, 39, 49, 61, 72, 74, 75, 76, 83], "accommod": [50, 84], "accompani": 64, "accomplish": [37, 50], "accord": [42, 47, 61, 73, 76, 82, 84, 93, 95], "accordingli": 57, "account": [38, 44], "accumul": 43, "accur": [11, 16, 19, 28, 49, 53, 58, 62, 73], "accuraci": [11, 15, 47, 53, 57, 69, 73, 80, 82], "achiam": [34, 85], "achiev": [4, 11, 15, 28, 41, 45, 47, 48, 49, 50, 53, 57, 58, 59, 60, 64, 69, 72, 73, 76, 82, 96], "acquir": [11, 46, 49, 80, 93], "across": [11, 13, 39, 42, 50, 53, 61, 82, 84, 93, 94, 96, 97], "action": [8, 26, 36, 57, 67], "activ": [1, 2, 46, 47, 48, 53, 93, 96, 100], "actor": [62, 65], "actor_learning_r": 65, "actual": [5, 62, 83], "ad": [2, 3, 18, 20, 29, 32, 41, 50, 57, 62, 73, 83, 99], "adam_offload": 65, "adamw": [48, 57, 87], "adapt": [2, 20, 32, 58, 64, 93, 94, 95, 104], "add": [1, 3, 5, 20, 32, 43, 44, 47, 48, 49, 51, 52, 56, 63, 73, 74], "addit": [1, 3, 5, 36, 39, 41, 42, 44, 45, 47, 51, 52, 57, 61, 63, 64, 70, 74, 80, 83, 84, 87, 93, 94, 96, 100, 102], "addition": [2, 11, 18, 28, 29, 47, 61, 93], "additionali": 62, "address": [15, 20, 32, 43, 50, 54, 57, 62, 69, 72], "adher": [39, 53, 54, 57, 80], "aditya": [34, 85], "adjust": [46, 50, 56, 72, 80, 96], "adopt": [8, 14, 24, 26, 27, 37, 41, 46, 47, 54, 58, 80, 96], "advanc": [11, 28, 42, 53], "advantag": [39, 62, 64, 65, 96], "advis": 68, "affect": [21, 28, 37, 93], "affin": [47, 51, 93], "aforement": 69, "after": [2, 3, 18, 20, 29, 32, 37, 41, 42, 45, 47, 48, 49, 50, 51, 54, 57, 60, 61, 62, 63, 69, 72, 73, 74, 80, 82, 84, 93, 94, 95], "again": [1, 69, 100], "against": [5, 7, 24, 43, 54, 68, 70, 82], "agarw": [34, 85], "agent": [54, 61, 67, 72], "aggreg": 42, "aggress": [8, 26], "agnost": [4, 64], "ahm": [34, 85], "ai": [7, 13, 15, 34, 43, 50, 57, 58, 74, 76, 85], "aidan": [34, 85], "aif": 74, "aift": 74, "aim": [15, 36, 37, 57, 62, 72, 74, 75, 83, 93], "aime24": 82, "ainsli": [34, 85], "aiohttp": 13, "aiosign": 13, "aixin": [34, 85], "ajudg": 54, "alec": [34, 85], "alethea": [34, 85], "alex": [34, 85], "algorithm": [5, 12, 18, 19, 29, 30, 41, 42, 43, 45, 46, 48, 49, 50, 53, 54, 57, 59, 62, 92], "alibi": 95, "align": [1, 2, 5, 15, 21, 31, 38, 39, 47, 49, 50, 51, 52, 54, 56, 57, 58, 59, 60, 61, 62, 63, 67, 69, 72, 93, 94, 95, 96, 98, 99, 100], "align_n": 64, "alignmentrelev": 39, "alik": 44, "all": [3, 5, 8, 12, 16, 20, 24, 26, 31, 32, 34, 35, 36, 38, 41, 42, 45, 46, 49, 50, 51, 54, 57, 59, 60, 61, 62, 63, 67, 68, 69, 71, 72, 73, 83, 84, 85, 87, 90, 93, 94, 95, 96, 97, 100], "allclos": [51, 97], "allevi": 93, "alloc": [54, 57, 80], "allow": [1, 3, 4, 34, 36, 41, 42, 47, 50, 51, 67, 68, 75, 93, 95, 99], "allowed_speci": 51, "allowed_token": 51, "almeida": [34, 85], "almost": [73, 87], "alon": [69, 73, 74, 87], "along": [2, 24, 41, 43, 44], "alongsid": [47, 62], "alpaca": [14, 20, 21, 22, 27, 32], "alpacaev": [37, 74], "alpha": [64, 67, 73, 83, 94], "alpha_": [38, 93], "alphacod": 44, "alreadi": [39, 41, 45, 51, 61, 74], "also": [1, 5, 8, 11, 13, 15, 16, 18, 26, 28, 29, 34, 35, 37, 38, 39, 41, 42, 43, 44, 45, 46, 47, 49, 50, 51, 52, 53, 54, 56, 59, 60, 62, 63, 68, 74, 75, 76, 82, 93, 94, 96, 100], "altdj": [34, 53, 85, 96], "alter": 43, "altern": [4, 49, 50, 59, 61, 64, 69], "although": [7, 36, 45, 47, 80, 93, 102], "alwai": [41, 50, 64, 73, 93, 94, 95], "amanda": [34, 85], "ambigu": 73, "amc": 15, "american": 102, "amodei": [34, 85], "among": [7, 9, 44, 46, 54, 61, 72, 93, 96], "amount": [28, 38, 42, 46, 50, 74, 80, 82, 84, 90, 94, 96, 100], "an": [1, 2, 3, 4, 5, 7, 8, 11, 12, 14, 18, 20, 26, 27, 28, 29, 32, 34, 36, 37, 39, 41, 42, 43, 44, 45, 46, 47, 49, 50, 51, 52, 54, 56, 57, 59, 60, 62, 64, 67, 68, 69, 70, 72, 73, 74, 75, 76, 80, 83, 84, 85, 90, 93, 94, 95, 96], "anaconda3": 13, "analogi": 39, "analysi": [43, 46, 50, 54, 64, 69], "analyz": [28, 54, 90], "anchor": 43, "andi": [34, 85], "andrew": [34, 85], "angela": [34, 85], "angl": [95, 98], "ani": [1, 3, 4, 8, 12, 14, 18, 24, 26, 27, 28, 29, 35, 36, 39, 43, 47, 50, 51, 54, 63, 68, 76, 80, 84, 94, 95, 96, 98, 99], "anneal": 28, "annot": [11, 13, 44, 47, 49, 50, 57, 69, 74], "anoth": [8, 26, 36, 41, 43, 49, 62, 64, 69, 73, 95], "answer": [2, 3, 10, 11, 15, 19, 28, 30, 31, 37, 44, 47, 49, 53, 54, 57, 68, 75, 80, 82, 83, 84, 87, 90, 95], "answer_1": 50, "anthrop": [13, 61], "anticip": 54, "anyio": 13, "anywher": [5, 63], "aop": 57, "api": [5, 13, 28, 53, 63], "app": 68, "appear": [24, 38, 43, 57, 95], "append": [18, 28, 29, 51, 52, 65, 68, 72, 82], "appli": [1, 2, 5, 14, 20, 27, 32, 37, 42, 43, 44, 45, 47, 50, 51, 52, 56, 57, 59, 61, 62, 63, 64, 67, 73, 75, 76, 80, 83, 90, 93, 94, 95, 100], "applic": [4, 5, 15, 44, 49, 94, 95], "apply_chat_templ": 65, "apply_rotary_emb": [51, 52, 95], "approach": [14, 18, 27, 28, 29, 36, 38, 42, 44, 46, 47, 49, 50, 54, 56, 57, 59, 64, 67, 69, 70, 74, 75, 76, 80, 82, 83, 93, 94, 95], "appropri": [1, 8, 26, 44, 67], "approx": [38, 47, 94, 98], "approxim": [3, 4, 7, 36, 45, 46, 50, 61, 73, 76, 84, 87, 90, 93, 95], "aptitud": 15, "ar": [1, 2, 3, 4, 5, 7, 8, 9, 12, 14, 15, 18, 20, 25, 26, 27, 28, 29, 32, 34, 35, 36, 37, 38, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 56, 57, 58, 59, 60, 61, 62, 63, 64, 67, 68, 69, 70, 72, 73, 74, 75, 78, 84, 85, 87, 90, 93, 94, 95, 96, 97, 99, 102], "arang": [12, 51, 52, 59, 95], "arbitrari": [5, 95], "archit": [34, 85], "architectur": [2, 3, 38, 41, 44, 54, 93, 95, 96, 97, 99, 104], "archiv": 3, "area": [11, 53], "arg": [49, 51, 52, 58, 65, 72, 95], "argmax": 51, "argu": [46, 72], "ariel": [34, 85], "aris": [68, 69], "arithmet": 15, "armando": [34, 85], "armo": 22, "armorm": 37, "around": [1, 7, 96], "arrang": 13, "art": [4, 15, 39, 48, 57], "articl": 3, "artifici": 58, "arvind": [34, 85], "arxiv": [14, 18, 20, 27, 29, 32, 34, 48, 85], "ascent": 67, "ascertain": 61, "ascii\u5c31\u662f\u6700\u521d\u7684\u4e00\u79cd\u6620\u5c04\u5173\u7cfb": 102, "ascii\u8d77\u521d\u53ea\u89c4\u5b9a\u4e86127\u4e2a\u5b57\u7b26": 102, "ashish": [34, 85], "ask": [5, 8, 16, 18, 26, 29, 43, 49, 50, 63, 69, 75], "askel": [34, 85], "aspect": [43, 53, 64, 74], "assert": [44, 51, 52, 95], "assertionerror": [51, 95], "assess": [11, 12, 15, 20, 32, 50, 54, 71, 82], "assign": [1, 36, 41, 49, 50, 56, 57, 61, 62, 64, 67, 72, 73, 76, 83, 84, 93], "assist": [8, 26, 39, 44, 51, 68, 70, 74, 75], "associ": [44, 57, 72], "assum": [2, 36, 60, 64, 72, 74, 75, 76, 83, 97, 101], "ast": [58, 59, 72, 83, 99], "asymptot": 15, "async": 13, "atcod": [13, 25], "atol": [51, 97], "att": 93, "attach": 56, "attain": 61, "attempt": [39, 82, 83, 84], "attend": 1, "attent": [2, 3, 4, 21, 22, 34, 38, 46, 47, 53, 60, 85, 93, 94, 95, 97, 98, 99], "attention_bia": 46, "attention_norm": [51, 52], "attn": 38, "attr": 13, "attract": 95, "attribut": [42, 70], "audio": [8, 26], "augment": 74, "auli": [34, 85], "auth": 13, "authent": 64, "author": [59, 74], "auto": [1, 7], "autom": [18, 25, 28, 29, 53], "automat": [7, 14, 18, 20, 27, 29, 32, 50, 54, 56, 70, 93], "autonom": 80, "autoregress": [2, 4, 36, 44, 49, 94], "auxiliari": [2, 39, 41, 47, 83], "avail": [19, 44, 48, 50, 53], "avenu": 5, "averag": [12, 41, 42, 51, 52, 62, 69, 82, 93], "avg": 57, "avoid": [5, 36, 39, 41, 42, 43, 57, 60, 62, 67, 72, 73, 94], "await": [20, 32], "ax": 50, "axi": [51, 97], "b": [2, 34, 36, 38, 51, 52, 61, 68, 75, 82, 83, 85, 94, 100, 104], "b_": [51, 52, 100], "b_1": 1, "b_2": 1, "b_i": 47, "b_j": 47, "babuschkin": [34, 85], "backbon": 56, "backend": 10, "background": 12, "backpropag": 49, "backtransl": 50, "backward": 38, "bad": [36, 39, 50], "bag": 49, "bai": [34, 85], "baker": [34, 85], "balaji": [34, 85], "balanc": [1, 47, 50, 54, 57, 75], "band": 4, "bao": [34, 85], "baosong": [34, 85], "baptist": [34, 85], "bar": 94, "barn": [34, 85], "basart": [34, 85], "base": [1, 2, 3, 5, 8, 14, 15, 18, 20, 22, 26, 27, 28, 29, 32, 35, 36, 39, 41, 42, 43, 44, 46, 47, 48, 49, 50, 51, 56, 59, 61, 62, 63, 64, 68, 72, 73, 74, 75, 76, 82, 83, 90, 94, 95, 98, 99, 102, 103], "baselin": [7, 15, 37, 57, 74, 75, 84], "basi": [51, 95], "basic": [15, 16, 21, 43, 53, 54], "basu": [34, 85], "batch": [5, 8, 26, 28, 38, 44, 48, 49, 50, 51, 57, 61, 63, 87, 96], "batchnorm1d": [51, 97], "batchnorm2d": 51, "bavarian": [34, 85], "bax": 104, "bbc": [34, 53, 85], "bbpe": [46, 53], "becaus": [1, 8, 18, 26, 29, 43, 50, 56, 68, 80, 97], "becom": [60, 69, 83, 93, 95, 96], "been": [36, 42, 43, 46, 47, 51, 53, 61, 62, 73, 76], "befor": [18, 24, 29, 39, 41, 45, 49, 54, 70, 94, 95], "begin": [1, 2, 3, 5, 38, 44, 47, 49, 50, 51, 52, 57, 58, 59, 60, 61, 62, 63, 68, 72, 76, 80, 93, 94, 95, 96, 98, 99, 100], "begin_of_text": 51, "behav": [39, 42, 95], "behavior": [3, 5, 15, 36, 39, 42, 43, 54, 57, 58, 68, 80, 95, 97], "behaviour": 41, "behind": 95, "bei": [34, 85], "beichen": [34, 85], "being": [1, 4, 14, 27, 34, 41, 50, 76], "believ": 56, "belong": 93, "below": [8, 14, 26, 27, 39, 44, 60, 67, 73], "bench": 21, "benchmark": [3, 4, 9, 19, 20, 30, 32, 34, 39, 45, 46, 50, 82, 85, 96], "benefici": [1, 43, 73], "benefit": [3, 49, 73, 90], "benfeng": [34, 85], "benjamin": [34, 85], "berner": [34, 85], "bert": [3, 34, 85], "besid": [57, 58], "best": [5, 15, 28, 37, 38, 41, 42, 43, 48, 49, 50, 54, 58, 61, 63, 73, 74, 76, 82, 83, 84], "bestof": 37, "beta": [5, 49, 51, 59, 60, 62, 63, 67, 72, 94, 97, 100], "beta_": [48, 83], "beta_1": 87, "beta_2": 87, "better": [5, 11, 39, 42, 43, 45, 47, 48, 49, 50, 53, 54, 60, 67, 68, 69, 72, 75, 82, 84, 90, 96], "between": [1, 5, 8, 11, 14, 15, 26, 27, 31, 36, 37, 39, 41, 42, 44, 46, 49, 50, 51, 52, 57, 59, 60, 62, 63, 67, 68, 69, 72, 73, 74, 80, 83, 84, 90, 93, 94, 97, 98, 99, 100], "beyond": [13, 39, 69, 93, 95], "bf16": [65, 103], "bi": [34, 85], "bia": [46, 47, 51, 52, 53, 56, 69, 75, 83, 100], "bias": [18, 29, 36], "bib": 34, "bibliographi": 34, "bibtex": 34, "bidirect": [34, 85], "bigger": [5, 94], "biggest": 42, "bilinear": 100, "billion": 4, "bin": [34, 85], "binari": [49, 50, 58, 69, 73], "bing": [34, 85], "bingxuan": [34, 85], "binom": [5, 12, 63, 82], "binyuan": [34, 85], "biologi": [11, 15], "bit": [61, 94], "black": 38, "bleu": 36, "blind": 11, "block": [2, 3, 10, 35, 44, 93], "blog": [5, 34, 54, 62, 66, 85], "blue": [20, 32], "bmr": [4, 5, 34, 85], "bn": [51, 97], "bo": [34, 51, 85], "bob": [34, 85], "bodi": [12, 102], "bofei": [34, 85], "boltzmann": 61, "bonu": 83, "book": [3, 34, 35, 48], "bool": 51, "bootstrap": [14, 18, 27, 29, 39], "bos_id": 51, "both": [1, 4, 7, 9, 11, 14, 15, 27, 34, 37, 41, 44, 47, 49, 50, 53, 54, 62, 63, 64, 68, 69, 70, 74, 76, 83, 84, 94, 95, 96, 97], "boto3": 13, "botocor": 13, "bottleneck": 96, "bottom": [1, 73], "bound": [37, 67, 93, 95], "boundari": [14, 18, 27, 29, 94], "bowen": [34, 85], "box": [15, 34, 47, 80], "boyang": [34, 85], "bpe": [3, 44, 48], "bpe\u6bcf\u4e00\u6b65\u90fd\u5c06\u6700\u5e38\u89c1\u7684\u4e00\u5bf9\u76f8\u90bb\u6570\u636e\u5355\u4f4d\u66ff\u6362\u4e3a\u8be5\u6570\u636e\u4e2d\u6ca1\u6709\u51fa\u73b0\u8fc7\u7684\u4e00\u4e2a\u65b0\u5355\u4f4d": 92, "bpe\u9009\u62e9\u9891\u6570\u6700\u9ad8\u7684\u76f8\u90bb\u5b50\u8bcd\u5408\u5e76": 101, "bradlei": [5, 56, 59, 72, 73], "brahma": [34, 85], "brainstorm": 75, "branch": 50, "breadth": [11, 20, 32], "break": [18, 29, 51, 72], "breviti": [93, 96], "bridg": [11, 36, 54], "brief": 4, "bright": 15, "bring": 62, "broad": [18, 29], "broadcast": 95, "broader": [11, 13], "broadli": 4, "brockman": [34, 85], "brook": [34, 85], "brown": [34, 85], "brundag": [34, 85], "brute": 43, "bsz": [51, 52], "bt": 72, "bucket": 50, "budget": [41, 42, 44, 48, 53, 82, 93], "buffer": [61, 65], "bug": [43, 65], "build": [3, 11, 13, 34, 37, 45, 50, 51, 53, 54, 56, 70, 74, 75], "built": [35, 46, 49, 54], "bullet": 43, "burda": [34, 85], "burden": 62, "burn": [34, 85], "busi": 11, "byte": [3, 34, 44, 46, 48, 53, 85], "bzip2": 13, "c": [2, 9, 12, 38, 41, 42, 46, 49, 50, 51, 52, 54, 58, 61, 73, 94, 95, 96, 97, 100], "c4": 48, "c_": [38, 46], "ca": 13, "cach": [10, 51, 52, 53, 57], "cache_k": [51, 52], "cache_len": [51, 52], "cache_v": [51, 52], "cachecontrol": 13, "cachetool": 13, "cai": [34, 68, 85], "caichat": 65, "calcul": [12, 15, 46, 51, 52, 57, 61, 62, 67, 68, 69, 73, 90, 93, 95, 97], "calibr": [37, 68], "call": [1, 3, 4, 5, 12, 20, 32, 34, 36, 39, 43, 47, 49, 50, 51, 63, 64, 68, 72, 78, 84, 95, 96, 100], "can": [1, 2, 4, 5, 12, 13, 14, 15, 18, 27, 28, 29, 34, 35, 36, 37, 38, 39, 41, 42, 43, 44, 45, 46, 47, 49, 50, 53, 54, 56, 57, 59, 60, 61, 62, 63, 64, 67, 68, 69, 72, 73, 74, 75, 76, 80, 82, 84, 85, 90, 93, 94, 95, 96, 97, 99, 100, 101], "cancel": 59, "candid": [41, 42, 50, 54, 61, 69, 72, 74, 76, 90], "cannot": [8, 15, 26, 36, 39, 95, 96], "canon": 69, "capabl": [7, 13, 14, 15, 20, 27, 28, 31, 32, 37, 39, 44, 47, 49, 53, 54, 64, 74, 80, 95], "capac": [2, 61, 93], "captur": [73, 80, 93], "cardin": [42, 61], "care": [39, 50, 82], "carefulli": [11, 46, 50, 53, 87], "carlo": [36, 62], "carr": [34, 85], "carri": [47, 96], "carrol": [34, 85], "cascad": 44, "case": [5, 8, 14, 16, 18, 24, 25, 26, 27, 28, 29, 36, 41, 42, 43, 44, 45, 47, 49, 54, 57, 58, 60, 64, 68, 76, 80, 82, 93, 94], "cast": 51, "catastroph": 95, "catch": 50, "categor": [42, 50, 54, 73, 75], "categori": [7, 75], "caus": [11, 51, 59, 60, 70, 93], "causal": [2, 44], "cd": [10, 13], "cdot": [1, 46, 57, 59, 60, 64, 72, 83, 93, 94, 99, 100], "ce": 61, "ceas": 38, "ceil": 39, "cell": 51, "central": 73, "centroid": 93, "certain": [2, 36, 47, 50, 57, 75, 76, 93, 94, 95], "certif": 13, "certifi": 13, "cffi": 13, "cfg": 10, "cgrs19": [4, 34, 85], "chai": [34, 85], "chain": [11, 47, 68, 69, 75, 90], "chainof": 90, "challeng": [7, 8, 11, 15, 19, 26, 30, 39, 43, 50, 53, 54, 57, 59, 73, 75, 80, 84], "chan": [34, 85], "chanc": [11, 43], "chang": [8, 13, 26, 34, 58, 70, 83, 85], "changhan": [34, 85], "changyu": [34, 85], "channel": [13, 51], "chantzi": [34, 85], "charact": [43, 44, 102], "character": [38, 46, 53], "characterist": [20, 32, 46], "charset": [13, 102], "chat": [34, 45, 46, 49, 51, 85], "chat_complet": 51, "chatbot": 7, "chatgpt": [14, 27, 39], "cheaper": 7, "check": [5, 16, 25, 28, 44, 50, 53, 54, 82, 83, 84], "checklist": 54, "checkpoint": [34, 45, 47, 49, 50, 51, 53, 65, 80, 85], "chelsea": [34, 85], "chemistri": [11, 15], "chen": [34, 85], "chenggang": [34, 85], "chengpeng": [34, 85], "chengqi": [34, 85], "chengqiang": [34, 85], "chengyuan": [34, 85], "chess": [34, 39, 85], "child": [34, 85], "chines": 46, "cho": [34, 85], "choic": [2, 11, 15, 28, 36, 49, 60, 61, 68, 72, 84, 99], "chong": [34, 85], "choos": [20, 32, 41, 43, 49, 68, 73, 75, 82, 84, 94], "chose": 1, "chosen": [38, 49, 50, 56, 73], "chosen_1": 50, "chosen_2": 50, "christiano": [34, 85], "christoph": [34, 85], "chu": [34, 85], "chuanqi": [34, 85], "chunqiu": [34, 85], "ci": 95, "cite": 34, "ckb": [15, 34, 85, 90], "ckpt_dir": 51, "ckpt_path": 51, "cl": [14, 27], "clamp": 68, "clarifi": 38, "clariti": [47, 54], "clark": [34, 85], "class": [13, 18, 29, 46, 51, 52, 60, 97], "classic": 67, "classif": [2, 8, 26, 39, 50, 73, 82, 84], "classifi": [13, 41, 50, 54, 64, 75, 82], "claud": [45, 82], "clean": [28, 45, 49, 50, 54], "clear": [54, 65, 73, 95], "clearli": [43, 45], "clemen": [34, 85], "cleo": 13, "clever": 59, "cli": [10, 65], "client": 13, "clip": [48, 62], "clone": [10, 13], "close": [14, 27, 36, 42, 43, 45, 47, 51, 68], "closer": 72, "closest": 43, "cluster": 50, "cly": [34, 54, 85], "cnn": 3, "co": [1, 44, 51, 52, 94, 95, 98, 99], "coars": [50, 54], "cobb": [34, 85], "code": [9, 12, 13, 14, 19, 21, 22, 25, 27, 28, 30, 31, 34, 35, 37, 39, 41, 42, 45, 46, 47, 48, 53, 54, 75, 80, 85, 102], "code1": 13, "code2": 13, "code_alpaca_20k": [8, 26], "code_generation_lit": 13, "code_list": 13, "codealpaca": [8, 14, 21, 22, 26, 27], "codebert": 54, "codecontest": [41, 42], "codeexecut": 13, "codeforc": [13, 24, 25, 41, 42], "codegen": 10, "codellama": [14, 27], "codenet": 24, "codeqwen1": 54, "coder": [14, 21, 22, 27, 28, 34, 47, 53, 85, 103], "codewar": 25, "codex": [12, 21, 44], "coeffici": [5, 56, 59, 63, 67, 95], "cognit": 80, "coher": [13, 69], "collabor": 54, "collaps": 57, "collect": [3, 5, 13, 14, 18, 27, 28, 29, 44, 46, 48, 50, 51, 54, 63, 64, 65, 68, 75, 76, 80, 90], "collin": [34, 85], "colon": 70, "com": [10, 13], "combin": [3, 5, 8, 14, 26, 27, 28, 42, 43, 44, 47, 49, 50, 54, 57, 63, 64, 69, 71, 72, 80, 93, 94], "come": [1, 5, 8, 18, 26, 29, 45, 94, 95], "command": [13, 35], "commbal": 93, "comment": [44, 50, 54], "common": [3, 5, 13, 36, 43, 45, 50, 54, 56, 61, 93], "commoncrawl": 48, "commonli": [14, 27, 28], "commonmark": 34, "commun": [28, 47, 80, 87, 102], "commut": 96, "compar": [7, 11, 28, 36, 37, 45, 47, 50, 53, 54, 57, 60, 61, 62, 64, 74, 76, 82, 83, 84, 100], "comparison": [5, 37, 44, 45, 46, 52, 63, 64, 68, 72, 73], "compat": [1, 44, 95, 97], "compet": [4, 50], "competit": [1, 4, 13, 15, 24, 41, 42, 47, 57, 82], "competitor": 5, "compil": [45, 46, 47, 50, 80], "complementari": 47, "complet": [5, 8, 12, 20, 26, 28, 32, 37, 43, 44, 47, 49, 50, 51, 54, 59, 60, 63], "complex": [2, 11, 14, 20, 27, 32, 39, 44, 50, 51, 52, 53, 54, 62, 67, 78, 80, 94, 95, 99], "complex64": [51, 52, 95], "compli": 42, "complic": [20, 32, 39, 62], "compon": [28, 42, 49, 51, 52, 53, 73, 74, 100], "compos": [1, 15, 93], "composit": 50, "comprehens": [3, 12, 20, 32, 47, 53, 64, 67], "compress": [46, 92], "compris": [11, 46, 47, 50, 53, 54, 61, 93], "comput": [1, 9, 11, 12, 28, 39, 42, 44, 48, 49, 51, 52, 53, 54, 58, 61, 62, 67, 68, 69, 71, 80, 82, 84, 93, 94, 95, 96, 97, 99, 100, 102], "concat": 1, "concaten": [1, 18, 29, 49, 69, 83], "concept": [44, 54], "concis": [47, 53, 61], "conclud": 46, "conclus": [46, 96], "concret": [20, 32, 57, 83], "concurr": 43, "conda": 13, "condit": [2, 18, 29, 36, 41, 64], "conduct": [42, 46, 50, 58, 64, 69, 72, 84, 95], "conduct_rejection_sampl": 72, "confer": [34, 64, 85], "confid": [39, 68, 76], "config": [46, 93], "configur": [37, 46], "confin": 49, "conform": 50, "confus": 57, "conjug": 99, "connect": 1, "consecut": [14, 27, 42], "consequ": [50, 64], "consid": [12, 36, 39, 50, 54, 56, 57, 60, 61, 64, 68, 69, 72, 73, 75, 94, 95], "consider": [45, 57, 64], "consist": [1, 2, 5, 9, 11, 14, 15, 24, 25, 27, 39, 44, 45, 46, 54, 56, 57, 59, 63, 67, 76, 80, 90, 93, 97], "console_script": 10, "consolid": 93, "constant": [36, 57, 93, 94, 99, 100], "constitut": 58, "constrain": [57, 59, 67, 72, 82], "constraint": [20, 32, 43, 49], "construct": [7, 9, 20, 31, 32, 44, 45, 54, 58, 64, 70, 72, 74, 80, 93], "consum": [1, 76], "contain": [1, 3, 5, 7, 8, 10, 11, 14, 15, 16, 18, 26, 27, 28, 29, 39, 41, 42, 43, 44, 46, 48, 49, 50, 51, 54, 57, 58, 70, 82, 84, 90, 95, 96], "container": 50, "contamin": [13, 34, 85], "content": [8, 26, 28, 34, 35, 51, 64, 68], "contest": [13, 42], "context": [2, 3, 8, 18, 26, 28, 29, 36, 38, 46, 50, 56, 62, 64, 68, 69, 70, 93, 99], "context_messag": 65, "contextu": 28, "contextwindow": 47, "contigu": [2, 51, 52], "continu": [1, 5, 13, 14, 15, 27, 43, 50, 54, 56, 62, 63, 68, 72, 73], "contrast": [4, 11, 36, 43, 70, 76, 95], "contribut": [28, 57, 67, 73], "control": [5, 39, 51, 59, 63, 69, 82], "convei": 64, "convent": 96, "converg": [2, 93, 97], "convers": [38, 50, 51, 57, 68, 95], "convert": [1, 35, 69, 72], "convinc": 84, "convolut": [1, 34, 85], "cookbook": 28, "coordin": 94, "copi": 12, "core": [3, 7, 13, 39, 96], "corpora": [28, 54], "corpu": [2, 4, 14, 27, 45, 46, 47, 54, 61, 95], "corr": 56, "correct": [16, 21, 22, 28, 34, 36, 41, 42, 43, 44, 47, 50, 53, 54, 56, 57, 58, 60, 64, 75, 80, 82, 84, 85, 90], "correctli": [15, 43, 76, 83, 95], "correl": [4, 7, 56, 64, 74, 90], "correspond": [1, 5, 15, 18, 20, 28, 29, 32, 36, 42, 43, 44, 45, 47, 51, 52, 56, 58, 59, 62, 64, 72, 95, 98, 99], "correspondingli": 62, "cosin": [1, 28, 48, 50, 103], "cost": [8, 26, 45, 47, 93], "costli": 93, "cot": [11, 21, 22, 57, 68, 69, 80], "could": [37, 39, 41, 42, 50, 56, 58, 67, 73, 76, 82, 95], "count": [12, 28, 82], "counteract": [1, 50], "counterclockwis": 98, "counterpart": [49, 50], "coupl": [47, 83], "cover": [11, 43, 50, 54, 74, 93], "coverag": [18, 29, 45, 53, 69, 76], "cpu": 51, "cr": 61, "crack": 22, "craft": 61, "crashtest": 13, "crawl": [3, 45, 54], "creat": [3, 5, 8, 13, 15, 18, 20, 26, 28, 29, 32, 39, 44, 45, 50, 53, 54, 70, 74], "creation": 43, "creativ": [39, 47], "credit": [36, 67], "criteria": [49, 50, 53, 70, 82], "critic": [41, 45, 53, 62, 65, 67, 83], "critic_learning_r": 65, "critiqu": [21, 22, 64], "crmsnorm": [34, 85], "cross": [38, 41, 50, 58, 61, 69], "cross_entropi": 51, "crowd": 16, "crowdwork": 68, "crucial": [28, 45, 46, 54], "crux": 22, "cruxev": [34, 85], "cryptographi": 13, "ctj": [12, 34, 85], "ctx": 38, "cu12": 13, "cubla": 13, "cuda": [13, 51], "cudnn": 13, "cufft": 13, "cui": [34, 85], "cum": [34, 85], "cumbersom": 61, "cumsum": 51, "cumul": [51, 67], "cup": 90, "cupti": 13, "cur_po": 51, "curand": 13, "curat": [11, 24, 25, 28, 41, 46, 47, 50, 54, 56, 80, 87], "curiou": [7, 73], "current": [3, 4, 5, 8, 19, 20, 26, 28, 32, 36, 53, 61, 62, 63, 72, 75, 82, 84, 94, 96], "curv": [38, 49], "cusolv": 13, "cuspars": 13, "custom": 5, "custom_evalu": 13, "custom_output_fil": 13, "cut": [51, 64], "cutoff_len": 103, "cycl": 50, "d": [5, 10, 20, 32, 34, 35, 36, 38, 44, 49, 51, 52, 56, 57, 59, 60, 61, 62, 63, 69, 70, 71, 72, 73, 76, 83, 85, 90, 92, 93, 94, 95, 96, 98, 99, 104], "d_": [1, 5, 31, 38, 46, 49, 63, 72, 73, 96, 100], "d_c": [46, 96], "d_h": [46, 96], "d_k": 1, "d_v": 1, "dab": [34, 46, 85], "dai": [34, 85], "daili": 3, "dalf": [34, 45, 46, 47, 85, 93, 96], "damai": [34, 85], "dan": [34, 85], "dang": [34, 85], "danger": 68, "daniel": [34, 85], "dario": [34, 85], "data": [2, 3, 4, 5, 14, 20, 21, 22, 24, 25, 27, 32, 36, 37, 39, 44, 53, 56, 58, 59, 61, 62, 63, 64, 68, 70, 71, 72, 74, 75, 76, 80, 83, 92, 95, 97], "dataclass": [51, 52], "datalabel": 84, "dataset": [2, 8, 10, 11, 12, 13, 14, 15, 16, 19, 20, 22, 26, 27, 28, 30, 31, 32, 39, 42, 45, 46, 47, 48, 49, 50, 53, 54, 56, 61, 62, 64, 71, 72, 73, 74, 76, 80, 82, 83, 84, 90, 103], "date": 28, "dateutil": 13, "dauphin": [34, 85], "dave": [34, 85], "david": [34, 85], "davinci": [8, 26], "dawn": [34, 85], "daya": [34, 85], "dayiheng": [34, 85], "dclt19": [3, 34, 85], "ddot": [98, 99], "de": [34, 44, 85], "deal": 75, "debat": 46, "debias": 53, "debug": [20, 32, 50, 54], "debugg": 56, "decad": 15, "decai": [48, 87, 95, 99], "decid": [45, 54, 74, 80, 84, 93], "decis": [36, 43, 56], "declin": [20, 32, 46], "decod": [3, 8, 14, 26, 27, 37, 41, 51, 53, 56, 69, 82, 90], "decompos": 49, "decomposit": 104, "decoupl": [46, 47, 57], "decreas": [11, 37, 38, 47, 57, 59, 60, 64, 72, 93], "dedic": 93, "deduc": 83, "dedupl": [5, 28, 44, 45, 50, 90], "deem": 75, "deep": [34, 85], "deepen": [20, 32], "deepseek": [14, 21, 22, 27, 34, 57, 62, 85, 93, 96, 103], "deepseekcod": [47, 103], "deepseekmath": 45, "deepseekmo": [46, 47], "deepseekv2attent": 46, "deepseekv2config": 46, "deepseekv2forcausallm": 46, "deepseekv2mlp": 46, "deepseekv2model": 46, "deepseekv2pretrainedmodel": 46, "deepseekv2rmsnorm": 46, "deepspe": 103, "def": [10, 12, 46, 51, 52, 72, 95, 97], "default": [7, 13, 35, 51, 57, 95], "defin": [18, 28, 29, 35, 36, 38, 39, 49, 50, 51, 52, 53, 54, 58, 59, 67, 74, 90, 94, 95, 98, 99, 100], "definit": [31, 94], "degener": 59, "degrad": [47, 50], "degre": [4, 49], "dejian": [34, 85], "deli": [34, 85], "deliber": 84, "delimit": [2, 82], "deliv": 50, "delta": [36, 49, 104], "delta_": [62, 67], "delta_t": 67, "demonstr": [2, 3, 4, 5, 11, 36, 37, 47, 53, 54, 57, 61, 63, 64, 73, 78, 80, 87, 94, 95, 96], "deng": [34, 85], "dengr": [34, 85], "denni": [34, 85], "denomin": 36, "denot": [31, 36, 38, 41, 44, 56, 58, 60, 61, 67, 72, 73, 83, 93, 96, 97, 99], "dens": [4, 50, 53, 96], "densifi": 47, "densiti": 72, "depend": [2, 4, 13, 28, 34, 38, 51, 52, 59, 61, 64, 76, 82, 94, 95, 97, 98, 99], "depict": [3, 68, 80], "deploi": [50, 93], "deploy": 96, "depth": [20, 32], "deriv": [36, 44, 47, 72, 74, 95], "descend": 51, "descent": 2, "describ": [1, 2, 8, 26, 43, 50, 69, 74, 94], "descript": [16, 31, 41, 42, 43, 50, 70], "description2cod": 24, "design": [11, 16, 19, 28, 30, 31, 47, 53, 60, 68, 70, 80, 84, 93, 96], "desir": [5, 36, 50, 64, 70, 72], "despit": [4, 39, 47, 63, 64, 67], "destabil": 67, "detail": [3, 35, 42, 43, 50, 52, 53, 54, 62, 64, 66, 67], "detect": [18, 29, 41, 50, 53, 64], "determin": [47, 50, 53, 54, 58, 61, 76], "determinist": [37, 47, 80], "detoken": 101, "devbal": 93, "develop": [28, 39, 44, 47, 53, 58, 68, 72, 75, 80, 82, 83], "deviat": [37, 51, 59, 62, 67, 73], "devic": [47, 51, 52, 95, 102], "devis": [14, 15, 27], "devlin": [34, 85], "dfag17": [34, 53, 85], "dhariw": [34, 85], "diagon": [44, 51, 52], "dialog": 51, "dialogu": [44, 49, 50], "diamond": 15, "dict": 51, "dictionari": [8, 26, 72], "did": [54, 69], "differ": [1, 4, 5, 8, 9, 11, 18, 26, 29, 34, 36, 37, 39, 41, 42, 43, 44, 46, 48, 49, 50, 52, 53, 54, 57, 59, 60, 61, 62, 63, 64, 65, 68, 70, 72, 84, 90, 93, 94, 96, 98, 99], "differenti": 70, "difficult": [15, 39, 43, 90], "difficulti": [11, 15, 19, 20, 28, 30, 32, 42, 50, 54, 82], "dill": 13, "dim": [51, 52, 95, 97], "dimens": [1, 38, 44, 46, 51, 52, 56, 60, 93, 94, 95, 96, 97, 99, 100], "dimension": [1, 56, 98, 99], "diminish": [64, 90], "ding": [34, 85], "diogo": [34, 85], "direct": [4, 11, 15, 20, 32, 35, 43, 53, 54, 64, 69, 70, 71, 72, 80, 85], "directli": [2, 12, 14, 27, 37, 44, 45, 50, 57, 59, 62, 67, 69, 72, 82, 83, 94, 95, 99, 101], "directori": 51, "disagr": 50, "disagre": 75, "disallowed_speci": 51, "disallowed_token": 51, "disanalogi": 39, "discard": [8, 26, 50, 54, 75], "discontinu": [20, 32], "discount": 67, "discrep": [36, 41, 49, 73], "discret": 49, "discrimin": [2, 41], "discuss": [44, 68, 84, 99], "displai": [35, 64], "dispref": [59, 60], "disproportion": [50, 57], "distanc": [60, 95], "distil": [14, 27, 49, 70, 76], "distinct": [46, 47, 53, 61, 64, 73, 90], "distinguish": [58, 61, 72], "distlib": 13, "distort": 97, "distract": 67, "distribut": [2, 4, 5, 20, 28, 32, 36, 41, 44, 47, 49, 51, 56, 57, 59, 61, 62, 63, 67, 68, 69, 72, 73, 75, 76, 80, 82, 83, 84, 93], "distro": 13, "div_": 51, "diverg": [5, 49, 59, 62, 67], "divers": [2, 3, 5, 8, 11, 18, 19, 20, 26, 28, 29, 30, 32, 37, 41, 42, 43, 44, 47, 50, 53, 54, 74, 75, 80, 82], "divid": [1, 41, 43, 53, 62, 98, 99], "divis": 42, "dkv": 96, "do": [4, 8, 9, 14, 18, 26, 27, 29, 34, 36, 39, 42, 43, 45, 49, 50, 51, 52, 63, 75, 80, 82, 83, 84, 94, 96], "do_train": 103, "docstr": [12, 44], "doctyp": 102, "document": [2, 3, 14, 27, 34, 35, 44, 50, 54, 95], "doe": [5, 18, 29, 44, 47, 51, 54, 59, 67, 73, 90, 95, 96, 97], "doesn": 95, "domain": [3, 11, 15, 20, 32, 36, 47, 53, 58, 80, 82], "domin": 36, "don": 67, "done": [20, 32, 54], "dong": [34, 85], "dongji": [34, 85], "dot": [2, 36, 51, 52, 54, 60, 62, 64, 71, 74, 75, 93, 94, 95, 96, 98, 99, 101], "doubl": [43, 61, 90], "down": [42, 94, 95, 97], "down_proj": 46, "downstream": [4, 50, 104], "dpo": [21, 22, 37, 50, 53, 54, 65, 72, 74, 75], "dpop": [21, 50], "dq": 96, "dr": 3, "draw": [14, 27, 72], "drawback": 80, "drawn": [53, 61], "drive": 41, "drop": [11, 47], "drop_last": 65, "dschat": 65, "dtype": [51, 52], "du": [34, 85], "duan": [34, 85], "due": [28, 43, 47, 54, 56, 57, 93, 95, 97], "dulwich": 13, "duplic": 44, "dure": [1, 2, 8, 11, 19, 26, 28, 36, 38, 41, 44, 45, 47, 49, 50, 53, 54, 57, 59, 67, 68, 80, 84, 87, 90, 93, 95, 96, 97], "dynam": [50, 64], "dz": 61, "e": [2, 5, 8, 9, 10, 12, 13, 14, 18, 26, 27, 29, 36, 39, 47, 49, 50, 51, 52, 54, 56, 57, 58, 59, 60, 61, 62, 63, 64, 67, 68, 69, 70, 72, 73, 74, 75, 80, 82, 83, 92, 93, 94, 95, 97], "e501": 51, "e_": 75, "e_j": 90, "each": [1, 2, 3, 5, 7, 8, 11, 12, 14, 15, 16, 18, 19, 20, 25, 26, 27, 28, 29, 30, 32, 36, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 53, 54, 56, 57, 58, 60, 61, 62, 63, 67, 68, 69, 70, 71, 72, 73, 74, 75, 82, 83, 84, 90, 93, 94, 95, 96, 97, 104], "earli": [68, 82], "earlier": [5, 49, 50, 68, 82], "easi": [7, 36, 39, 43, 44, 54, 65, 72], "easier": [2, 39, 43, 57, 72, 95], "easili": [1, 9, 14, 27, 41], "eason": 9, "echo": [51, 90], "econom": [11, 34, 46, 47, 85], "ecosystem": 34, "ecut": 9, "edg": 54, "edit": [8, 16, 26, 43, 50, 60, 83], "editor": 44, "educ": [28, 54], "edward": [34, 85], "ee": [21, 22], "effect": [1, 14, 19, 27, 28, 39, 43, 45, 47, 53, 57, 58, 60, 62, 67, 69, 72, 73, 74, 87, 90, 95, 98, 99], "effici": [2, 34, 42, 46, 47, 53, 62, 67, 84, 85, 93, 96], "effort": [28, 45, 46, 47, 76], "eft": 74, "either": [8, 26, 42, 46, 49, 50, 64, 67, 83, 84], "electron": 102, "element": [1, 5, 44, 63, 95, 96, 98, 99], "elementari": [11, 15], "elev": 46, "elicit": [39, 61, 68, 69], "elimin": [20, 32, 67], "elizabeth": [34, 85], "elment": 51, "elo": 68, "els": [46, 51, 52, 68, 76, 95], "embed": [2, 34, 44, 46, 48, 51, 52, 53, 85, 98], "embed_token": 46, "emerg": 78, "emit": 60, "emphas": [3, 83], "empir": [3, 36, 39, 90, 94, 95], "emploi": [1, 5, 7, 14, 20, 27, 28, 32, 37, 44, 45, 46, 47, 50, 51, 53, 54, 56, 62, 73, 80, 93], "empow": [20, 32, 34, 85], "empti": [8, 26, 93], "emptyset": 76, "enabl": [4, 15, 43, 44, 47, 58, 64, 80, 93, 94, 95, 98, 99], "encod": [2, 3, 18, 29, 41, 44, 46, 48, 51, 53, 94, 95, 98, 99, 102], "encode_dialog_prompt": 51, "encode_head": 51, "encode_messag": 51, "encoding_for_model": 51, "encompass": 45, "encount": [1, 50, 93], "encourag": [3, 18, 29, 36, 39, 42, 50, 58, 70, 82], "end": [1, 2, 5, 7, 8, 14, 18, 26, 27, 29, 38, 42, 44, 47, 49, 51, 52, 57, 58, 59, 60, 62, 63, 68, 69, 70, 72, 76, 82, 83, 84, 87, 93, 94, 95, 96, 98, 99, 100], "end_header_id": 51, "end_of_text": 51, "enforc": [80, 82, 95], "engin": [11, 28, 54, 65], "english": [8, 26, 46], "enhanc": [11, 14, 27, 28, 31, 34, 37, 46, 47, 53, 54, 57, 73, 85], "enlarg": 90, "enlighten": [14, 27], "enlist": 47, "enough": [4, 9, 39, 46, 49, 60, 72, 96], "ensembl": [37, 73], "ensur": [1, 5, 9, 15, 16, 19, 20, 32, 41, 47, 49, 50, 53, 54, 60, 67, 76, 84, 93, 97], "entail": 2, "entir": [24, 48, 83, 84, 97], "entri": [14, 16, 27, 28, 44, 51, 52], "entropi": [38, 41, 50, 57, 58, 61, 69], "entry_point": 10, "enumer": [51, 52, 95], "env": 13, "environ": [5, 13, 36, 50, 54, 63, 67], "eo": [51, 67], "eos_id": 51, "eos_idx": 51, "eos_reach": 51, "eot_id": 51, "ep": [51, 52, 97], "episod": [5, 63, 65], "epoch": [20, 28, 32, 46, 48, 49, 61, 63, 65, 80, 84, 87], "epsilon": [51, 52, 57, 62, 67, 97], "epsilon_": 57, "equal": [36, 48, 57, 94, 96, 98], "equat": [72, 83, 90, 99], "equip": [82, 83, 96, 102], "equival": [3, 34, 41, 43, 70, 72, 85, 93], "erhang": [34, 85], "eric": [34, 85], "ermon": [34, 85], "error": [18, 29, 39, 43, 50, 51, 54, 62, 67, 100], "especi": [18, 29, 90, 97], "essenti": [41, 43, 64], "est": 92, "establish": [47, 72, 82], "estat": 92, "estim": [12, 36, 38, 42, 49, 59, 62, 67, 72, 92], "estrang": 92, "etc": [8, 18, 26, 29, 46], "ethic": [11, 68], "eval": [7, 9, 21, 22, 61], "eval_step": 65, "evalperf": 10, "evalplu": [14, 22, 27], "evalu": [7, 8, 11, 12, 19, 26, 30, 34, 36, 38, 39, 45, 49, 57, 68, 71, 73, 74, 80, 82, 83, 84, 85, 90, 96], "evan": [34, 85], "evas": 68, "even": [4, 20, 32, 36, 39, 47, 50, 54, 59, 60, 72, 95, 96, 98, 99], "evenli": [25, 42], "event": [5, 61, 62], "everi": [1, 4, 15, 18, 29, 69, 94, 97], "evid": [39, 95], "evol": [14, 27, 28], "evolut": [20, 32], "evolutionari": [20, 32], "evolv": [20, 32], "exact": [15, 44], "exactli": [36, 67], "exam": 11, "examin": 58, "exampl": [4, 5, 8, 12, 14, 18, 26, 27, 28, 29, 34, 36, 39, 41, 43, 44, 46, 49, 50, 53, 56, 59, 60, 64, 68, 69, 70, 73, 74, 75, 76, 80, 82, 87, 94, 95, 97], "exce": [8, 26, 37, 51, 82], "exceed": [94, 95], "excel": 47, "except": [4, 20, 32, 37, 46, 47, 50, 51, 60, 96], "exceptiongroup": 13, "excess": [47, 57, 67], "exchang": 87, "exclud": 57, "exclus": [11, 48, 84], "execut": [9, 13, 28, 34, 35, 41, 42, 44, 50, 53, 54, 85, 95], "executor": 54, "exemplar": [69, 78], "exemplifi": 70, "exhibit": [46, 57, 64, 80, 83], "exist": [11, 12, 14, 18, 24, 27, 29, 41, 50, 54, 75, 82, 94, 95, 101], "exit": 82, "exp": [5, 51, 52, 56, 59, 72, 99, 100], "expand": [47, 53], "expans": 95, "expbal": 93, "expect": [5, 18, 29, 36, 42, 47, 50, 54, 57, 62, 64, 67, 68, 90, 95], "expens": [39, 44, 59, 76], "experi": [1, 2, 5, 15, 20, 32, 46, 49, 50, 58, 59, 61, 63, 65, 68, 69, 73, 84, 87], "experience_mak": 65, "experiment": [11, 28], "expert": [15, 34, 46, 47, 50, 54, 85], "expert1": 93, "expert2": 93, "expert3": 93, "expertis": 50, "explain": [43, 50, 54, 56, 69], "explan": [50, 69], "explicit": [3, 47, 94, 95, 98, 99], "explicitli": [8, 26, 49, 68, 70, 83], "exploit": [4, 57], "explor": [43, 49, 57, 58, 62, 64, 78, 80], "exponenti": 95, "export": 13, "express": [4, 12, 59, 61], "extend": [22, 44, 46, 47, 50, 51, 53, 65, 76, 80], "extens": [13, 34, 43, 53, 64, 95], "extern": [28, 80], "extra": [2, 9, 43, 82, 94, 95], "extract": [14, 27, 50, 56, 58, 69], "extractor": 56, "extrapol": [1, 94], "extrem": [1, 15, 36, 39, 47], "f": [10, 51, 52, 82, 92, 94, 95, 100], "f_": [56, 58, 76, 93, 98, 99], "face": [73, 80], "facilit": [1, 41, 64, 67], "fact": 95, "factor": [1, 38, 54, 56, 67, 93, 94, 95, 100], "factual": [53, 80], "fail": [43, 50, 53, 54], "failur": [39, 50, 54], "fair": 96, "faith": 50, "faithfulli": [39, 64], "fake": 64, "fals": [46, 51, 52, 97], "famili": [39, 42, 44, 95], "fan": [34, 85], "fangyun": [34, 85], "fanjia": [34, 85], "far": [50, 57, 61, 67], "fashion": [18, 29, 83], "fast": 51, "fastavro": 13, "faster": [7, 57], "fastjsonschema": 13, "faulti": 50, "favor": [56, 82], "featur": [39, 44, 51, 56, 65, 97], "februari": 54, "fed": [2, 54], "federico": [34, 85], "feed": [38, 51, 52, 53, 93, 100], "feed_forward": [51, 52], "feedback": [5, 34, 39, 44, 45, 46, 47, 50, 53, 54, 59, 61, 64, 72, 74, 76, 80, 84, 85], "feedforward": 2, "fei": [34, 85], "felip": [34, 85], "feng": [34, 85], "few": [3, 4, 5, 11, 18, 29, 34, 41, 42, 44, 68, 69, 74, 76, 78, 84, 85], "fewer": [43, 46, 94, 95], "ff": [38, 100], "ffff": 102, "ffff\u7684\u8303\u56f4": 102, "ffn": [1, 46, 47, 51, 52, 53, 93], "ffn_norm": [51, 52], "fiction": 3, "field": [8, 15, 18, 19, 26, 29, 30, 54], "fifo": 61, "figur": [1, 38, 45, 47, 49, 61, 64, 68, 80, 90], "file": [8, 13, 26, 28, 35, 44, 51, 54], "filelock": 13, "fill": [44, 47, 49], "filter": [5, 9, 14, 27, 28, 45, 46, 50, 53, 54, 57, 58, 75, 76, 82, 84, 90], "fim": [47, 54], "final": [1, 2, 3, 5, 15, 18, 20, 29, 31, 32, 36, 42, 45, 46, 47, 48, 49, 50, 54, 56, 57, 58, 59, 63, 64, 67, 68, 69, 70, 71, 72, 74, 75, 76, 80, 82, 84, 93], "find": [8, 11, 26, 37, 38, 39, 41, 42, 46, 49, 50, 57, 63, 68, 69, 72, 74, 80, 82, 83, 84, 90, 96], "fine": [4, 5, 8, 19, 20, 26, 28, 30, 32, 50, 54, 59, 62, 64, 70, 71, 72, 74, 83, 84, 87, 90, 94, 95], "finer": 93, "finest": 92, "finetun": [4, 14, 20, 27, 32, 39, 53, 68, 76, 82, 84], "finetuning_typ": 103, "finn": [34, 85], "fire": 10, "first": [1, 2, 3, 5, 9, 14, 18, 27, 28, 29, 36, 42, 43, 44, 45, 46, 47, 49, 50, 51, 52, 54, 57, 58, 59, 60, 61, 63, 64, 65, 69, 72, 73, 74, 75, 80, 82, 83, 84, 90, 93, 96, 99, 100], "first_k_dense_replac": 46, "firstli": 43, "fit": [38, 61, 65, 72, 95], "five": [20, 32, 54], "fix": [1, 5, 38, 43, 50, 61, 63, 84, 94, 97], "flag": 51, "flagopen": 19, "flash": [21, 22], "flash_attn": 65, "flatten": [51, 52, 95], "flavor": 34, "flaw": 54, "flexibl": [58, 93], "flexibli": 93, "flip": [64, 83], "float": [51, 52, 72, 95, 97], "float32": [51, 52], "fluenci": 36, "focu": [8, 26, 28, 36, 48, 53, 61, 84], "focus": [7, 11, 13, 19, 28, 30, 45, 47, 53, 64, 80], "follow": [1, 2, 3, 4, 5, 7, 8, 9, 10, 13, 14, 15, 20, 26, 27, 32, 34, 35, 44, 45, 47, 48, 49, 50, 51, 52, 53, 56, 57, 59, 60, 62, 63, 64, 67, 68, 69, 70, 72, 73, 75, 80, 85, 87, 93, 94, 95, 96], "fool": 84, "foral": [2, 58], "forc": [15, 41, 43, 82, 83], "forcefulli": 82, "forgo": 61, "form": [1, 20, 32, 42, 44, 47, 59, 61, 67, 70, 74, 84, 94, 95], "formal": [58, 62], "format": [13, 15, 18, 20, 29, 32, 44, 47, 68, 80], "formatt": 51, "former": 58, "formul": [36, 58, 59, 67, 73, 93], "fortun": 59, "forum": [34, 85, 87], "forward": [38, 51, 52, 53, 93, 94, 97, 100, 104], "foster": 58, "fotio": [34, 85], "found": [1, 2, 5, 11, 18, 29, 41, 42, 43, 49, 54, 58, 68, 94, 95], "foundat": [14, 20, 27, 32, 44, 48, 50, 54, 67], "four": [3, 11, 20, 32, 44, 49, 50, 73, 80, 96], "frac": [1, 5, 12, 36, 38, 51, 52, 56, 57, 59, 60, 62, 63, 64, 67, 72, 73, 82, 93, 94, 95, 96, 97, 98, 99, 100, 101], "fraction": 12, "framework": [10, 18, 29, 46, 47, 53, 54, 64], "fraser": [34, 85], "free": [13, 34, 47, 54, 75, 85], "freez": [56, 104], "freq": [51, 52, 95], "freqs_ci": [51, 52, 95], "frequenc": [1, 44, 93, 95, 98, 99], "frequent": [94, 95], "fresh": 84, "friendli": 68, "from": [1, 3, 4, 5, 7, 8, 10, 11, 12, 13, 15, 18, 20, 21, 24, 25, 26, 28, 29, 31, 32, 34, 37, 39, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 56, 57, 59, 60, 61, 62, 63, 67, 70, 71, 72, 73, 74, 75, 76, 80, 82, 83, 84, 85, 87, 90, 93, 94, 95, 96, 97, 99, 101, 102], "frontier": 15, "frozen": 57, "frozenlist": 13, "fsdp": 82, "fsfairx": 37, "fsspec": 13, "fu": [34, 85], "fulfil": 64, "fuli": [34, 85], "full": [39, 43, 44, 45, 47, 51, 52, 65, 103], "fulli": [1, 8, 26, 31, 39, 47, 54, 57], "function": [1, 4, 5, 9, 16, 34, 36, 39, 43, 44, 48, 49, 53, 54, 57, 58, 59, 60, 61, 62, 63, 64, 67, 72, 73, 82, 83, 94, 95, 97, 98, 99, 100], "fundament": [4, 39], "further": [8, 14, 26, 27, 28, 37, 42, 44, 45, 46, 49, 50, 53, 54, 58, 61, 62, 76, 93, 95, 99], "furthermor": [47, 57, 68], "futur": [39, 46, 47], "g": [13, 14, 18, 27, 29, 47, 49, 50, 54, 56, 57, 62, 69, 70, 72, 75, 80, 82, 83, 94, 98, 99], "g_": [47, 56, 93], "g_t": 67, "gabriel": [34, 85], "gae": [62, 67], "gain": [2, 4, 14, 27, 43, 49, 54, 64, 69], "gamma": [5, 36, 47, 51, 52, 62, 63, 64, 67, 94, 97], "gao": [34, 85], "gap": [11, 14, 27, 36, 37, 39, 50, 54, 71], "gate": [34, 47, 56, 85, 93], "gate_proj": 46, "gather": [49, 51, 53], "gating_dim": 93, "gaussian": 100, "gave": [8, 26], "gb2312\u56e0\u4e3a\u53ea\u8868\u793a\u666e\u901a\u6570\u5b57": 102, "gbk\u662fascii": 102, "ge": [12, 18, 29, 34, 56, 58, 60, 62, 72, 85], "geglu": 100, "gelu": 100, "gemini": [42, 45, 61], "gen": 51, "gener": [1, 2, 3, 4, 5, 8, 9, 12, 13, 14, 19, 20, 26, 27, 28, 30, 31, 32, 34, 36, 41, 42, 43, 44, 45, 46, 47, 49, 50, 53, 54, 57, 59, 60, 61, 62, 64, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 78, 80, 82, 83, 85, 90, 94, 96], "generalist": 58, "generate_kwarg": 65, "generate_max_len": 65, "generation_logprob": 51, "generation_token": 51, "generativeai": 13, "generativelanguag": 13, "geometr": [1, 99], "geometri": 50, "get": [15, 34, 35, 36, 44, 54, 56, 72], "get_unique_el": 44, "gg": 38, "gibb": 59, "gibberish": [36, 57], "girish": [34, 85], "git": [10, 13], "github": [8, 10, 13, 19, 26, 28, 41, 45, 48, 51, 52, 53, 54], "give": [2, 74, 75, 95], "given": [1, 2, 5, 8, 18, 20, 26, 29, 32, 36, 38, 39, 41, 43, 44, 48, 49, 50, 51, 52, 53, 54, 56, 58, 59, 60, 61, 62, 63, 69, 70, 72, 74, 75, 76, 83, 84, 90, 94, 95], "glob": 51, "glu": [51, 52], "go": [65, 67], "goal": [2, 43, 54, 57, 59, 72, 75, 82, 83], "gold": [21, 41, 42], "gomez": [34, 85], "good": [9, 21, 36, 39, 41, 47, 50, 62, 72, 75], "googl": [10, 13], "googleapi": 13, "google\u7684bert\u6a21\u578b\u5728\u5206\u8bcd\u7684\u65f6\u5019\u4f7f\u7528\u7684\u662fwordpiece\u7b97\u6cd5": 101, "gpt": [3, 4, 5, 7, 8, 14, 15, 21, 22, 26, 27, 34, 39, 50, 51, 74, 84, 85, 90], "gpt2": 21, "gpt3": 21, "gpt4": 45, "gpu": [47, 82, 103], "gqa": [34, 53, 85], "grade": 15, "gradient": [1, 2, 5, 48, 60, 61, 63, 67], "gradient_accumulation_step": 103, "gradient_checkpoint": 65, "gradual": 44, "grai": [34, 85], "grain": [19, 30, 50, 64], "gram": [28, 54], "grammar": [18, 29], "grammat": [18, 29], "grangier": [34, 85], "granular": 11, "graphic": 15, "great": 51, "greater": 11, "greatli": [4, 104], "greedi": [3, 10, 14, 27, 37, 50, 51, 90], "greg": [34, 85], "gretchen": [34, 85], "grid": 95, "grl": [9, 34, 85], "ground": [16, 25, 39, 45, 46, 47, 54, 57, 58, 59, 72, 75], "group": [41, 42, 45, 46, 47, 51, 53, 54, 57, 73, 80, 93, 96], "grow": [1, 4, 61], "grpcio": 13, "grpo": [21, 22, 45, 53, 57, 58], "gsm8k": [21, 37, 54, 90], "gu": [34, 85], "guan": [34, 85], "guangbo": [34, 85], "guant": [34, 85], "guarante": [12, 58, 60, 62, 93], "guard": 24, "guess": 11, "guid": [18, 29, 37, 58, 72, 73, 80], "guo": [34, 85], "guowei": [34, 85], "guss": [34, 85], "h": [1, 34, 46, 49, 51, 52, 85, 93, 94, 96, 104], "h06a4308_0": 13, "h100": 82, "h11": 13, "h1181459_1": 13, "h1234567_1": 13, "h39e8969_0": 13, "h5eee18b_0": 13, "h5eee18b_1": 13, "h5eee18b_6": 13, "h6a678d5_0": 13, "h6a678d5_1": 13, "h800": 47, "h955ad1f_1": 13, "h_": [2, 95], "h_j": 95, "h_n": 2, "ha": [1, 3, 4, 5, 7, 15, 18, 29, 36, 39, 45, 46, 49, 50, 51, 52, 53, 57, 61, 62, 73, 76, 83, 90, 94, 96, 100], "hack": [47, 49, 56, 57, 68, 80], "had": [5, 16, 41], "half": [39, 41, 44], "hallucin": 54, "hallucinatori": 46, "halv": 1, "ham": 60, "han": [34, 85], "hand": [12, 16, 38, 39, 47, 61], "handl": [2, 54], "handwritten": 12, "hanq": [34, 85], "hanwei": [34, 85], "hao": [34, 85], "haoran": [34, 85], "haowei": [34, 85], "haoyu": [34, 85], "happen": 95, "har": 47, "hard": [42, 43, 59, 70, 96], "harder": 42, "harm": [68, 70], "harmless": [53, 69, 70, 74], "harri": [34, 85], "hat": [36, 57, 59, 62, 67, 73, 83], "have": [1, 2, 3, 12, 14, 15, 18, 27, 29, 35, 36, 38, 39, 42, 43, 45, 47, 49, 50, 53, 56, 57, 59, 60, 68, 72, 73, 74, 75, 82, 87, 90, 93, 94, 95, 97, 100], "hbb": [11, 34, 85], "hd_": 1, "he": [34, 85], "head": [2, 34, 38, 46, 47, 51, 52, 74, 85, 94, 95, 102], "head_dim": [51, 52], "health": 11, "heart": 75, "heavi": [38, 83, 96], "hebgen": [34, 85], "heewoo": [34, 85], "heidi": [34, 85], "height": 51, "held": 4, "help": [2, 5, 15, 34, 43, 44, 46, 47, 49, 50, 53, 54, 56, 61, 63, 68, 70, 74, 94], "helpfulli": 63, "helpsteer2": 21, "henc": [45, 59, 74], "hendryck": [34, 85], "henighan": [34, 85], "henriqu": [34, 85], "herbert": [34, 85], "herd": 50, "here": [1, 4, 8, 20, 26, 32, 34, 36, 38, 48, 49, 50, 90, 94, 95], "hess": [34, 85], "heurist": [5, 14, 15, 18, 27, 29], "hf": 72, "hh": 68, "hidden": [46, 51, 52, 56, 93, 94, 100], "hidden_dim": [51, 52], "hidden_s": [46, 93], "high": [2, 12, 14, 15, 27, 28, 36, 37, 41, 47, 49, 50, 53, 54, 56, 57, 58, 60, 61, 67, 73, 74, 75, 76, 83, 87, 92, 98, 99], "highconfid": 76, "higher": [19, 30, 37, 42, 43, 47, 50, 53, 54, 61, 64, 73, 93, 94], "highest": [7, 8, 15, 20, 26, 32, 37, 47, 50, 61, 73, 74, 92, 93, 98], "highli": [15, 39, 54, 56, 75, 76, 84], "highlight": 28, "highqual": 47, "hilton": [34, 85], "hinder": [67, 73], "hing": 59, "histor": 62, "histori": [11, 36], "hoc": 69, "hold": [38, 84], "holdgraf_evidence_2014": 34, "holist": [13, 34, 85], "home": 13, "homepag": 57, "honesti": 56, "hongcheng": [34, 85], "honghui": [34, 85], "hongyi": [34, 85], "hood": [14, 27], "hook": 13, "hope": 15, "hotfix": 13, "hou": [34, 85], "hour": 47, "hous": 45, "how": [5, 7, 11, 15, 35, 36, 39, 42, 50, 54, 56, 58, 59, 62, 69, 70, 78, 80, 84, 90, 95], "howev": [12, 18, 29, 36, 37, 39, 46, 49, 50, 56, 57, 59, 63, 64, 70, 72, 73, 82, 93, 94, 95, 97, 99], "hstack": [51, 52], "html": 102, "http": [10, 13, 14, 18, 20, 27, 29, 32, 34, 85], "httpcore": 13, "httplib2": 13, "httpx": 13, "hu": [34, 85], "huajian": [34, 85], "huan": [34, 85], "huang": [34, 85], "huazuo": [34, 85], "hub": 13, "huge": [43, 95], "huggingfac": [13, 65], "hugh": [34, 85], "hui": [34, 85], "human": [4, 5, 7, 9, 11, 15, 16, 18, 20, 25, 29, 32, 34, 36, 39, 44, 46, 47, 50, 53, 54, 59, 61, 63, 64, 68, 69, 70, 71, 72, 74, 75, 76, 80, 84, 85], "humanev": [20, 22, 28, 32, 37, 54], "humanevalplu": 10, "humanevalplus_releas": 10, "hundr": [47, 95], "hunter": [34, 85], "hurt": 54, "hybridengin": 65, "hyc": [34, 54, 85], "hyper": [47, 48, 51, 52, 53, 57, 62, 64], "hyperparamet": [38, 42, 50, 53, 57, 60, 61, 64, 67, 87], "hypothes": 1, "hypothesi": 4, "i": [2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 14, 15, 18, 19, 20, 21, 22, 26, 27, 28, 29, 30, 31, 32, 33, 35, 36, 38, 39, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 56, 57, 58, 59, 60, 61, 62, 63, 64, 67, 68, 69, 70, 72, 73, 74, 75, 76, 80, 82, 84, 85, 90, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102], "i_": [18, 29], "i_t": [18, 29], "icl": 90, "id": [5, 34, 51, 85], "id1": 13, "id2": 13, "idea": [64, 76, 94, 95], "ideal": [8, 11, 26, 95], "ident": [1, 53, 57, 59, 61, 75, 84, 93], "identif": 54, "identifi": [5, 7, 11, 18, 29, 37, 53, 54, 57, 58, 61, 64, 68, 70, 73, 76], "idna": 13, "ifev": 46, "ift": 74, "ignor": [36, 41, 51, 95], "ignore_index": 51, "igor": [34, 85], "ij": 90, "ik_": [51, 52, 94, 95], "illeg": 68, "illia": [34, 85], "illustr": [39, 45, 47, 49, 51, 52, 80], "ilya": [34, 85], "im": [51, 52, 94, 95], "imag": [38, 82], "imaginari": [94, 95], "imbal": [47, 93], "imit": 39, "impact": [28, 37, 42, 54, 57, 84, 94], "imped": 57, "imper": [8, 26], "imperfect": 76, "implement": [5, 12, 20, 32, 46, 47, 48, 53, 54, 67, 76], "impli": 57, "implicit": 59, "implicitli": [39, 59, 83], "import": [10, 12, 28, 36, 39, 49, 51, 52, 59, 72, 74, 75, 87, 95, 97], "importantli": [54, 59, 82, 96], "importlib": 13, "impress": 54, "improv": [2, 4, 14, 27, 31, 34, 36, 39, 41, 43, 44, 46, 47, 48, 49, 50, 51, 53, 54, 58, 60, 62, 69, 73, 74, 75, 76, 78, 80, 82, 83, 84, 85, 90, 97], "inabl": 57, "inappropri": 64, "incentiv": 83, "includ": [2, 8, 11, 12, 15, 19, 20, 24, 25, 26, 28, 30, 32, 35, 37, 38, 39, 41, 42, 44, 46, 47, 49, 50, 51, 53, 54, 59, 68, 71, 72, 94, 95, 97, 103], "inclus": 54, "incorpor": [1, 28, 47, 49, 50, 53, 57, 62, 67, 98, 99], "incorrect": [41, 43, 73, 76, 83, 84], "incorrectli": [15, 59], "increas": [11, 14, 20, 27, 28, 32, 42, 44, 47, 50, 57, 59, 60, 64, 90, 93, 95, 99], "increasingli": 80, "increment": 94, "inde": 64, "indent": 54, "independ": [18, 29, 44, 82, 97, 101], "index": [51, 52, 61, 62, 94, 95, 96], "indic": [5, 11, 42, 51, 56, 61, 64, 67, 70, 82, 90, 94, 95, 96, 98], "individu": [38, 57, 58, 97, 98, 99], "induc": [3, 47, 72], "inequ": 59, "inf": [51, 52], "infer": [8, 26, 28, 36, 44, 46, 47, 48, 50, 51, 57, 64, 65, 69, 72, 75, 93, 95, 96, 97], "inference_mod": [51, 52], "infin": 28, "inflat": 54, "influenc": [57, 58, 90], "infomax": 61, "inform": [1, 5, 8, 15, 26, 28, 34, 35, 41, 49, 50, 53, 62, 73, 85, 93, 95, 98, 99, 102], "infrastructur": 95, "infti": 38, "inher": [2, 5], "inherit": 56, "init": 35, "init_kl_coef": 65, "initi": [5, 14, 18, 20, 27, 29, 32, 43, 44, 46, 49, 50, 51, 57, 59, 61, 63, 68, 70, 76, 80, 87], "inject": [1, 94, 95, 104], "inlin": [34, 59], "inner": [98, 99], "innov": [46, 96], "input": [1, 5, 8, 9, 14, 18, 20, 26, 27, 29, 32, 34, 41, 42, 43, 44, 48, 49, 51, 52, 54, 58, 60, 61, 63, 64, 67, 69, 72, 75, 87, 94, 95, 96, 97, 98, 99, 100, 101], "input_kei": 65, "input_text_mask": 51, "inputgen": 10, "inputoutput": 43, "insert": [1, 34, 41, 51, 61], "insid": 62, "insight": 59, "inspect": 16, "inspir": [14, 27, 58], "inst": 44, "instabl": [12, 67, 97], "instag": 50, "instal": 10, "instanc": [2, 8, 26, 46, 47, 51, 56, 68, 80], "instead": [1, 3, 5, 8, 12, 20, 26, 32, 39, 42, 44, 48, 51, 52, 57, 61, 62, 64, 67, 68, 76, 94, 95, 97, 99], "instruciton": [8, 26], "instruct": [4, 5, 7, 8, 21, 22, 26, 34, 35, 37, 39, 45, 46, 49, 50, 56, 58, 64, 68, 69, 73, 80, 82, 83, 85, 87], "instructgpt": 63, "instruction_prefix": 10, "instructionfollow": 74, "int": [46, 51, 52, 72, 95, 97], "int_": 61, "integ": [51, 57], "integr": [11, 47, 53, 54, 58], "intend": 57, "intens": [46, 54], "intent": [5, 73], "intention": 64, "interact": [36, 50, 51, 52, 61, 64, 67, 97], "interc": [56, 93, 94, 95, 96, 98], "interchang": 102, "interdepend": 97, "interest": [39, 50, 72], "interesting": 36, "interestingli": 54, "interfac": 5, "interfer": 97, "interleav": 83, "interlm2": 21, "intermedi": [31, 38, 39, 45, 46, 47, 78, 80, 84, 93], "intermediate_s": 46, "intern": [39, 47, 50], "internet": 63, "interpol": 44, "interpret": [5, 28, 49, 62], "interv": 90, "intervent": 82, "interview": 44, "intric": 53, "intrigu": 80, "intrins": 80, "introduc": [7, 11, 14, 15, 18, 27, 28, 29, 31, 36, 42, 47, 50, 51, 53, 54, 57, 58, 62, 69, 72, 80, 93, 94, 95, 97], "introduct": 69, "intuit": [36, 39, 59, 99], "invas": 68, "invest": [46, 61, 94, 95], "investig": [47, 57, 58, 64, 84, 87, 90], "involv": [8, 15, 26, 38, 47, 50, 93], "ion": [34, 85], "ip": 69, "ipo": 59, "ipynb": 34, "iq_": [51, 52, 94, 95], "irrelev": 28, "is_safeti": 49, "ise": 10, "isequival": 57, "isin": 51, "isol": 54, "issu": [3, 45, 47, 50, 56, 68, 69, 80, 83, 93, 95, 97], "item": [65, 72], "iter": [5, 18, 20, 29, 32, 43, 51, 53, 61, 63, 64, 72, 73, 74, 83], "itertool": 13, "its": [4, 14, 16, 18, 27, 29, 39, 41, 42, 44, 46, 47, 50, 51, 52, 53, 54, 56, 57, 59, 61, 64, 67, 68, 72, 74, 80, 82, 83, 84, 90, 93, 94, 95, 96, 97, 99, 100], "itself": [18, 29, 45, 74, 83], "ix_": [51, 52, 94, 95], "j": [34, 36, 47, 51, 52, 57, 58, 60, 62, 64, 67, 75, 85, 93, 94, 95, 96, 99], "j_": [62, 75], "j_1": 64, "j_q": 64, "jack": [34, 85], "jacob": [34, 85], "jain": [34, 85], "jakob": [34, 85], "jame": [34, 85], "jan": [34, 85], "jaraco": 13, "jare": [34, 85], "java": 54, "javascript": 54, "jeepnei": 13, "jeff": [34, 85], "jeffrei": [34, 85], "jerri": [34, 85], "jgzp23": [34, 53, 85], "jhg": [13, 34, 85], "ji": [34, 85], "jiaheng": [34, 85], "jiajun": [34, 85], "jian": [34, 85], "jiang": [34, 85], "jianhong": [34, 85], "jianlin": [34, 85], "jianwei": [34, 85], "jianxin": [34, 85], "jianzhong": [34, 85], "jiaqi": [34, 85], "jiashi": [34, 85], "jiatao": [34, 85], "jiawei": [34, 85], "jiaxi": [34, 85], "jie": [34, 85], "jin": [34, 85], "jingren": [34, 85], "jingxiang": [34, 85], "jingyang": [34, 85], "jinja2": 13, "jinz": [34, 85], "jmespath": 13, "joblib": 13, "john": [34, 85], "jointli": 1, "jone": [34, 85], "jong": [34, 85], "joseph": [34, 85], "josh": [34, 85], "joshua": [34, 85], "json": [8, 26, 46, 51, 103], "jsonl": 10, "jsonlin": 13, "judg": [7, 22, 42, 43, 50, 64, 74, 75], "judgement": 64, "judgment": [36, 54], "jun": [34, 85], "junji": [34, 85], "junxiao": [34, 85], "junyang": [34, 85], "jupyt": [34, 35], "jupyterbook": 34, "jupytext": 35, "just": [11, 13, 34], "k": [1, 2, 3, 5, 12, 15, 20, 32, 34, 41, 47, 49, 50, 51, 52, 56, 58, 60, 61, 63, 67, 71, 85, 90, 93, 94, 95, 96, 98, 99, 104], "k_": [47, 51, 52, 58, 62, 93, 94, 95], "k_1": 62, "k_i": 62, "k_r": [47, 93], "kai": [34, 85], "kaig": [34, 85], "kaiser": [34, 85], "kang": [34, 85], "kaplan": [34, 85], "karl": [34, 85], "karma": 3, "katarina": [34, 85], "kati": [34, 85], "katti": 25, "ke": [34, 85], "keep": [42, 46, 47, 49, 50, 54, 57, 83, 93, 100], "keepdim": [51, 52, 97], "kei": [1, 15, 28, 39, 42, 43, 44, 46, 47, 51, 52, 53, 54, 57, 64, 67, 72, 90, 94, 95, 97, 98, 99], "kelton": [34, 85], "keme": [34, 85], "kenton": [34, 85], "keqin": [34, 85], "kernel": 35, "kexin": [34, 85], "keyr": 13, "keyword": 7, "khlaaf": [34, 85], "kind": [5, 34, 64], "king": [34, 85], "kl": [5, 49, 59, 62, 63, 72], "kmh": [4, 34, 85], "knew": 36, "knight": [34, 85], "know": [36, 39, 90], "knowledg": [11, 14, 27, 28, 43, 49, 87, 93, 98, 99], "known": [36, 51, 52, 73, 95], "kosaraju": [34, 85], "koushik": [34, 85], "kr": 96, "kristina": [34, 85], "krueger": [34, 85], "kto": [21, 37], "kv": [46, 53], "kv_a_layernorm": 46, "kv_a_proj_with_mqa": 46, "kv_b_proj": 46, "kv_lora_rank": 46, "kw_": 1, "kyunghyun": [34, 85], "l": [2, 5, 18, 29, 31, 34, 36, 38, 49, 51, 52, 58, 59, 60, 61, 62, 63, 67, 72, 73, 74, 75, 76, 83, 85, 92, 93, 94, 95, 96, 97], "l_": [2, 57, 59, 64, 73], "l_1": 64, "l_2": 64, "label": [2, 4, 5, 18, 19, 29, 30, 39, 45, 46, 49, 50, 53, 63, 68, 70, 72, 75, 76, 84, 90], "labor": 54, "lack": [57, 72], "lambda": [2, 60, 61, 62, 64, 67, 94, 95], "lambda_": [56, 94], "land": 68, "langl": [51, 52, 94, 95, 99], "languag": [2, 3, 4, 5, 7, 8, 11, 12, 15, 18, 26, 28, 29, 33, 34, 39, 41, 42, 43, 44, 45, 46, 47, 48, 49, 51, 54, 59, 61, 63, 76, 80, 82, 84, 85, 87, 93, 98, 99], "larg": [1, 2, 3, 4, 5, 7, 9, 12, 18, 28, 29, 33, 34, 36, 39, 42, 46, 47, 49, 50, 51, 52, 54, 57, 59, 60, 61, 63, 67, 76, 80, 85, 87, 93, 96, 99], "larger": [3, 19, 30, 49, 50, 82, 84, 95, 99], "largest": [3, 19, 41, 42, 48, 49, 50], "last": [1, 4, 41, 50, 51, 56, 62, 68, 84], "latent": [46, 47], "later": [18, 29, 50, 82], "latest": [10, 49, 50], "latex": 15, "law": [4, 11, 34, 39, 53, 85, 96], "layer": [1, 2, 3, 4, 5, 38, 41, 46, 48, 51, 52, 53, 56, 63, 65, 93, 94, 95, 96, 100, 104], "layer_id": [51, 52], "layer_idx": 46, "layernorm": [1, 97], "lcb": 13, "lcb_runner": 13, "lcft": 50, "ld_impl_linux": 13, "le": [12, 20, 32, 34, 47, 57, 60, 82, 85, 93, 95, 100], "lead": [28, 36, 39, 43, 44, 47, 48, 50, 57, 60, 68, 84, 94, 95, 97], "leakag": [24, 39, 54], "lean": [34, 85], "learn": [1, 2, 3, 4, 5, 11, 15, 28, 36, 37, 38, 39, 41, 44, 48, 51, 52, 54, 57, 58, 59, 62, 67, 70, 72, 73, 74, 85, 87, 93, 100], "learnabl": 51, "learner": [3, 4, 34, 85], "learning_r": 103, "least": [3, 4, 7, 42, 68], "leather": [34, 85], "leav": [41, 43, 57], "lebr\u00f3n": [34, 85], "lecong": [34, 85], "led": [49, 50, 68], "lee": [34, 85], "leed": 93, "leetcod": [13, 22, 45, 47, 80], "left": [1, 5, 12, 36, 38, 42, 44, 51, 52, 56, 57, 59, 60, 62, 63, 64, 67, 69, 72, 83, 94, 95, 96, 98, 99, 101], "leftarrow": [67, 96], "legal": 68, "lei": [34, 85], "leik": [34, 85], "len": [51, 65, 72], "length": [1, 38, 44, 46, 47, 49, 50, 51, 56, 57, 60, 64, 69, 82, 93, 94, 95, 96, 97], "lengthen": 82, "lengthi": 43, "less": [1, 5, 8, 18, 21, 22, 26, 29, 37, 45, 50, 57, 61, 62, 68, 73, 84, 90, 94], "let": [15, 34, 35, 36, 46, 56, 58, 60, 67, 68, 72, 76, 85, 94, 96, 98, 99], "level": [3, 11, 15, 16, 19, 20, 28, 30, 32, 34, 36, 39, 42, 44, 46, 47, 49, 50, 53, 54, 61, 84, 85], "leverag": [14, 18, 27, 29, 43, 47, 49, 53, 80, 95, 98, 99], "lex": 95, "leyi": [34, 85], "lezama": [34, 85], "li": [34, 85], "liang": [34, 85], "libffi": 13, "libgcc": 13, "libgomp": 13, "librari": [28, 44], "libstdcxx": 13, "libuuid": 13, "lie": [68, 96], "lightman": [34, 85], "like": [2, 8, 11, 26, 34, 35, 36, 39, 43, 47, 53, 54, 60, 61, 68, 70, 75, 80, 84], "likelihood": [2, 36, 37, 41, 59, 60, 69, 72, 73, 76, 84], "limit": [4, 5, 18, 29, 39, 41, 42, 45, 47, 49, 50, 57, 61, 69, 72, 87, 93, 94, 95, 96], "lin": [34, 85], "line": [14, 18, 20, 27, 29, 32, 34, 35, 38, 39, 44, 51], "linear": [1, 2, 44, 46, 48, 49, 51, 52, 53, 56, 57, 82, 90, 95, 99], "linearli": [1, 42, 87], "lingm": [34, 85], "link": 3, "linter": 50, "linzheng": [34, 85], "liqun": [34, 85], "list": [8, 20, 26, 32, 42, 43, 44, 51, 65, 72, 90, 92], "liter": 51, "littl": 54, "litwin": [34, 85], "liu": [34, 85], "livecodebench": [21, 22, 34, 85], "liyu": [34, 85], "lkb": [15, 34, 85], "ll": [34, 51, 96, 104], "llama": [8, 9, 21, 26, 37, 51, 65, 74, 87, 90, 94, 97], "llama2": [21, 49, 90], "llama3": [21, 22, 52], "llion": [34, 85], "llm": [7, 13, 14, 20, 27, 28, 31, 32, 33, 34, 43, 44, 49, 50, 54, 56, 58, 59, 60, 62, 64, 67, 70, 74, 75, 80, 83, 85, 95], "llm4code": 10, "lm": [3, 63], "lm_head": 46, "ln": [34, 51, 61, 85, 95, 97], "load": [47, 51], "load_checkpoint": 65, "load_state_dict": 51, "load_tiktoken_bp": 51, "local": [4, 83], "localhost": 103, "locat": [44, 95], "log": [2, 3, 5, 36, 42, 49, 51, 59, 60, 62, 63, 64, 67, 68, 69, 72, 73, 83, 84, 90, 101], "logging_step": [65, 103], "logic": [43, 47, 53, 54, 80], "logist": 73, "logit": [41, 49, 51, 59, 60, 94], "logprob": 51, "logprobs_i": 51, "long": [2, 5, 8, 26, 34, 50, 51, 54, 57, 60, 80, 82, 85, 94, 95, 99], "longer": [1, 44, 46, 56, 57, 60, 69, 82, 87, 94, 95], "longterm": [34, 85], "look": 43, "loos": 95, "lora": [21, 22], "lose": 74, "loss": [4, 5, 36, 38, 39, 41, 46, 47, 49, 50, 56, 58, 60, 61, 62, 63, 64, 69, 73, 87], "lot": [34, 94], "low": [9, 18, 28, 29, 34, 36, 49, 54, 57, 58, 60, 62, 76, 82, 85, 94, 99, 104], "lower": [8, 26, 57, 60, 90, 94], "lowest": [73, 74, 92, 93, 94], "lr": 28, "lr_scheduler_typ": 103, "lu": [34, 85], "luan": [34, 85], "lukasz": [34, 85], "luke": [34, 85], "luo": [34, 85], "lxwz23": [10, 34, 85], "m": [2, 13, 18, 29, 34, 47, 49, 51, 52, 58, 60, 61, 64, 72, 73, 85, 93, 94, 95, 98, 99], "m_": 72, "m_0": 74, "m_1": 74, "m_2": 74, "m_3": 74, "m_t": 74, "ma": [34, 85], "machin": [1, 3, 34, 44, 85], "maddi": [34, 85], "made": [20, 32, 53, 61, 69, 73], "magic": [21, 22], "magicod": [10, 28, 34, 50, 54, 85], "magnitud": [1, 3, 4, 39, 41, 95, 96], "mai": [1, 28, 37, 39, 43, 45, 47, 50, 57, 60, 62, 68, 69, 70, 73, 80, 93, 95], "mail": 3, "main": [10, 13, 37, 39, 42, 43, 44, 48, 49, 50, 58], "mainli": [39, 43, 47, 54, 58, 73, 80], "mainstream": 54, "maintain": [45, 46, 47, 49, 53, 54, 67, 93], "major": [15, 50, 54, 70, 82, 84], "make": [1, 2, 5, 8, 11, 18, 19, 26, 29, 36, 39, 42, 44, 51, 52, 53, 56, 57, 59, 61, 64, 65, 68, 74, 80, 82, 83, 95, 97, 98, 99], "make_experience_list": 65, "man": [34, 85], "manag": 13, "mani": [3, 4, 15, 18, 29, 34, 35, 44, 54, 63, 95, 102], "mann": [34, 85], "manner": [74, 75], "manta": [34, 85], "manual": [16, 18, 29, 57, 87], "map": 1, "map_loc": 51, "margin": [20, 32, 37, 49, 50], "mark": [34, 44, 50, 85], "markdown": [10, 45], "markdownfil": 35, "markedli": 34, "markup": 34, "markupsaf": 13, "mask": [1, 41, 44, 50, 51, 52, 57], "mass": [20, 32, 36, 51], "massiv": [11, 34, 53, 54, 85], "master_port": 103, "match": [15, 19, 30, 47, 51, 53, 83, 94, 95, 96], "mateusz": [34, 85], "math": [11, 34, 45, 46, 47, 51, 52, 53, 54, 57, 60, 62, 80, 82, 84, 85], "mathbb": [1, 5, 12, 36, 49, 56, 57, 59, 60, 62, 63, 64, 67, 72, 73, 82, 83, 93, 94, 96, 98, 99, 104], "mathbf": [1, 36, 44, 47, 51, 52, 58, 62, 64, 67, 93, 94, 95, 96, 97, 98, 99], "mathcal": [2, 31, 36, 49, 56, 57, 59, 60, 61, 71, 72, 73, 75, 76, 82, 83, 90, 93, 96, 100], "mathemat": [11, 12, 15, 34, 45, 46, 47, 50, 53, 54, 59, 62, 82, 85], "mathmix": 84, "mathrm": 71, "matmul": [51, 52], "matplotlib": 59, "matric": [1, 51, 52, 96, 100, 104], "matrix": [1, 2, 44, 51, 52, 96, 98, 99, 100], "matthew": [34, 85], "matthia": [34, 85], "max": [1, 49, 51, 52, 57, 60, 72, 76, 82, 83, 94, 100], "max_": [58, 59, 72, 82, 95], "max_batch_s": [51, 52], "max_epoch": 65, "max_gen_len": 51, "max_prompt_len": 51, "max_reward": 72, "max_sampl": 65, "max_seq_len": [51, 52], "maxim": [2, 5, 8, 14, 20, 26, 27, 32, 36, 42, 48, 57, 59, 61, 62, 63, 72, 76, 83, 84, 94, 95], "maximum": [36, 41, 44, 49, 50, 51, 57, 58, 59, 61, 72, 82, 94, 95, 96], "mayer": [34, 85], "mazeika": [34, 85], "mbox": [67, 99], "mbpp": [20, 21, 22, 28, 32, 54], "mbppplu": 10, "mbppplus_releas": 10, "mccandlish": [34, 85], "mceval": [28, 34, 54, 85], "mcgrew": [34, 85], "md": [34, 35], "me": 68, "mean": [1, 49, 51, 52, 57, 60, 61, 62, 67, 73, 74, 93, 97, 99], "meaning": 43, "meansquar": [51, 52, 97], "meanwhil": [94, 95, 98, 99], "measur": [5, 11, 34, 36, 41, 50, 62, 69, 76, 82, 85, 87], "mechan": [1, 42, 47, 53, 54, 57, 62, 67, 93, 97, 98, 99], "media": 3, "median": 49, "medium": 44, "mei": [34, 85], "melani": [34, 85], "memori": [1, 2, 9, 62, 96], "men": [34, 85], "meng": [34, 85], "mention": 7, "merg": [20, 32], "mergeable_rank": 51, "messag": [43, 51], "met": 50, "meta": [51, 58, 102], "metadata": [13, 42], "method": [4, 14, 20, 27, 28, 32, 37, 47, 49, 50, 51, 53, 54, 58, 59, 61, 64, 67, 70, 73, 78, 82, 94, 95, 98, 99], "methodologi": [47, 80], "meticul": [46, 54], "metric": [12, 20, 32, 36, 39, 41, 56, 69, 82, 90], "miao": [34, 85], "miaojun": [34, 85], "michael": [34, 85], "michiel": [34, 85], "micro_rollout_batch_s": 65, "micro_train_batch_s": 65, "middl": [15, 18, 29, 44, 47], "might": [7, 50, 87, 97], "mikhail": [34, 85], "mile": [34, 85], "miller": [34, 85], "million": [3, 4, 39, 41, 42, 44, 50, 53, 54], "min": [51, 56, 57, 62, 67, 72, 82, 104], "min_": 59, "min_prompt_len": 51, "mine": 72, "ming": [34, 85], "mingchuan": [34, 85], "mingfeng": [34, 85], "minghua": [34, 85], "minghui": [34, 85], "mingm": [34, 85], "mini": [51, 57, 97], "minim": [47, 59, 61, 82, 94, 95], "minimis": [41, 60], "minimum": [59, 67, 82], "minor": 83, "minu": 51, "minut": [9, 82], "mira": [34, 85], "misalign": 64, "mishkin": [34, 85], "mishra": [34, 85], "mismatch": [60, 83], "misra": [34, 85], "miss": 44, "mistak": [43, 64, 83], "mistralai": 13, "mitchel": [34, 85], "mitig": [5, 28, 46, 47, 50, 54, 56, 62, 63, 67, 69, 73, 76, 83, 93, 95], "mix": [5, 41, 44, 45, 50, 54, 63, 72, 80], "mixtral": 65, "mixtur": [34, 46, 47, 53, 65, 85], "mk": 93, "ml": [4, 95], "mla": [22, 46, 47], "mle": [59, 64, 72], "mlp": 56, "mmlu": 37, "mn": 93, "mode": [39, 59], "model": [2, 7, 8, 9, 10, 11, 12, 14, 15, 18, 19, 20, 21, 22, 26, 27, 28, 29, 30, 32, 33, 34, 36, 37, 39, 41, 43, 44, 45, 46, 47, 48, 59, 60, 62, 65, 67, 68, 69, 70, 71, 72, 82, 85, 87, 93, 96, 97, 98, 99, 104], "model_arg": 51, "model_name_or_path": 103, "modelarg": [51, 52], "modern": [39, 102], "modest": 84, "modif": [2, 3, 8, 20, 26, 32, 41, 50, 94], "modifi": [1, 8, 26, 44, 49, 50, 70, 74, 75, 83, 95, 104], "modul": [13, 28, 46, 51, 52, 54, 93, 97], "modular": 43, "modulelist": [51, 52], "modulenotfounderror": 51, "moe": [22, 46, 47, 53, 93, 96], "moe_intermediate_s": 46, "moegat": 93, "mohammad": [34, 85], "monoton": 72, "mont": [36, 62], "month": 49, "more": [1, 2, 3, 4, 5, 7, 8, 10, 11, 13, 14, 15, 18, 19, 20, 21, 22, 26, 27, 28, 29, 30, 32, 35, 39, 41, 42, 43, 44, 45, 46, 48, 49, 50, 52, 53, 54, 56, 57, 58, 60, 61, 62, 63, 64, 66, 68, 70, 72, 73, 80, 84, 90, 93, 94, 96], "moreov": [34, 46, 57, 83], "morikawa": [34, 85], "most": [1, 3, 4, 15, 28, 34, 37, 44, 47, 49, 50, 51, 54, 56, 64, 74, 84, 93, 95, 102], "mostli": [16, 50], "motiv": [3, 59, 72, 76, 82], "move": [3, 44, 67], "mpmath": 13, "msc": 82, "msgpack": 13, "mt": 93, "mtp": 47, "mu_": 67, "mu_a": 67, "much": [8, 26, 39, 44, 50, 57, 59, 68, 84, 94, 95, 99], "multi": [2, 21, 34, 38, 44, 46, 48, 50, 51, 52, 54, 85], "multidict": 13, "multihead": 1, "multilingu": [34, 50, 53, 54, 85], "multinomi": 51, "multipl": [5, 11, 15, 18, 28, 29, 31, 37, 43, 47, 50, 51, 52, 54, 61, 65, 68, 74, 82, 83, 93, 94, 96, 99], "multiple_of": [51, 52], "multipli": [1, 47, 56, 98, 99], "multiprocess": 13, "multistag": 53, "multitask": [3, 11, 34, 44, 85], "murati": [34, 85], "murtadha": [34, 85], "must": [1, 34, 36, 38, 44, 60, 82, 84, 94, 95], "my": 68, "n": [1, 2, 10, 12, 13, 20, 21, 22, 31, 32, 34, 36, 37, 41, 44, 47, 51, 52, 58, 61, 64, 72, 74, 75, 83, 84, 85, 92, 93, 94, 95, 97, 98, 99, 100, 101], "n_": [18, 29, 38, 58, 96], "n_h": [46, 96], "n_head": [51, 52], "n_layer": [51, 52], "n_routed_expert": [46, 93], "n_shared_expert": 46, "n_t": [18, 29], "n_vocab": 51, "n_word": 51, "nabla_": [36, 59, 60, 67], "naiv": [39, 57, 59, 90, 93], "nakano": [34, 85], "naman": [34, 85], "name": [13, 20, 24, 32, 41, 43, 51, 54, 59, 100], "nano": 61, "narrow": [4, 14, 27], "nativ": 50, "natur": [3, 4, 7, 18, 29, 39, 41, 43, 44, 45, 50, 51, 54, 69, 78, 80, 94, 95, 97], "nccl": 13, "ncurs": 13, "nderstand": 9, "ndim": [51, 52, 95], "ne": [58, 60, 90], "nearbi": 98, "nearli": [1, 3, 39, 54, 57, 61], "necess": 46, "necessari": [56, 57, 74, 87], "necessit": 93, "necssari": [8, 26], "need": [4, 18, 28, 29, 34, 35, 39, 43, 45, 46, 49, 50, 51, 52, 62, 67, 73, 85, 90, 95, 96], "neelakantan": [34, 85], "neg": [36, 53, 54, 56, 60, 64, 70, 73, 76, 83, 84, 90], "negligibli": 49, "neighbor": 68, "net": [34, 85], "network": [2, 34, 48, 51, 52, 53, 56, 67, 85, 93, 100], "networkx": 13, "neural": [1, 2, 34, 51, 52, 80, 85, 100, 101], "neutral": 84, "never": 93, "new": [1, 3, 4, 5, 8, 11, 13, 14, 18, 20, 24, 26, 27, 29, 32, 39, 41, 42, 44, 49, 50, 51, 52, 53, 56, 60, 61, 62, 63, 67, 74, 82, 90, 92, 100], "newli": [20, 24, 32], "newlygener": [18, 29], "next": [1, 2, 13, 18, 29, 36, 41, 43, 50, 61, 63, 68, 74, 75, 95, 99], "next_token": 51, "ng": 13, "ni": [34, 85], "nichol": [34, 85], "nichola": [34, 85], "nick": [34, 85], "nie": [34, 85], "niki": [34, 85], "nikola": [34, 85], "ning": [34, 85], "nl": 71, "nll": 50, "nlp": [2, 4, 5, 39, 63, 92, 97], "nn": [46, 51, 52, 93, 95, 97], "noam": [34, 85], "node": [47, 93], "nois": [57, 73], "noisi": [45, 73, 75, 76], "non": [4, 8, 15, 18, 26, 29, 47, 48, 53, 56, 57, 68, 72, 80, 82, 99], "none": [46, 51, 52], "nonlinear": [51, 52, 100], "noqa": 51, "norm": [51, 52], "norm_ep": [51, 52], "normal": [1, 3, 13, 48, 52, 53, 62, 65, 68, 69, 73, 93], "normalize_reward": 65, "normalized_shap": 51, "notabl": [7, 37, 45, 56, 95], "note": [33, 34, 38, 43, 47, 51, 59, 60, 61, 62, 75], "notebook": 34, "notin": [64, 72], "novel": [14, 20, 27, 32, 58, 64, 72, 75, 98, 99], "novelti": 12, "novemb": 45, "now": [43, 46, 56, 59, 60, 75, 83, 95], "np": [12, 59, 72], "nuanc": [53, 80], "nucleu": 51, "num": 96, "num_attention_head": 46, "num_base_token": 51, "num_channel": 51, "num_episod": 65, "num_experts_per_tok": 46, "num_featur": [51, 97], "num_head": 46, "num_reserved_special_token": 51, "num_sampl": [51, 72], "num_step": 1, "num_train_epoch": 103, "number": [2, 4, 5, 12, 18, 20, 29, 32, 37, 38, 39, 41, 44, 45, 46, 51, 57, 61, 62, 72, 82, 84, 93, 94, 95, 96, 97, 99, 100, 104], "numer": [12, 60, 67, 90], "numpi": [12, 13, 59, 72], "nvidia": [13, 82], "nvjitlink": 13, "nvrtc": 13, "nvtx": 13, "nw": 71, "o": [1, 9, 62, 70, 92, 96, 99], "o1": [21, 82], "o_": [46, 57, 62], "o_1": [62, 70], "o_2": [62, 70], "o_g": 62, "o_i": 57, "o_proj": 46, "obei": 96, "object": [2, 5, 31, 36, 41, 42, 44, 47, 49, 53, 57, 59, 62, 63, 64, 67, 72, 76, 93], "observ": [20, 28, 32, 37, 39, 43, 46, 50, 56, 57, 61, 69, 73, 80, 87, 90, 94, 96], "obstacl": 36, "obtain": [1, 2, 8, 14, 20, 26, 27, 32, 42, 45, 46, 50, 53, 54, 56, 57, 61, 69, 72, 82, 83, 90], "obviou": 36, "occasion": [4, 50], "occur": 80, "occurr": 80, "od": 9, "off": [18, 29, 34, 35, 37, 57, 64, 69], "offer": [54, 61, 64, 69], "offici": 57, "offlin": [21, 22, 36, 41, 46, 54, 64], "offset": 1, "often": [2, 4, 18, 29, 36, 43, 57, 64, 69, 70, 73, 76, 97], "ofthought": 78, "ol": 92, "old": [57, 62, 67, 92], "older": [28, 92], "oliveira": [34, 85], "omit": [46, 93, 96, 100], "onc": [1, 8, 20, 26, 32, 38, 96], "one": [1, 4, 5, 7, 18, 20, 28, 29, 32, 34, 36, 37, 39, 41, 42, 44, 47, 49, 50, 51, 52, 53, 54, 56, 58, 60, 61, 62, 64, 69, 70, 72, 74, 82, 84, 90, 93, 95, 97, 100], "ones": [1, 24, 46, 49, 50, 51, 52, 54, 76, 82, 93, 95, 97], "ones_lik": [51, 52, 95], "onli": [4, 5, 8, 9, 11, 15, 18, 26, 28, 29, 39, 41, 42, 44, 47, 49, 50, 51, 52, 54, 56, 58, 59, 60, 62, 64, 68, 70, 74, 75, 76, 82, 83, 84, 87, 94, 95, 96, 98, 99], "onlin": [21, 22, 24, 42, 46, 57, 58, 64], "open": [7, 8, 20, 22, 25, 26, 28, 32, 34, 47, 51, 53, 54, 57, 74, 80, 85], "openai": [3, 5, 13, 15, 34, 51, 63, 85], "opencod": 22, "openr1": 22, "openreview": [34, 85], "opensourc": 53, "openssl": 13, "oper": [2, 15, 20, 32, 61, 67, 95, 97], "opportun": 61, "oppos": [44, 100], "opposit": [64, 70], "optim": [2, 5, 8, 21, 26, 34, 36, 37, 38, 43, 45, 46, 47, 49, 53, 54, 57, 58, 61, 63, 67, 71, 72, 75, 80, 83, 85, 95], "optima": 83, "optimis": 60, "option": [8, 10, 26, 36, 46, 51, 52, 68, 69, 82, 95], "opu": 45, "oracl": [36, 37, 83], "order": [1, 2, 3, 4, 5, 39, 41, 42, 43, 44, 46, 49, 59, 63, 68, 69, 73, 74, 75, 93, 94, 95, 96, 98, 99], "org": [14, 18, 20, 27, 29, 32, 34, 85], "organ": [54, 59], "origin": [3, 10, 11, 18, 20, 29, 32, 42, 43, 47, 48, 49, 50, 56, 57, 58, 64, 69, 73, 74, 75, 93, 94, 95, 99, 100], "orjson": 13, "orthogon": [14, 27], "oss": [34, 85], "other": [1, 2, 5, 11, 20, 25, 32, 34, 35, 36, 39, 43, 44, 46, 47, 50, 51, 52, 54, 61, 64, 72, 74, 76, 83, 85, 90, 93, 97, 100, 102], "otherwis": [36, 47, 49, 57, 58, 64, 72, 84, 93, 94], "otim": [51, 52, 100], "our": [1, 2, 3, 5, 8, 9, 11, 14, 15, 18, 20, 26, 27, 28, 29, 32, 36, 39, 41, 42, 44, 45, 46, 47, 48, 49, 50, 53, 54, 57, 59, 60, 61, 62, 63, 68, 69, 71, 73, 74, 75, 80, 82, 83, 84, 90, 95, 98, 99, 104], "ourselv": 42, "out": [5, 42, 49, 50, 51, 52, 57, 58, 61, 63, 82, 90, 94, 95, 96], "out_logprob": 51, "out_token": 51, "outbound": 3, "outcom": [57, 58, 80], "outdat": 28, "outer": [51, 52, 95], "outermost": 65, "outlier": 67, "outlin": [20, 32, 73, 80], "outperform": [37, 39, 44, 45, 46, 47, 54, 69, 84, 90], "output": [1, 2, 5, 8, 9, 13, 14, 18, 26, 27, 29, 35, 36, 37, 38, 41, 42, 43, 44, 47, 48, 49, 50, 51, 52, 53, 54, 56, 57, 58, 62, 63, 64, 70, 73, 75, 76, 87, 93, 96, 97, 99], "output_dir": 103, "outsid": 73, "ouyang": [34, 85], "over": [1, 2, 3, 4, 5, 11, 13, 36, 38, 42, 44, 46, 49, 50, 51, 52, 53, 54, 57, 61, 62, 63, 64, 65, 69, 73, 74, 76, 83, 84, 96, 97, 99], "overal": [1, 2, 18, 28, 29, 38, 41, 47, 50, 56, 57, 62, 64, 69], "overconfid": 73, "overfit": [19, 63, 73], "overlap": [28, 54], "overload": 47, "oversight": 15, "overthink": 47, "overview": [34, 51, 52], "overwrite_cach": 103, "owj": [5, 34, 85], "own": [50, 64, 68, 74, 83, 84], "p": [2, 5, 31, 36, 37, 49, 51, 59, 61, 62, 64, 70, 72, 99, 101], "p_": [36, 58, 61, 73, 76, 83, 93], "p_1": 83, "p_i": [31, 58], "pa": 31, "pack": [1, 103], "packag": [13, 28], "pad": 97, "pad_id": 51, "padding_idx": 46, "page": [34, 35], "paino": [34, 85], "pair": [1, 2, 3, 5, 8, 9, 26, 28, 36, 39, 43, 44, 46, 48, 53, 54, 57, 61, 63, 64, 68, 69, 70, 71, 72, 73, 74, 76, 82, 83], "pairwis": [5, 49, 56, 58, 70, 71, 72, 74, 75], "palm": [51, 52], "pamela": [34, 85], "pan": [34, 85], "panda": 13, "panpan": [34, 85], "paper": [5, 12, 14, 15, 18, 20, 27, 29, 32, 33, 36, 42, 43, 44, 50, 90, 95], "par": 69, "paradigm": [4, 58, 64], "parallel": [1, 41, 51, 54, 82, 93, 97], "paralleliz": 1, "param": [12, 51, 52], "paramet": [1, 2, 3, 4, 5, 14, 27, 28, 37, 42, 44, 47, 48, 49, 50, 51, 52, 53, 56, 57, 59, 60, 61, 62, 63, 64, 67, 72, 73, 87, 90, 93, 94, 95, 96, 97, 98, 99, 100, 104], "parameter": [38, 58, 76], "parametr": 59, "parenthes": 70, "parmar": [34, 85], "pars": [50, 54], "parsed_arg": 10, "parser": 50, "part": [39, 44, 54, 61, 93, 99], "partial": [36, 43], "particip": [16, 41, 42], "particular": [1, 50, 51, 52, 57, 59, 61, 62, 78, 94, 100], "particularli": [13, 64, 80], "partit": [59, 73, 93], "pass": [2, 9, 12, 16, 20, 28, 32, 38, 41, 43, 44, 45, 50, 51, 52, 53, 56, 94, 100, 104], "pass_at_k": 12, "past": [61, 67], "pat_str": 51, "path": [35, 51, 90], "path_to_custom_output": 13, "pattern": [4, 47, 51, 57, 58], "paul": [34, 85], "pavlov": [34, 85], "pbar": 65, "pdf": [14, 18, 20, 27, 29, 32], "pe_": 1, "peak": [8, 26], "pearson": 56, "pebbl": 13, "pei": [34, 85], "peiyi": [34, 85], "penal": [39, 57, 73], "penalti": [5, 49, 56, 57, 62, 63, 83], "peng": [34, 85], "per": [5, 11, 12, 36, 38, 41, 42, 46, 51, 62, 63, 70, 71, 74, 84, 96], "per_device_train_batch_s": 103, "percent": 68, "percentag": [41, 69], "perceptu": 36, "perfect": [50, 74], "perform": [1, 3, 4, 5, 7, 8, 9, 11, 14, 15, 26, 27, 28, 37, 39, 41, 42, 43, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 57, 58, 61, 63, 64, 69, 72, 74, 76, 78, 82, 84, 87, 90, 93, 94, 96, 97], "period": [44, 46, 99], "permit": [8, 26], "permut": [51, 97], "perplex": 94, "person": [5, 68], "perspect": [4, 57, 99], "peter": [34, 85], "petroski": [34, 85], "petrov": [34, 85], "pexpect": 13, "pgr": 39, "phase": [18, 28, 29, 43, 44, 47, 49, 53, 59, 68, 80], "phd": 15, "phenomenon": [39, 56, 57, 80], "phi": [5, 56, 59, 62, 63, 69, 75, 100], "phi4": 21, "philipp": [34, 85], "philosophi": 11, "php": 50, "phrase": 2, "physic": [11, 15], "pi": [1, 5, 36, 49, 59, 61, 63, 67, 72, 73, 76, 83, 90, 94, 95], "pi_": [5, 36, 49, 57, 60, 62, 63, 67, 72, 76, 83, 90], "piao": [34, 85], "pick": [37, 41], "piec": [48, 50, 68, 69, 82], "piecewis": 49, "pii": 5, "pile": 95, "pinto": [34, 85], "pioneer": 47, "pip": [10, 13], "pipelin": [7, 8, 26, 28, 45, 47, 53, 59, 70, 80, 84], "pivot": 37, "pkginfo": 13, "place": [51, 52, 58, 70, 100], "placehold": [8, 26], "plai": [37, 45, 46, 47], "plain": [5, 43], "plan": [47, 95], "plane": 99, "plappert": [34, 85], "platform": [3, 13, 24, 41, 42], "platformdir": 13, "playground": 5, "pleas": [10, 68], "plethora": 50, "plot": [38, 59, 61], "plot_loss": 103, "plt": 59, "plu": 13, "plugin": 13, "pm": 76, "pmatrix": [44, 98, 99], "po": 1, "poetri": 13, "point": [1, 28, 37, 42, 43, 46, 49, 54, 57, 74, 80, 95], "pointwis": [58, 72], "polar": [51, 52, 94, 95], "polici": [5, 45, 46, 47, 49, 50, 53, 59, 61, 63, 67, 68, 69, 72, 76, 80, 83], "polit": 68, "polosukhin": [34, 85], "pond": [34, 85], "pool": [18, 29, 41, 72, 75, 76, 82], "poor": [43, 47, 61, 80, 97], "poorli": 39, "pop": 72, "popular": [7, 39], "portion": [44, 50, 54, 80], "posit": [2, 34, 39, 44, 47, 48, 51, 52, 53, 60, 62, 64, 67, 69, 70, 74, 75, 76, 82, 84, 85, 87, 97, 98, 100], "positionwis": 1, "possess": 74, "possibl": [3, 18, 29, 39, 42, 43, 47, 48, 50, 57, 61, 68, 70, 71, 75, 83, 84], "possibli": [28, 54, 68], "post": 69, "post0": 13, "postpon": 43, "potenti": [4, 5, 31, 43, 46, 49, 54, 57, 58, 64, 69, 80], "pow": [51, 52, 97], "power": [4, 14, 27, 34, 41, 42, 51, 52, 57, 80, 85], "ppo": [5, 21, 22, 45, 49, 57, 63, 70], "ppo_train": 65, "pq": 31, "practic": [1, 4, 28, 36, 39, 43, 54, 56, 59, 61, 73, 76, 93, 95], "practition": [14, 27], "prafulla": [34, 85], "pranav": [34, 85], "pre": [1, 4, 22, 24, 28, 34, 41, 43, 45, 51, 56, 58, 68, 82, 85, 92, 94, 95, 98, 99, 101], "preambl": 69, "preced": [2, 49], "precis": [36, 44, 64], "precomput": 95, "precompute_freqs_ci": [51, 52, 95], "predecessor": [53, 54], "predefin": [14, 27, 67, 71, 80], "predicetd": 58, "predict": [1, 2, 5, 9, 13, 39, 41, 42, 44, 51, 56, 58, 61, 63, 73, 76, 83, 84, 95], "predominantli": 44, "prefer": [5, 7, 21, 22, 34, 36, 37, 45, 46, 47, 53, 54, 56, 60, 61, 63, 68, 70, 71, 72, 74, 75, 80, 83, 84, 85, 95, 97], "prefix": [5, 13, 18, 29, 36, 44, 84], "preliminari": [46, 58, 80], "prepend": [51, 68], "prescrib": 61, "presenc": [35, 54], "present": [5, 43, 47, 50, 61, 63, 64, 67, 68, 69, 75, 84, 94, 95], "preserv": [54, 94, 95], "pressur": 94, "pretrain": [4, 5, 11, 18, 29, 39, 44, 46, 54, 58, 63, 65, 68, 74, 84, 87, 94, 95, 104], "pretrained_weight": 103, "prev_po": 51, "prevent": [1, 44, 47, 67], "preview": 82, "previou": [1, 4, 18, 20, 29, 32, 46, 50, 51, 52, 53, 61, 64, 83, 94, 95], "previous": 1, "primarili": [5, 15, 45, 50, 53, 63, 64], "princip": 93, "principl": [68, 70], "print": [35, 51], "prior": [3, 4, 15, 28, 49, 59, 64, 68, 76], "priorit": [50, 53], "privaci": 68, "prm800k": 84, "pro": [42, 45, 61], "prob": 51, "probabl": [1, 2, 5, 12, 36, 37, 51, 57, 59, 60, 61, 62, 64, 67, 68, 69, 72, 76, 84], "problem": [4, 9, 10, 11, 12, 13, 14, 15, 16, 19, 24, 25, 27, 28, 30, 34, 39, 41, 42, 43, 44, 47, 50, 54, 57, 59, 72, 73, 80, 82, 84, 85], "probs_idx": 51, "probs_sort": 51, "probs_sum": 51, "proce": [20, 32], "procedur": [2, 5, 39, 42, 60, 61, 63, 69, 74], "process": [3, 18, 20, 28, 29, 32, 34, 36, 37, 39, 43, 45, 46, 47, 51, 53, 54, 56, 57, 58, 61, 64, 67, 82, 85, 95, 97, 99, 101], "prod": 12, "produc": [1, 2, 5, 8, 14, 18, 20, 26, 27, 29, 32, 36, 37, 41, 42, 43, 46, 47, 50, 51, 54, 56, 61, 63, 64, 69, 70, 73, 76, 83, 87, 96], "product": [12, 36, 51, 52, 98, 99, 100], "profession": [11, 44], "profici": 46, "program": [12, 16, 24, 25, 28, 41, 42, 43, 44, 45, 47, 50, 54], "programm": [9, 16, 25], "programmat": 12, "progress": [1, 25, 43, 47, 49, 57, 61, 62], "project": [1, 41, 96], "promin": [20, 32], "promis": [3, 4, 37, 54, 69, 82], "promot": [18, 29], "prompt": [5, 7, 8, 10, 11, 12, 14, 18, 26, 27, 28, 29, 41, 42, 43, 44, 45, 46, 47, 49, 50, 51, 53, 56, 57, 61, 63, 65, 67, 68, 70, 71, 72, 73, 74, 75, 76, 80, 87], "prompt_data": 65, "prompt_max_len": 65, "prompt_token": 51, "prompts_dataload": 65, "prone": [19, 73], "prop": 48, "proper": [54, 56, 58], "properli": [34, 49], "properti": [95, 98], "proport": [44, 46, 54, 60, 93], "propos": [1, 18, 29, 36, 39, 44, 51, 52, 54, 56, 57, 58, 59, 62, 64, 72, 75, 76, 82, 83, 90, 93, 96, 100, 104], "proprietari": [44, 53], "propto": 60, "proto": 13, "protobuf": 13, "protocol": [28, 50], "prove": [28, 99], "proven": [45, 62], "provid": [2, 5, 8, 10, 11, 12, 13, 16, 19, 26, 30, 39, 44, 45, 47, 49, 51, 54, 59, 60, 62, 64, 73, 74, 78, 80, 82, 84, 95, 99], "proxim": 62, "prune": 16, "pseudo": 76, "pseudocod": 31, "pseudolabel": 76, "psi": [59, 62, 72, 73], "psi_": 73, "psm": 44, "psychologi": 11, "pth": 51, "ptimiz": 57, "ptx": [5, 63], "ptyprocess": 13, "public": [5, 41, 42, 43, 45, 54, 63], "publicli": [44, 48, 53], "punish": 57, "punit": 57, "pure": 80, "puri": [34, 85], "purpos": [34, 39, 44, 87, 95], "pursu": 15, "push": [1, 14, 27, 64], "put": [8, 18, 26, 29, 36, 41, 80], "puzzl": [39, 47], "py": [10, 103], "py310h06a4308_0": 13, "pyarrow": 13, "pyasn1": 13, "pycpars": 13, "pydant": 13, "pydoc": 28, "pyext": 13, "pypars": 13, "pyplot": 59, "pyproject": 13, "python": [9, 10, 13, 14, 16, 27, 28, 41, 42, 44, 50, 54, 90], "pytorch": 82, "pytz": 13, "pyyaml": 13, "q": [1, 5, 31, 34, 36, 51, 52, 53, 54, 57, 62, 64, 85, 90, 94, 95, 96, 98, 99], "q_": [51, 52, 94, 95], "q_0": [51, 52], "q_1": [51, 52], "q_2": [51, 52], "q_3": [51, 52], "q_a_layernorm": 46, "q_a_proj": 46, "q_b_proj": 46, "q_head_dim": 46, "q_i": [31, 90], "q_lora_rank": 46, "qa": [28, 31, 48, 80], "qihao": [34, 85], "qime": [34, 85], "qin": [34, 85], "qinyu": [34, 85], "qiu": [34, 85], "qiufeng": [34, 85], "qiushi": [34, 85], "qk": 1, "qk_nope_head_dim": 46, "qk_rope_head_dim": 46, "qkv": 53, "qlora": 65, "qpa": 31, "qr": 96, "qu": [34, 85], "quad": [2, 38, 47, 49, 57, 58, 73, 76, 93, 94, 95], "qualiti": [1, 3, 14, 15, 18, 19, 21, 27, 28, 29, 30, 31, 36, 42, 46, 47, 49, 53, 54, 56, 57, 69, 70, 71, 74, 75, 76, 82, 94, 95], "quan": [34, 85], "quantifi": [73, 82], "quantil": 76, "quantiti": [21, 50], "quartil": 50, "queri": [1, 5, 7, 28, 34, 44, 46, 51, 52, 53, 54, 58, 61, 62, 73, 76, 85, 94, 95, 98, 99], "query_1": 50, "query_2": 50, "question": [2, 3, 8, 10, 11, 15, 16, 20, 26, 28, 31, 32, 37, 43, 44, 47, 54, 57, 62, 69, 82, 84, 87, 90, 94, 95], "question_id": 13, "quickli": 57, "quit": [43, 45], "qw_": 1, "qwen": [21, 34, 57, 85], "qwen2": [21, 22, 34, 53, 57, 82, 85], "qy": [34, 53, 54, 85], "r": [1, 5, 9, 20, 32, 34, 36, 44, 46, 47, 49, 50, 51, 56, 57, 60, 61, 62, 67, 72, 73, 83, 85, 90, 93, 94, 96, 98, 99, 104], "r1": [22, 47, 57], "r_": [5, 36, 46, 49, 56, 57, 58, 59, 61, 62, 63, 67, 69, 72, 73, 74, 90, 98, 99], "r_1": [58, 62], "r_h": 49, "r_i": [57, 58, 62, 90], "r_l": 58, "racist": 68, "radford": [34, 85], "rafael": [34, 85], "rafailov": [34, 85], "rai": [34, 85], "rais": [11, 51, 95], "ramesh": [34, 85], "ramp": 94, "ran": 42, "rand_prompt": 65, "randn": [51, 97], "random": [3, 11, 14, 27, 41, 42, 51, 61, 72, 73, 75, 82], "randomli": [9, 11, 14, 20, 27, 32, 41, 44, 54, 68, 70], "rang": [2, 4, 5, 11, 13, 14, 27, 39, 44, 48, 49, 50, 51, 52, 57, 65, 67, 68, 69, 92, 95], "rangl": [51, 52, 94, 95, 99], "rank": [5, 37, 41, 43, 49, 50, 59, 63, 72, 73, 74, 104], "rapidfuzz": 13, "rapidli": 64, "rate": [28, 39, 41, 42, 43, 46, 48, 49, 50, 56, 57, 61, 63, 67, 69, 84, 87], "rater": 61, "rather": [59, 70, 80], "ratio": [47, 54, 60, 67, 94], "rational": 69, "raul": [34, 85], "raw": [45, 101], "re": [43, 51, 52, 59, 72, 94, 95, 99], "reach": [4, 15, 18, 29, 42, 43, 49, 84, 93], "read": [3, 51], "readabl": [50, 80], "readlin": 13, "real": [28, 44, 65, 87, 95, 99], "realist": [8, 15, 26], "realiti": [72, 95], "realiz": 2, "realli": [34, 85], "realm": [14, 20, 27, 32], "realworld": 28, "rearrang": [59, 72], "reason": [4, 11, 12, 20, 22, 32, 34, 36, 37, 43, 44, 46, 47, 50, 53, 57, 60, 62, 69, 75, 84, 85, 95, 97], "recal": [36, 50, 61, 64], "receiv": [3, 36, 49, 61, 93], "recent": [4, 41, 42, 48, 49, 50, 51, 56, 63], "recip": [44, 50, 75], "recommend": [7, 74], "recov": [39, 61], "rectifi": [51, 52, 100], "recurr": 1, "red": [20, 32, 96], "reddit": 3, "reduc": [3, 8, 11, 26, 28, 36, 37, 49, 60, 83, 93, 95, 96, 100, 104], "reduct": [51, 60], "redund": [42, 47, 93], "reevalu": 80, "ref": [59, 60, 62, 83], "refer": [5, 16, 19, 34, 36, 44, 49, 50, 56, 57, 59, 60, 61, 62, 68, 74, 82, 84, 94], "refin": [47, 50, 61], "reflect": [16, 28, 43, 64, 82], "regard": 57, "regardless": [56, 57], "regener": 28, "regex": 51, "regim": 84, "region": [1, 36, 67, 95], "reglu": [51, 52, 100], "regress": [1, 5, 44, 49, 50, 56, 63], "regul": 57, "regular": [34, 50, 53, 61, 62], "regularli": [47, 54], "rehears": 44, "reiichiro": [34, 85], "reinforc": [5, 39, 44, 59, 62, 70, 84, 87], "reject": [21, 22, 44, 47, 49, 50, 54, 56, 71, 73, 75, 76, 90], "rejected_1": 50, "rejected_2": 50, "rel": [1, 5, 37, 41, 45, 46, 47, 51, 52, 53, 57, 60, 68, 80, 87, 95, 98, 99], "relat": [8, 13, 26, 28, 37, 44, 45, 47, 53, 54, 62, 72, 90, 93, 96, 99], "relationship": [34, 85], "releas": [8, 10, 26, 28, 44], "relev": [39, 43, 53, 54, 57, 75, 76, 84], "reli": [14, 27, 36, 39, 42, 43, 47, 76, 82, 97], "reliabl": [9, 11, 15, 39, 46, 47, 53, 80, 84], "relianc": 97, "relu": [1, 48, 51, 52, 59, 100], "remain": [18, 29, 38, 39, 41, 42, 60, 73, 76, 84], "remark": [80, 87], "remind": [8, 26], "remov": [5, 20, 28, 32, 42, 44, 48, 50, 54, 63, 68, 72, 75, 90], "ren": [34, 85], "render": 34, "renorm": 51, "reorder": 44, "repair": 13, "reparameter": 59, "repeat": [8, 18, 26, 29, 43, 57, 82], "repeatedli": 41, "repetit": [3, 57], "replac": [48, 51, 52, 53, 68, 69, 95, 100], "replai": [61, 62, 65], "replay_buff": 65, "repo": [50, 54], "report": [12, 34, 54, 57, 73, 85, 90], "repositori": [13, 28, 41, 44, 45, 54], "repres": [1, 18, 20, 29, 32, 36, 44, 50, 51, 52, 60, 67, 73, 93, 94, 95, 99, 100, 102], "represent": [1, 31, 39, 41, 47, 51, 52, 59, 69, 99, 100], "reproduc": 28, "request": [8, 13, 26, 68, 74], "requir": [1, 2, 4, 8, 9, 26, 39, 42, 43, 44, 47, 49, 50, 51, 61, 72, 80, 93, 94, 95, 96, 97, 98, 99], "rerank": 42, "resampl": [53, 61], "rescal": 95, "research": [15, 28, 49, 64], "reserv": 46, "reserved_special_token_": 51, "reserved_special_token_0": 51, "reserved_special_token_1": 51, "reserved_special_token_2": 51, "reserved_special_token_3": 51, "reserved_special_token_4": 51, "reshap": [51, 52, 95], "reshape_for_broadcast": [51, 52, 95], "residu": [1, 38], "resolv": [41, 56], "resort": 80, "resourc": [43, 49, 54, 82], "respect": [1, 5, 37, 57, 60, 62, 63, 64, 67, 69, 70, 73, 93, 96], "respons": [5, 7, 8, 10, 13, 20, 26, 28, 32, 37, 46, 47, 49, 50, 51, 53, 54, 56, 57, 58, 61, 62, 63, 64, 67, 68, 69, 71, 72, 73, 74, 76, 80, 83, 87, 93], "response_candid": 72, "response_reward": 72, "rest": 35, "restrict": [36, 47, 57, 60, 67], "result": [1, 3, 4, 11, 12, 14, 19, 27, 28, 36, 39, 42, 43, 44, 49, 54, 62, 64, 72, 73, 80, 84, 90, 93, 94, 95, 96, 98, 99], "retain": [28, 44, 47, 50, 54, 68, 76], "retriev": 28, "return": [12, 36, 51, 52, 64, 67, 72, 95, 97], "reus": [53, 80, 95], "reveal": [47, 50], "revers": 69, "review": [53, 59], "revis": [50, 83], "reward": [5, 21, 22, 34, 37, 39, 43, 44, 45, 46, 47, 53, 59, 62, 64, 65, 69, 71, 72, 75, 76, 85], "reward_pretrain": 65, "rewon": [34, 85], "rewrit": 68, "rft": [21, 58, 90], "rho": 90, "rho_": 72, "rich": 28, "right": [1, 5, 12, 36, 38, 44, 51, 52, 56, 57, 59, 60, 62, 63, 64, 67, 69, 72, 83, 84, 94, 95, 96, 99, 101], "rigor": [10, 11, 28, 34, 85], "risk": [47, 54], "rl": [5, 21, 22, 41, 45, 46, 47, 49, 53, 57, 59, 67, 68, 69, 70, 76, 80, 84], "rlaif": [22, 68, 70, 76], "rlcd": [21, 22, 76], "rlhf": [39, 44, 56, 57, 59, 61, 63, 65, 68, 72], "rlhf1": 22, "rlhf2": 22, "rm": [5, 21, 39, 46, 47, 49, 50, 56, 65, 69, 71], "rm_": 46, "rmboost": 21, "rmsnorm": [34, 46, 48, 53, 85], "roberta": 50, "robust": [18, 28, 29, 43, 45, 53, 68], "roform": [34, 85], "role": [37, 45, 46, 47, 51], "rollout": [57, 83], "rollout_batch_s": 65, "room": 43, "rope": [21, 22, 44, 47, 48, 53, 96], "rope_theta": [51, 52], "rotari": [34, 44, 48, 53, 85, 98], "rotat": [44, 98, 99], "roug": [18, 29], "roughli": [38, 42, 48, 73, 84], "round": [20, 32, 42], "rout": [46, 47, 93], "row": [51, 52, 95], "rozi\u00e8r": [34, 85], "rsa": 13, "rsm": [34, 50, 53, 85], "rso": [21, 22], "rsqrt": [51, 52, 97], "rtol": [51, 97], "rtx4090": 65, "ruan": [34, 85], "rui": [34, 85], "ruiqi": [34, 85], "ruizh": [34, 85], "rule": [15, 43, 45, 46, 47, 50, 80], "rulebas": 47, "run": [9, 35, 43, 44, 50, 60, 68, 82, 83], "runji": [34, 85], "runner": 13, "runtim": [13, 42], "runxin": [34, 85], "ruyi": [34, 85], "rwc": [3, 4, 34, 85], "rx": 61, "ryan": [34, 85], "ryder": [34, 85], "s3transfer": 13, "s_": [36, 47, 58, 60, 67, 93, 95], "s_1": [54, 58, 95], "s_2": 95, "s_i": 58, "s_j": 58, "s_n": 54, "s_t": 67, "safe": 63, "safeti": [39, 44, 46, 49], "sahil280114": [8, 26], "sai": 68, "salienc": 39, "salient": 39, "sam": [34, 85], "same": [1, 4, 20, 28, 32, 34, 41, 42, 43, 44, 45, 46, 54, 57, 59, 60, 68, 70, 72, 73, 74, 80, 90, 93, 94, 95, 96, 97], "sampl": [3, 9, 10, 11, 12, 14, 18, 20, 21, 22, 27, 28, 29, 32, 36, 37, 44, 45, 47, 48, 49, 50, 51, 53, 54, 58, 59, 61, 62, 67, 68, 69, 70, 71, 74, 75, 76, 82, 84, 87, 90, 97], "sample_top_p": 51, "sandbox": [53, 54], "sandhini": [34, 85], "sanghai": [34, 85], "sanit": 10, "sastri": [34, 85], "satisfactori": [46, 64], "satisfi": 64, "saunder": [34, 85], "save": [43, 96], "save_path": 65, "save_step": [65, 103], "scail": [67, 82], "scalabl": [2, 15, 54, 58, 64, 67, 69], "scalar": [5, 49, 56, 58, 63, 64], "scale": [2, 4, 18, 19, 28, 29, 30, 34, 39, 49, 51, 52, 53, 54, 57, 59, 64, 67, 80, 85, 87, 95, 96], "scan": 38, "scarciti": 54, "scenario": [13, 28, 54, 57], "schedul": [28, 48], "scheme": [49, 61, 102], "school": 15, "schulman": [34, 85], "scienc": [11, 28, 48], "scientif": 28, "scope": 47, "score": [11, 47, 49, 50, 51, 52, 53, 54, 56, 58, 61, 62, 63, 67, 68, 69, 70, 72, 73, 74, 76, 84, 93, 94, 95], "scorer": 54, "scott": [34, 85], "scrape": [3, 24, 41, 57], "scratch": [21, 73, 94, 95], "script": [10, 103], "scy": [31, 34, 54, 85], "search": [42, 43, 84], "seattl": 92, "second": [1, 3, 8, 9, 26, 28, 36, 42, 46, 47, 54, 61, 69, 83, 100], "secret": [21, 22], "secretli": [34, 85], "secretstorag": 13, "section": [28, 45, 57, 68, 80, 95, 99], "secur": 54, "see": [7, 9, 11, 34, 35, 36, 38, 50, 52, 60, 66, 69, 95], "seed": [8, 14, 18, 26, 27, 28, 29, 54, 74, 75], "seek": [49, 82], "seen": [50, 76], "segment": [15, 49, 101], "select": [7, 9, 20, 28, 32, 37, 41, 42, 43, 44, 47, 49, 50, 51, 56, 57, 61, 63, 84, 87, 90, 93], "selector": 84, "self": [1, 2, 3, 5, 8, 10, 13, 14, 16, 21, 22, 26, 27, 34, 44, 46, 50, 51, 52, 54, 62, 65, 85, 93, 94, 95, 97, 98, 99], "selfattent": 99, "selfinstruct": [18, 29], "semant": [16, 41, 42, 43, 50, 54, 75], "semi": [18, 29, 58], "sen": [34, 85], "send": [38, 61, 93], "sensit": [5, 11, 96, 97], "sent": [28, 47, 93], "sentenc": [1, 2, 3, 8, 26, 28, 48, 69, 101], "separ": [1, 7, 42, 44, 49, 84], "seq_len": [51, 97], "seqlen": [51, 52], "sequenc": [1, 2, 15, 34, 36, 44, 47, 49, 51, 52, 53, 57, 61, 64, 72, 74, 85, 93, 94, 96, 97, 98, 99, 100, 101], "sequenti": [36, 61, 82], "seri": [14, 27, 48, 53, 54, 74, 78, 84], "serv": [28, 34, 37, 47, 58, 61, 73, 93], "servic": 53, "set": [1, 4, 5, 7, 8, 9, 11, 12, 15, 18, 19, 20, 25, 26, 28, 29, 30, 32, 36, 37, 39, 41, 44, 45, 46, 47, 49, 50, 51, 53, 54, 57, 58, 61, 62, 63, 68, 72, 73, 74, 75, 82, 84, 87, 90, 93, 94, 95, 99], "setup": [10, 39], "setuptool": 13, "seventh": [34, 85], "sever": [12, 37, 41, 42, 44, 45, 49, 50, 53, 54, 61, 80, 97], "sexist": 68, "sft": [5, 21, 22, 28, 46, 47, 53, 54, 59, 60, 62, 65, 67, 69, 71, 72, 73, 74, 80, 83, 90], "sh": 65, "sha": [34, 85], "shallow": 56, "shang": [34, 85], "shanghao": [34, 85], "shanghaoran": [34, 85], "shangyan": [34, 85], "shanhuang": [34, 85], "shantanu": [34, 85], "shao": [34, 85], "shaoq": [34, 85], "shape": [38, 51, 52, 95, 97], "share": [1, 5, 25, 46, 47, 80, 96], "sharegpt": 28, "sharma": [34, 85], "shazeer": [34, 85], "shelf": [37, 64, 69], "shellingham": 13, "shen": [34, 85], "shengfeng": [34, 85], "shengguang": [34, 85], "shift": [28, 58, 84], "shiji": [34, 85], "shirong": [34, 85], "shiyu": [34, 85], "short": [9, 16, 47, 95], "shorter": [37, 57], "shot": [4, 5, 11, 18, 29, 34, 68, 69, 74, 76, 84, 85, 90], "should": [4, 8, 9, 15, 26, 35, 38, 39, 42, 60, 64, 68, 70, 72, 96], "show": [1, 4, 5, 11, 34, 35, 36, 44, 48, 50, 59, 60, 64, 68, 72, 73, 74, 78, 80, 84, 95, 96, 99], "shown": [1, 5, 12, 14, 27, 28, 36, 39, 62, 63, 69, 73, 76], "shuai": [34, 85], "shuang": [34, 85], "shuffl": [20, 32], "shuip": [34, 85], "shukai": [34, 85], "shunfeng": [34, 85], "shusheng": [34, 85], "shyam": [34, 85], "sida": [34, 85], "siddhartha": [34, 85], "sidestep": 61, "sigler": [34, 85], "sigma": [5, 49, 51, 52, 59, 60, 63, 69, 72, 73, 100], "sigma_": 67, "sigmoid": [51, 52, 59, 72, 100], "signal": [18, 29, 36, 43, 45, 47, 50, 53, 57, 69, 70, 80], "signatur": [12, 16, 28, 42, 44], "signific": [3, 11, 28, 46, 49, 50, 53, 94, 95, 96, 97], "significantli": [1, 8, 11, 14, 26, 27, 31, 37, 43, 44, 46, 49, 50, 53, 57, 58, 60, 68, 73, 78, 84, 96], "silu": [51, 52], "sim": [5, 36, 38, 49, 57, 58, 59, 60, 62, 63, 72, 73, 76, 83, 100], "simen": [34, 85], "similar": [4, 7, 11, 18, 29, 34, 41, 42, 50, 54, 68, 70, 84, 90, 94], "similarili": [94, 96], "similarli": [1, 42, 47, 60, 74, 76, 80], "simpl": [1, 4, 8, 12, 14, 20, 26, 27, 32, 34, 38, 39, 41, 47, 51, 52, 58, 59, 69, 76, 78, 99, 101], "simpler": [2, 36, 59], "simplest": 82, "simpli": [5, 8, 26, 39, 56, 82, 83, 94], "simplic": [2, 43, 67], "simplifi": [8, 12, 20, 26, 32, 51, 90], "simul": [41, 61, 70], "simultan": [1, 38, 58, 74], "sin": [1, 44, 51, 52, 94, 95, 98, 99], "sinan": [34, 85], "sinc": [1, 2, 36, 39, 42, 45, 51, 52, 54, 59, 60, 62, 68, 69, 72, 93, 95, 96, 97], "sine": 1, "singl": [3, 5, 8, 12, 16, 26, 41, 42, 43, 44, 47, 63, 70, 75, 84, 93], "sinusoid": [1, 99], "site": 25, "situat": 93, "six": [13, 20, 32, 50], "size": [2, 3, 4, 19, 28, 30, 34, 44, 46, 48, 51, 52, 53, 57, 62, 64, 73, 85, 87, 93, 94, 95, 96, 97], "skill": [15, 19, 30, 44, 54, 74], "skywork": 21, "sl": 68, "slama": [34, 85], "slice": 96, "slight": 34, "slightli": [49, 50], "slope": 82, "slow": 97, "slowli": [94, 95], "slp": [34, 47, 53, 85, 94, 96, 98], "small": [1, 9, 18, 29, 34, 39, 41, 43, 44, 50, 54, 67, 74, 80, 82, 94, 95, 96, 97, 99], "smaller": [37, 48, 49, 83, 84, 87, 93, 94, 96, 98], "smallest": [3, 41, 51], "smallscal": 84, "smarter": 39, "smooth": [4, 20, 32], "snapshot": [41, 68], "sniffio": 13, "snippet": [14, 27, 28, 44, 50, 53, 54], "so": [5, 9, 35, 41, 43, 49, 50, 58, 60, 61, 62, 68, 74, 93, 94, 95, 96], "social": [3, 11], "soft": [43, 57, 69, 73], "softmax": [2, 51, 52, 69, 93, 94, 96], "softwar": 28, "solar": [34, 85], "sole": [1, 36, 49, 57, 64], "solid": 96, "solut": [10, 14, 15, 16, 19, 24, 25, 27, 30, 31, 41, 42, 43, 44, 50, 56, 59, 69, 75, 82, 84, 96, 97, 99], "solv": [10, 11, 12, 15, 16, 34, 37, 41, 42, 43, 44, 50, 80, 83, 84, 85], "solvabl": 16, "some": [1, 2, 8, 26, 34, 41, 43, 45, 49, 50, 56, 61, 64, 72, 76, 93], "someon": 68, "someth": 4, "sometim": [4, 28], "song": [34, 85], "sonnet": 82, "sort": [50, 51, 61], "sound": 57, "sourc": [3, 7, 12, 16, 20, 28, 32, 34, 44, 45, 47, 50, 51, 53, 54, 57, 72, 80, 85], "space": [3, 20, 32, 42, 43, 98, 99], "span": [11, 15, 34, 39, 44, 54, 82], "spars": [4, 34, 43, 85], "speak": 72, "spearman": 56, "special": [11, 34, 49, 51, 53, 76, 84, 93], "special_token": 51, "specif": [4, 5, 8, 20, 26, 28, 32, 34, 36, 37, 43, 44, 47, 50, 51, 53, 54, 58, 59, 60, 68, 72, 73, 74, 82, 84, 93, 94, 95], "specifi": [16, 51, 80, 82], "speed": [5, 47, 63], "spent": 45, "sphinx": 34, "split": [5, 24, 25, 42, 44, 51, 52], "split_experience_batch": 65, "spm": 44, "spot": 11, "spread": 94, "spuriou": 4, "sqlite": 13, "sqrt": [1, 51, 52, 73, 94, 95, 96, 97, 99], "squre": 51, "src": 103, "sse": 13, "stabil": [11, 48, 49, 51, 57, 62, 67, 97], "stabl": [10, 12, 37, 53, 67], "stack": [14, 27, 87, 93], "stackexchang": 48, "stage": [2, 44, 46, 47, 49, 50, 53, 54, 62, 73, 80, 82, 87, 94, 95, 103], "stai": 36, "stale": 69, "stand": 34, "standalon": 44, "standard": [2, 15, 16, 18, 25, 29, 37, 38, 41, 44, 45, 50, 51, 53, 54, 60, 61, 62, 64, 67, 69, 70, 73, 84, 87, 93, 102], "star": [28, 83], "starcod": [14, 20, 27, 32, 44], "starcoderdata": [14, 27], "starkli": 11, "start": [5, 18, 20, 29, 32, 34, 35, 37, 42, 43, 44, 51, 59, 61, 63, 72, 74], "start_header_id": 51, "start_po": [51, 52], "starter": 34, "state": [4, 5, 15, 36, 39, 48, 56, 57, 67, 68, 93, 94], "statement": [16, 41, 44], "static": [36, 50, 53], "staticmethod": 51, "statist": 97, "statu": [13, 65], "std": [51, 57, 62, 73, 97], "steadi": 38, "steer": [39, 50, 61, 76], "stefano": [34, 85], "steinhardt": [34, 85], "stem": [11, 53], "step": [1, 5, 15, 18, 20, 21, 22, 28, 29, 32, 34, 36, 38, 42, 43, 44, 46, 47, 48, 49, 50, 57, 62, 63, 65, 68, 69, 75, 78, 80, 85, 87, 94, 95], "stepbi": 84, "steven": [34, 85], "still": [4, 39, 41, 45, 47, 59, 69, 95], "stochast": 2, "stoica": [34, 85], "stop": [38, 49], "stop_token": 51, "store": [34, 54, 72], "str": [51, 72], "straightforward": [2, 7, 15, 36, 50, 64, 72, 80, 90, 94, 95], "straightforwardli": 56, "strateg": [47, 50], "strategi": [28, 37, 44, 46, 47, 49, 53, 56, 57, 58, 65, 76, 83, 94, 96], "stream": 38, "streamlin": [20, 32], "strength": [5, 47, 50, 61, 63, 64], "strict": [18, 24, 29, 51], "strictli": 74, "string": [51, 69, 82, 83], "stringent": 50, "strip": [10, 51], "strong": [34, 46, 47, 56, 64, 73, 80, 82, 84, 85, 87, 94, 95], "stronger": [14, 27, 39, 47, 96], "strongest": 15, "strongli": [68, 87], "structur": [1, 2, 34, 43, 69, 93], "struggl": [4, 37, 73, 80], "stuck": 36, "student": [14, 15, 27, 39, 54], "studi": [4, 18, 29, 38, 39, 53, 61, 64], "style": [11, 44, 50], "su": [34, 85], "sub": [1, 3, 43, 45, 48, 51, 72, 97, 98, 99], "subbiah": [34, 85], "subject": [11, 44, 82], "sublay": 1, "submiss": [41, 42], "submit": [5, 42, 61, 63], "suboptim": [28, 97], "subsequ": [1, 44, 49, 50, 51, 52, 61, 62, 74, 100], "subset": [3, 5, 15, 16, 38], "subspac": 1, "substanti": [4, 8, 20, 26, 32, 39, 62, 64, 83], "substitut": [46, 59, 64, 93], "subtract": 62, "subword": [34, 85, 101], "succ": [5, 59, 72, 73, 76], "succeed": 43, "success": [11, 12, 39, 43, 49, 54, 74, 75], "successfulli": [28, 57], "suchir": [34, 85], "sufeng": [34, 85], "suffer": [47, 57, 67, 80, 83], "suffici": [4, 5, 41, 49, 54, 60, 62, 78, 95], "suffix": 44, "suggest": [4, 37, 54, 59, 60, 64, 69, 87, 100], "suit": 2, "suitabl": [44, 56, 73], "sujoi": [34, 85], "sum": [1, 36, 47, 51, 56, 58, 61, 62, 67, 93, 99], "sum_": [2, 36, 51, 52, 57, 58, 59, 62, 64, 67, 69, 72, 73, 82, 83, 93, 94, 95, 96, 99, 101], "sumit": [34, 85], "summar": 95, "summari": [3, 69], "sun": [34, 85], "sup": 21, "super": [46, 51, 52, 97], "superalign": 39, "superhuman": 39, "superior": [1, 37, 45, 57, 74], "supervis": [3, 4, 5, 15, 18, 29, 36, 39, 44, 54, 59, 70, 71, 72, 74, 76, 82, 87, 98, 99], "supervison": 39, "supervisor": 39, "supplement": 50, "suppli": 90, "support": [35, 44, 46, 50, 54, 65, 102], "suppos": [56, 64, 93], "suppress": [57, 82], "sure": [8, 26, 44, 68], "surfac": [42, 70, 84], "surpass": [4, 15, 20, 32, 54, 71], "surpris": [5, 50, 62], "surprisingli": 60, "surrog": 62, "surround": [44, 46], "suspect": [1, 68], "sutskev": [34, 85], "swaroop": [34, 85], "swiglu": [48, 53], "swish": [21, 51, 52], "symbol": 1, "sympi": 13, "syncheck": 10, "synnaev": [34, 85], "syntact": [41, 50], "syntax": [34, 50, 54], "synthes": [28, 53, 54], "synthesi": [28, 44, 54], "synthet": [14, 27, 50, 53, 54, 71, 75, 76], "system": [4, 15, 28, 34, 47, 50, 51, 57, 61, 80, 82, 85, 94], "systemat": 64, "t": [1, 2, 18, 20, 28, 29, 32, 34, 36, 41, 47, 51, 52, 57, 62, 64, 67, 74, 85, 92, 93, 94, 95, 96, 99], "t1": [51, 97], "t2": [51, 97], "t_": [60, 101], "t_1": 101, "t_2": 101, "t_n": 101, "tabl": [3, 70, 96], "tackl": 42, "taco": 22, "tag": [41, 42, 44, 54, 80], "tail": 54, "tailor": [42, 47, 61], "take": [5, 14, 15, 27, 36, 44, 51, 52, 56, 59, 60, 61, 63, 64, 67, 72, 74, 75, 76, 84, 98, 99, 100], "taken": 36, "talent": 15, "tan": [34, 85], "tang": [34, 85], "tao": [34, 85], "target": [2, 42, 50, 51, 64, 68, 72, 84, 90, 93, 95], "task": [1, 3, 4, 5, 8, 9, 11, 12, 14, 16, 19, 26, 27, 28, 30, 36, 37, 39, 41, 43, 44, 46, 47, 50, 54, 57, 62, 64, 67, 68, 69, 73, 80, 84, 94, 95, 97, 104], "task_id": 10, "taskspecif": 3, "tau": [36, 61, 83], "taught": 22, "td": [62, 67], "teach": [39, 49, 87], "teacher": [14, 27, 28], "team": [5, 63], "technic": [34, 39, 85], "techniqu": [8, 20, 21, 22, 26, 32, 36, 37, 39, 43, 49, 50, 57, 67, 73, 94, 95], "teddi": [34, 85], "telecommun": 102, "tell": [39, 59], "temper": 41, "temperatur": [13, 28, 41, 42, 47, 51, 53, 57, 61, 90, 94], "templat": [14, 18, 20, 27, 28, 29, 32, 103], "tempor": 24, "ten": [4, 11, 44, 49, 54, 95], "tend": [57, 61, 69], "tensor": [51, 52, 95], "term": [2, 5, 12, 43, 45, 47, 49, 53, 57, 58, 59, 62, 64, 84, 95, 99, 100], "termin": [36, 82], "terri": [5, 56, 59, 72, 73], "test": [3, 4, 5, 10, 11, 12, 13, 15, 16, 19, 24, 25, 28, 30, 38, 39, 41, 42, 43, 44, 45, 47, 50, 53, 54, 74, 80, 83, 84, 90], "tester": 50, "text": [1, 2, 3, 4, 5, 8, 12, 16, 26, 34, 35, 36, 38, 44, 45, 47, 49, 51, 52, 54, 56, 57, 58, 59, 60, 61, 62, 63, 64, 67, 69, 72, 73, 74, 76, 82, 83, 84, 87, 90, 93, 94, 95, 96, 100, 101, 102], "text_complet": 51, "textbf": 36, "textbook": 11, "textual": 2, "tezak": [34, 85], "th": [20, 32, 58, 60, 62, 74, 93, 94, 95, 96], "than": [1, 3, 4, 7, 8, 10, 14, 18, 26, 27, 28, 29, 37, 39, 41, 42, 43, 44, 46, 48, 50, 56, 57, 59, 60, 68, 69, 70, 72, 73, 84, 87, 90, 93, 94, 95, 96], "thank": 64, "thei": [3, 5, 14, 27, 34, 39, 41, 44, 49, 50, 73, 74, 75, 82, 84, 93, 96, 97, 100, 102], "them": [2, 5, 18, 29, 39, 42, 43, 45, 46, 50, 53, 57, 70, 72, 75, 80, 93, 96, 97, 99], "themselv": [5, 39, 58, 63], "theoret": [28, 60], "therebi": [47, 57, 93], "therefor": [14, 18, 27, 29, 42, 45, 46, 47, 49, 57, 60, 62, 64, 73, 84, 95, 96], "theta": [2, 5, 36, 44, 49, 51, 52, 56, 57, 58, 59, 60, 61, 62, 63, 67, 76, 83, 94, 95, 98, 99], "theta_": [44, 51, 52, 57, 60, 62, 67, 94, 95, 98, 99], "theta_0": [51, 52, 95], "theta_1": [51, 52, 95], "theta_d": 94, "theta_j": [51, 52], "thi": [1, 2, 3, 4, 5, 8, 11, 12, 14, 15, 18, 19, 20, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 39, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 53, 54, 56, 57, 58, 59, 60, 61, 62, 63, 64, 66, 67, 68, 72, 73, 74, 75, 76, 80, 82, 83, 84, 90, 93, 94, 95, 96, 97, 99], "thing": [35, 68], "think": [68, 80, 82, 93, 95], "third": [1, 9], "thirti": [34, 85], "thompson": 61, "thorough": [46, 50], "thoroughli": 47, "thorp": [34, 85], "those": [1, 5, 7, 34, 37, 41, 45, 47, 49, 50, 53, 57, 61, 75, 84, 93], "though": 12, "thought": [11, 47, 50, 68, 69, 90], "thousand": [4, 41, 42, 44, 49, 50, 95], "three": [4, 5, 12, 13, 16, 28, 36, 39, 44, 50, 54, 58, 59, 63, 64, 69, 72, 73, 82, 84, 96, 100], "threshold": [50, 51, 71, 76], "through": [2, 11, 14, 20, 27, 32, 50, 51, 52, 53, 54, 56, 57, 58, 64, 67, 69, 72, 76, 80, 82, 87, 94, 95, 96, 98, 99, 100], "throughout": [50, 80], "thu": [20, 32, 36, 38, 39, 49, 51, 52, 56, 57, 61, 67, 70, 72, 74, 83, 94], "tian": [34, 85], "tianhang": [34, 85], "tianhao": [34, 85], "tianjian": [34, 85], "tianjun": [34, 85], "tianyi": [34, 85], "tianyu": [34, 85], "tier": 28, "tild": [49, 61, 62], "tillet": [34, 85], "time": [1, 7, 13, 20, 32, 36, 41, 44, 49, 50, 56, 57, 69, 76, 80, 83, 84, 93, 96, 97, 104], "timeout": [13, 54], "tingyu": [34, 85], "tini": 4, "tip": 53, "titl": [59, 102], "tk": 13, "tl": 3, "to_remov": 72, "todai": 39, "togeth": [1, 18, 29, 41, 42, 50, 68, 69], "tok": 51, "tok_embed": [51, 52], "token": [1, 2, 3, 5, 13, 21, 22, 36, 41, 44, 45, 46, 48, 49, 50, 52, 54, 56, 60, 62, 63, 64, 69, 82, 83, 84, 87, 92, 94, 95, 96, 97, 98, 99, 101], "token1": 93, "token2": 93, "token3": 93, "token_logprob": 51, "tokenization\u4e4b\u540e": 92, "tokenization\u7b97\u6cd5\u4e4b\u4e00": 92, "tokenize\u7684\u610f\u601d\u662f\u628a\u4e00\u4e2a\u53e5\u5b50\u6216\u957f\u8bed\u6599\u5206\u6210token": 92, "token\u53ef\u4ee5\u7406\u89e3\u4e3a\u4e00\u4e2a\u7b26\u53f7": 92, "token\u6570": 92, "token\u66ff\u6362\u5b83\u4eec": 92, "toler": 84, "tolist": 51, "tom": [34, 85], "tomli": 13, "tomlkit": 13, "tone": 50, "tong": [34, 85], "tongliang": [34, 85], "tongzheng": [34, 85], "too": [39, 54, 57, 67, 95], "took": 82, "tool": [7, 15, 34, 41, 50, 101], "toolbelt": 13, "top": [3, 14, 27, 28, 35, 37, 41, 47, 49, 50, 51, 52, 56, 58, 73, 76, 87, 93], "top_p": 51, "topic": [19, 30, 46, 50], "topk": [47, 93], "topp": [13, 57], "torch": [13, 51, 52, 93, 95, 97], "toreproduc": 7, "total": [12, 14, 18, 25, 27, 29, 38, 42, 45, 46, 47, 49, 62, 82, 93, 96], "total_len": 51, "toutanova": [34, 85], "toward": [18, 29, 34, 37, 70, 76, 83, 85, 93], "toxic": [5, 68], "tqdm": 13, "trace": [82, 83], "traceback": 51, "track": 72, "tractabl": 39, "trade": 57, "tradeoff": 64, "tradit": [4, 11], "train": [1, 5, 8, 14, 15, 18, 19, 24, 25, 26, 27, 29, 30, 34, 36, 37, 39, 41, 42, 44, 45, 49, 51, 56, 58, 59, 60, 62, 63, 64, 65, 67, 68, 69, 70, 71, 72, 73, 82, 84, 85, 93, 94, 95, 96, 97, 101], "train_bash": 103, "train_batch_s": 65, "train_ppo": 65, "train_ppo_llama": 65, "trainabl": 104, "trainer": 65, "trainin": 45, "training_step_actor": 65, "training_step_crit": 65, "trajectori": [36, 58, 67, 80], "transduct": 1, "transfer": [98, 99], "transform": [1, 3, 4, 34, 41, 44, 46, 47, 48, 49, 50, 53, 57, 85, 94, 95, 96, 97, 98, 99, 100, 104], "transformerblock": [2, 51, 52], "transit": [36, 83], "translat": [1, 3, 34, 50, 80, 85], "transmit": 61, "transpos": [51, 52], "trap": 83, "treat": [35, 51, 53, 73, 95], "tremend": 46, "trend": 38, "tri": [74, 82], "trick": [43, 65, 94], "trigonometr": [51, 52, 94, 95], "trigonometri": 50, "trillion": [45, 47, 53, 54], "trim": [42, 87], "triplet": [2, 44, 64, 72], "triton": 13, "triu": [51, 52], "trivial": 57, "trl": 65, "troubl": 68, "trough": 44, "trove": 13, "true": [49, 51, 52, 65, 97, 103], "truncat": 57, "trust": 67, "truth": [15, 16, 25, 39, 45, 46, 47, 50, 53, 56, 57, 58, 59, 72, 75], "try": [7, 8, 26, 43, 51, 54, 60, 74, 90], "trylimit": 43, "tu": [34, 85], "tune": [4, 5, 7, 8, 18, 20, 26, 29, 32, 34, 50, 54, 59, 60, 62, 64, 65, 70, 71, 72, 74, 83, 84, 85, 87, 90, 94, 95], "tupl": [51, 52, 95], "turbo": [7, 14, 27, 45, 51], "turn": [44, 50, 51, 84, 95], "tutori": 54, "two": [1, 2, 9, 18, 20, 29, 32, 34, 35, 36, 37, 39, 41, 42, 43, 44, 46, 47, 49, 50, 51, 52, 53, 56, 57, 58, 59, 60, 61, 64, 68, 69, 70, 72, 73, 74, 75, 76, 80, 82, 83, 84, 87, 93, 94, 95, 96, 100], "tworek": [34, 85], "tx": 61, "txt": [8, 26], "type": [7, 8, 13, 18, 20, 26, 28, 29, 32, 39, 41, 44, 47, 48, 51, 52, 58, 64, 68, 69, 72, 73, 80, 95], "type_a": [51, 52, 95, 97], "typeddict": 51, "typescript": 50, "typic": [3, 4, 37, 39, 48, 50, 57, 60, 61, 62, 64, 68, 70, 76, 93, 94, 95, 98, 99], "tzdata": 13, "u": [2, 3, 4, 9, 31, 41, 42, 47, 59, 64, 72, 73, 75, 84, 93, 96], "u_": [2, 93], "u_1": 2, "u_i": 2, "u_n": 2, "uation": 9, "uiuc": 10, "uk": 96, "ultim": [20, 32, 39, 50, 93], "ultrafeedback": 56, "unambigu": 16, "unbalanc": 93, "unbias": [12, 51, 62, 97], "uncertainti": 61, "unchang": 83, "unclear": [39, 83], "uncur": 75, "under": [11, 14, 27, 36, 37, 46, 59, 60, 67], "underli": 95, "underload": 47, "underset": [49, 56, 61, 62, 76, 83], "understand": [2, 11, 31, 34, 35, 39, 43, 44, 50, 54, 85, 90], "undesir": [36, 57], "unembed": [5, 46, 63], "uneth": 68, "unexpect": [57, 80], "unhealthi": 57, "unicod": [34, 54, 85], "unicode\u548cutf": 102, "unicode\u5f53\u524d\u9ed8\u8ba4\u7684\u7248\u672c\u662fuc": 102, "unicode\u662fascii": 102, "unicode\u7684\u5b57\u7b26\u96c6\u8303\u56f4": 102, "unicode\u76f4\u63a5\u517c\u5bb9\u4e86\u521d\u671fascii": 102, "unifi": [20, 32], "uniform": [44, 72, 84, 94, 97], "uniformli": [3, 61, 82, 84], "union": 51, "uniqu": [43, 44, 46], "unit": [12, 15, 28, 44, 49, 50, 53, 54, 94, 95], "univers": [31, 34, 85, 95], "unk": 72, "unknown": [59, 72], "unlabel": [2, 76], "unleash": [31, 54], "unlik": [15, 28, 57, 94], "unlikelihood": 64, "unlimit": 3, "unlock": [44, 46], "unmerg": 65, "unpack": [21, 22], "unsatisfactori": 64, "unsupervis": [3, 34, 39, 54, 85], "unsur": 49, "until": [18, 29, 42, 43, 50, 61, 72, 93], "untruth": 5, "unveil": [20, 32], "up": [2, 4, 5, 8, 18, 19, 26, 28, 29, 30, 42, 49, 50, 57, 61, 63, 64, 68, 82, 84, 87, 94, 95], "up_proj": 46, "updat": [38, 42, 47, 54, 57, 65, 94], "upgrad": [10, 20, 32], "uplift": 57, "upon": [37, 47, 53, 54], "upper": [37, 57, 95], "upweight": 36, "uq": 96, "uritempl": 13, "url": [34, 85], "urllib3": 13, "us": [1, 2, 3, 4, 5, 8, 9, 12, 13, 14, 15, 18, 20, 26, 27, 28, 29, 32, 34, 35, 36, 37, 38, 39, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 56, 57, 58, 59, 61, 62, 63, 64, 65, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 80, 82, 84, 87, 90, 94, 95, 96, 97, 98, 99, 100], "usag": [20, 28, 32, 50], "use_fast_token": 103, "user": [5, 7, 8, 26, 28, 39, 44, 49, 50, 51, 63, 73, 74, 75, 87], "usual": [1, 18, 29, 50, 57, 59, 62, 72, 75, 82, 93, 96, 98, 99], "uszkoreit": [34, 85], "ut": 64, "util": [11, 20, 32, 43, 44, 47, 49, 50, 53, 54, 57, 59, 68, 73, 80], "uv": 96, "uw_": 2, "v": [1, 36, 51, 52, 56, 64, 67, 70, 96, 98, 99, 100], "v0": [7, 10, 65], "v1": [14, 27, 49], "v2": [21, 22, 34, 47, 49, 85, 93, 96], "v3": [22, 49, 80], "v5": [44, 49], "v_": [56, 62], "v_head_dim": 46, "valid": [4, 5, 15, 18, 19, 24, 28, 29, 41, 43, 47, 53, 54, 57, 59, 63, 73, 76, 82], "valu": [1, 5, 15, 28, 38, 41, 47, 51, 52, 54, 60, 62, 63, 64, 67, 69, 72, 76, 93, 94, 95, 99], "valuabl": [98, 99], "valueerror": 51, "vanish": 61, "var": [51, 97], "vare": 38, "vari": [3, 11, 20, 32, 38, 42, 46, 48, 93, 94], "variabl": [54, 97, 98, 99], "varianc": [12, 36, 49, 53, 61, 62, 67, 97], "variant": [2, 44, 51, 52, 67, 74], "variat": [11, 34, 41, 51, 52, 70, 100], "varieti": [38, 50], "variou": [9, 14, 18, 20, 27, 29, 32, 38, 45, 48, 50, 53, 54, 68, 74, 94, 95], "vast": 54, "vaswani": [34, 85], "vdot": [98, 99], "ve": 51, "vector": [1, 2, 15, 44, 51, 52, 56, 60, 61, 94, 95, 96, 97, 99, 100], "vedant": [34, 85], "verb": [8, 26], "verbos": 56, "verdict": 75, "veri": [5, 12, 38, 39, 41, 54, 63, 68, 70, 94, 95], "verif": [50, 54, 80], "verifi": [7, 15, 16, 34, 47, 57, 80, 83, 85], "versatil": [34, 85], "version": [1, 5, 11, 14, 27, 28, 42, 44, 46, 49, 50, 51, 52, 59, 75, 80, 90, 100, 103], "versu": [67, 90], "veryeasyhack": 68, "via": [5, 28, 34, 37, 43, 44, 50, 56, 57, 59, 67, 71, 75, 78, 80, 85], "view": [51, 52, 95], "view_as_complex": [51, 52, 95], "view_as_r": [51, 52, 95], "vineet": [34, 85], "violat": 70, "virtualenv": 13, "visual": [8, 26], "vllm": 10, "vocab": 60, "vocab_s": [46, 51, 52], "vocabulari": [46, 53, 60], "voss": [34, 85], "vote": [58, 82, 84], "vsp": [1, 2, 34, 53, 85, 96], "vw_": 1, "w": [1, 5, 34, 51, 52, 56, 59, 60, 63, 72, 74, 75, 85, 92, 96, 98, 99, 100], "w1": [51, 52], "w2": [51, 52, 100], "w3": [51, 52], "w_": [1, 2, 36, 51, 52, 99, 100, 104], "w_1": 54, "w_1s_1": 54, "w_2": 1, "w_e": 2, "w_n": 54, "w_ns_n": 54, "w_p": 2, "w_y": 2, "wa": [2, 3, 9, 16, 20, 28, 32, 41, 42, 44, 45, 47, 49, 54, 74, 83], "wai": [12, 15, 18, 29, 36, 37, 39, 43, 54, 57, 68, 72, 90, 93, 94, 95], "wainwright": [34, 85], "wait": 82, "waitlist": 5, "wake": [8, 26], "wan": [34, 85], "wang": [34, 85], "wangd": [34, 85], "want": [5, 18, 29, 36, 63, 72, 90, 94, 95], "warm": 57, "warmup": [28, 48, 87], "wast": 41, "wastag": 93, "wavecod": [34, 54, 85], "wavelength": [1, 94], "wcg19": [34, 53, 85], "we": [1, 2, 3, 4, 5, 7, 8, 9, 11, 12, 14, 15, 16, 18, 20, 24, 26, 27, 28, 29, 31, 32, 36, 37, 38, 39, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 56, 57, 58, 59, 60, 61, 62, 63, 64, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 78, 80, 82, 83, 84, 87, 90, 93, 94, 95, 96, 98, 99, 100, 104], "weak": [21, 64, 95], "weaker": [14, 27], "weakli": [38, 39], "web": [3, 45, 57, 72], "webpag": [3, 63], "websit": [28, 48, 53, 54, 57], "webtext": 3, "webtext2": 38, "wei": [34, 85], "weigh": 59, "weight": [1, 2, 28, 36, 41, 46, 48, 49, 51, 52, 54, 57, 59, 64, 69, 87, 93, 95, 97, 99, 100, 104], "welind": [34, 85], "well": [1, 2, 7, 36, 53, 60, 64, 68, 94, 95], "wellcalibr": 68, "wen": [34, 85], "wenbin": [34, 85], "wenfeng": [34, 85], "wenji": [34, 85], "wenjun": [34, 85], "wentao": [34, 85], "wenxiang": [34, 85], "were": [12, 47, 49, 84], "west": [21, 22], "what": [8, 26, 39, 45, 48, 61, 95], "wheel": 13, "when": [1, 3, 5, 8, 15, 18, 26, 28, 29, 34, 35, 36, 38, 39, 42, 43, 50, 51, 52, 56, 57, 60, 61, 62, 64, 67, 70, 73, 75, 82, 83, 84, 90, 93, 97, 99, 100], "where": [1, 2, 5, 12, 15, 18, 20, 25, 29, 31, 32, 36, 37, 38, 43, 44, 46, 47, 49, 50, 51, 52, 53, 54, 56, 57, 58, 59, 60, 61, 62, 63, 64, 67, 69, 72, 73, 74, 75, 78, 82, 90, 93, 94, 95, 96, 97, 98, 99, 100, 104], "wherea": [34, 36, 41], "wherebi": 49, "wherein": 64, "wherev": 47, "whether": [18, 29, 34, 39, 41, 45, 47, 51, 54, 64, 67, 69, 75, 80, 83, 84, 93], "which": [1, 2, 3, 4, 5, 8, 9, 11, 12, 14, 15, 16, 18, 20, 26, 27, 29, 32, 35, 36, 37, 38, 41, 42, 43, 44, 45, 46, 47, 49, 50, 51, 52, 53, 54, 56, 57, 58, 59, 60, 61, 62, 63, 67, 68, 69, 70, 72, 73, 74, 75, 76, 80, 84, 90, 93, 94, 95, 97, 98, 99, 100, 104], "while": [1, 3, 4, 5, 14, 15, 27, 36, 38, 42, 43, 44, 46, 47, 50, 53, 54, 60, 61, 62, 67, 68, 72, 73, 74, 83, 87, 90, 93, 94, 95, 99, 101], "white": 12, "whiten": 49, "who": 15, "whole": 64, "whose": [5, 51, 54], "why": [39, 43], "wide": [2, 5, 11, 38, 39, 42, 50, 56, 62, 73], "widespread": [34, 85], "width": 51, "wifi": 68, "wiki": 48, "wikihow": 87, "wikipedia": [3, 48], "wildchat": 28, "william": [34, 85], "win": [61, 69, 72, 74, 75], "window": [2, 47, 50], "winner": 75, "winter": [34, 85], "wise": [2, 46, 47, 51, 52, 68, 82, 95, 97, 100], "within": [47, 50, 54, 57, 67, 68, 75, 76, 80, 94, 95, 97], "without": [3, 9, 36, 39, 54, 56, 57, 59, 60, 61, 63, 68, 75, 80, 84, 87, 99], "wizard": [21, 22], "wk": [51, 52], "wo": [51, 52], "wojciech": [34, 85], "word": [2, 3, 5, 7, 8, 15, 26, 34, 54, 57, 70, 85, 92, 98, 99, 101], "wordpiece\u6bcf\u6b21\u9009\u62e9\u5408\u5e76\u7684\u4e24\u4e2a\u5b50\u8bcd": 101, "wordpiece\u7b97\u6cd5\u4e5f\u662f\u6bcf\u6b21\u4ece\u8bcd\u8868\u4e2d\u9009\u51fa\u4e24\u4e2a\u5b50\u8bcd\u5408\u5e76\u6210\u65b0\u7684\u5b50\u8bcd": 101, "work": [1, 3, 4, 14, 15, 18, 27, 29, 31, 36, 39, 46, 48, 50, 51, 52, 58, 59, 60, 68, 75, 95, 100], "world": [8, 11, 26], "world_siz": 65, "wors": [72, 90], "worst": [37, 76], "would": [1, 36, 39, 49, 56, 68, 75, 84], "wq": [51, 52], "write": [5, 7, 8, 16, 26, 34, 35, 44, 46, 47, 49, 63, 80, 87, 94, 99], "writer": 15, "written": [5, 12, 15, 18, 25, 29, 34, 35, 68], "wrong": [43, 60, 84, 90], "wrote": [8, 26, 68], "wu": [34, 85], "wv": [51, 52], "wwl": [28, 34, 50, 54, 85], "wx": 104, "x": [1, 2, 5, 9, 34, 36, 44, 49, 51, 52, 56, 58, 59, 60, 61, 62, 63, 64, 67, 69, 71, 72, 73, 75, 76, 83, 85, 92, 94, 95, 97, 98, 99, 100, 101, 104], "x_": [18, 29, 51, 52, 75, 83, 94, 95, 99], "x_0": [51, 52, 94, 95], "x_1": [1, 51, 52, 64, 71, 83, 94, 95], "x_2": [51, 52, 83], "x_i": [74, 75, 83], "x_m": 64, "x_n": [1, 71], "xdxac": 92, "xia": [34, 85], "xiangyu": [34, 85], "xianji": [34, 85], "xianzu": [34, 85], "xiao": [34, 85], "xiaodong": [34, 85], "xiaohan": [34, 85], "xiaohuan": [34, 85], "xiaojin": [34, 85], "xiaokang": [34, 85], "xiaosha": [34, 85], "xiaotao": [34, 85], "xiaowen": [34, 85], "xiaoxiang": [34, 85], "xie": [34, 85], "xin": [34, 85], "xingkai": [34, 85], "xingxuan": [34, 85], "xingzhang": [34, 85], "xinnan": [34, 85], "xinyi": [34, 85], "xinyu": [34, 85], "xiong": [34, 85], "xk": [51, 52, 95], "xk_": [51, 52, 95], "xk_out": [51, 52, 95], "xp": 100, "xq": [51, 52, 95], "xq_": [51, 52, 95], "xq_out": [51, 52, 95], "xu": [34, 85], "xuan": [34, 85], "xuancheng": [34, 85], "xue": [34, 85], "xuecheng": [34, 85], "xv": [51, 52, 100], "xw": [51, 52, 100], "xw_": [51, 52, 100], "xw_1": 1, "xx": [10, 22], "xxhash": 13, "xxx": 13, "xz": 13, "x\u4e3a\u5b57\u7b26\u7684unicode\u4e8c\u8fdb\u5236": 102, "y": [2, 5, 34, 36, 49, 51, 52, 56, 57, 59, 60, 61, 63, 64, 67, 69, 71, 72, 73, 75, 83, 85, 92, 94, 97, 101], "y1": 59, "y2": 59, "y3": 59, "y_": [5, 18, 29, 36, 49, 56, 58, 59, 60, 63, 64, 71, 72, 73, 74, 75, 76, 83], "y_1": [1, 5, 59, 64, 69, 73, 76, 83], "y_2": [5, 59, 69, 73, 76, 83], "y_c": 73, "y_i": [58, 83], "y_l": 60, "y_m": 1, "y_n": 64, "y_r": 73, "y_t": 64, "y_w": 60, "yaml": [13, 43], "yan": [34, 85], "yang": [34, 85], "yangyu": [34, 85], "yanhong": [34, 85], "yann": [34, 85], "yanp": [34, 85], "yao": [34, 85], "yaofeng": [34, 85], "yaohui": [34, 85], "yarl": 13, "yarn": 47, "ye": [34, 85], "yellow": 12, "yi": [34, 85], "yibo": [34, 85], "yichang": [34, 85], "yichao": [34, 85], "yield": [1, 5, 16, 50, 61, 62, 73, 104], "yifeng": [34, 85], "yiliang": [34, 85], "yilong": [34, 85], "yin": [34, 85], "ying": [34, 85], "yishi": [34, 85], "yishuji": [34, 85], "yixin": [34, 85], "yixuan": [34, 85], "yiyuan": [34, 85], "yml": 13, "yongji": [34, 85], "yongqiang": [34, 85], "you": [7, 8, 13, 26, 34, 35, 44, 49, 67, 68, 73, 85, 90], "young": 15, "your": [7, 13, 34, 35, 44, 68, 85], "yu": [34, 85], "yuan": [34, 85], "yuchen": [34, 85], "yuduan": [34, 85], "yuheng": [34, 85], "yukun": [34, 85], "yuliang": 22, "yunfei": [34, 85], "yunfeng": [34, 85], "yunlong": [34, 85], "yunxian": [34, 85], "yuqiong": [34, 85], "yura": [34, 85], "yuri": [34, 85], "yute": [34, 85], "yuwei": [34, 85], "yuxiang": [34, 85], "yuxuan": [34, 85], "yuyao": [34, 85], "yyl": [34, 85, 90], "yz": [34, 54, 85], "yzh": [34, 54, 85], "z": [1, 34, 59, 61, 67, 72, 85, 92, 101], "z_": 72, "z_1": 1, "z_n": 1, "zabdzabac": 92, "zaremba": [34, 85], "zehui": [34, 85], "zekun": [34, 85], "zemlyanskii": [34, 85], "zeng": [34, 85], "zero": [4, 11, 36, 49, 51, 52, 90, 99], "zero_stag": 65, "zeros_lik": 51, "zeyu": [34, 85], "zh": 45, "zha": [34, 85], "zhang": [34, 85], "zhangli": [34, 85], "zhao": [34, 85], "zhaojian": [34, 85], "zhe": [34, 85], "zhen": [34, 85], "zhenda": [34, 85], "zheng": [34, 85], "zhenru": [34, 85], "zhewen": [34, 85], "zhihong": [34, 85], "zhiniu": [34, 85], "zhipeng": [34, 85], "zhongyu": [34, 85], "zhou": [34, 85], "zhoujun": [34, 85], "zhu": [34, 85], "zhuoshu": [34, 85], "ziegler": [34, 85], "zihan": [34, 85], "zihui": [34, 85], "zilin": [34, 85], "zip": [51, 72], "zipp": 13, "ziwei": [34, 85], "zixuan": [34, 85], "zlib": 13, "zlm": [7, 34, 85], "zou": [34, 85], "zy": 92, "zydzyac": 92, "\u4e00": 102, "\u4e00\u4e2a\u5728\u5f00\u5934": 92, "\u4e00\u822c\u4e0d\u76f4\u63a5\u4f7f\u7528unicod": 102, "\u4e00\u90e8\u5206\u6b63\u8d1f\u62b5\u6d88\u4e00\u90e8\u5206\u632f\u8361": 94, "\u4e00\u95e8\u8bed\u8a00\u4e2d": 92, "\u4e0a\u9762\u4ecb\u7ecd\u4e86\u6587\u5b57\u5b57\u7b26\u6620\u5c04\u4e3aunicode\u5b57\u7b26\u96c6": 102, "\u4e0b\u4e00\u4e2a\u5e38\u89c1\u7684\u5b57\u8282\u5bf9\u662f": 92, "\u4e0b\u8f7d": 10, "\u4e0b\u8f7d\u8bc4\u4f30\u6587\u4ef6": 13, "\u4e0b\u9762\u4e3e\u4e2a\u4f8b\u5b50": 92, "\u4e0b\u9762\u7684\u793a\u4f8b\u5c06\u89e3\u91ca": 92, "\u4e0d\u4f1a\u51fa\u73b0\u53ea\u5b66\u90a3\u4e9b\u7b80\u5355\u4e14": 62, "\u4e0d\u4f1a\u5355\u72ec\u51fa\u73b0": 92, "\u4e0d\u8868\u793a\u5176\u4ed6\u8bed\u8a00": 102, "\u4e0d\u8db3\u7684\u5728\u9ad8\u4f4d\u75280\u8865\u5145": 102, "\u4e0ebpe\u7684\u6700\u5927\u533a\u522b\u5728\u4e8e": 101, "\u4e0ebpe\u7b97\u6cd5\u7c7b\u4f3c": 101, "\u4e14\u5047\u8bbe\u5404\u4e2a\u5b50\u8bcd\u4e4b\u95f4\u662f\u72ec\u7acb\u5b58\u5728\u7684": 101, "\u4e24\u4e2a\u5b57\u6bb5": 10, "\u4e2a": 92, "\u4e2a\u4e0d\u540c\u7684token": 92, "\u4e2a\u5355\u8bcd": 92, "\u4e2a\u5b50\u8bcd\u7ec4\u6210": 101, "\u4e2d": 10, "\u4e2d\u4f7f\u7528\u4e86\u4e0a\u8ff0\u7b97\u6cd5\u7684\u4e00\u4e2a\u53d8\u4f53": 92, "\u4e2d\u51cf\u5c11\u8ba1\u6570": 92, "\u4e2d\u5b58\u5728": 92, "\u4e2d\u62bd\u53d6\u51fa\u6765": 13, "\u4e2d\u6587": 102, "\u4e2d\u7684": 10, "\u4e2d\u76f8\u5bf9\u597d\u7684": 62, "\u4e2d\u88ab\u9996\u6b21\u63d0\u51fa": 92, "\u4e3a\u4e86\u4ee5\u6700\u6709\u6548\u7684\u65b9\u5f0f\u6784\u5efa\u8bed\u6599\u5e93": 92, "\u4e3a\u4e86\u5408\u5e76": 92, "\u4e3a\u4e86\u66f4\u6e05\u6670\u7684\u7406\u89e3": 92, "\u4e3a\u4e86\u672c\u6587\u7684\u7b80\u5355\u8d77\u89c1": 92, "\u4e3a\u4e86\u8282\u7701\u7a7a\u95f4": 102, "\u4e3a\u4e86\u89e3\u51b3": 102, "\u4e3a\u4ec0\u4e48\u4e0d\u76f4\u63a5\u8c03\u7528": 10, "\u4e3a\u53e5\u5b50\u7684\u4e00\u4e2a\u5206\u8bcd\u7ed3\u679c": 101, "\u4e3a\u8865\u5145": 102, "\u4e3e\u4f8b1": 102, "\u4e3e\u4f8b2": 102, "\u4e4b\u524d": 102, "\u4e5f\u53eb\u5b57\u7b26\u96c6": 102, "\u4e5f\u5c31\u662f\u4e24\u5b50\u8bcd\u5728\u8bed\u8a00\u6a21\u578b\u4e0a\u5177\u6709\u8f83\u5f3a\u7684\u5173\u8054\u6027": 101, "\u4e5f\u5c31\u662f\u628a\u6bcf\u4e00\u4e2a\u5355\u8bcd\u770b\u6210\u4e00\u4e2atoken": 92, "\u4e5f\u5c31\u662f\u7528\u5b9a\u957f2\u4e2a\u5b57\u8282\u6765\u8868\u793a\u5b57\u7b26": 102, "\u4e5f\u662fnlp\u4e2d\u6700\u91cd\u8981\u7684\u7f16\u7801\u65b9\u5f0f\u4e4b\u4e00": 92, "\u4e5f\u80fd\u88ab\u7b97\u4f5c\u5b57\u7b26\u5bf9\u7684\u4e00\u90e8\u5206": 92, "\u4e8c\u8fdb\u5236\u5c31\u662f01000001": 102, "\u4e8c\u8fdb\u5236\u5c31\u662f110000": 102, "\u4eca\u5929\u5c31\u6765\u4e86\u89e3\u4e0b\u5b57\u7b26\u5728\u8ba1\u7b97\u673a\u4e2d\u7684\u7f16\u7801\u65b9\u5f0f": 102, "\u4ece": 13, "\u4ece\u6700\u957f\u5230\u6700\u77ed": 92, "\u4ece\u800c\u6211\u4eec\u77e5\u9053\u5269\u4f59\u7684": 92, "\u4ed6\u4eec\u5177\u6709\u6700\u5927\u7684\u4e92\u4fe1\u606f\u503c": 101, "\u4ee5\u4e2d\u6587": 102, "\u4f1a\u4e0d\u4f1a\u4f7f\u5f97\u8bad\u7ec3\u53d8\u5f97\u5f88\u6162": 62, "\u4f1a\u4e0d\u4f1a\u66f4\u597d": 62, "\u4f20\u5165\u89e3\u6790\u540e\u7684\u53c2\u6570": 10, "\u4f3c\u7136\u503c\u7684\u53d8\u5316\u53ef\u8868\u793a\u4e3a": 101, "\u4f46\u4e00\u4e2a\u8bcd\u5728\u7ed3\u5c3e\u6709\u4e00\u4e2a": 92, "\u4f46\u4ecd\u6709\u53ef\u80fd\u51fa\u73b0\u4e0d\u5728\u91cc\u9762\u7684\u5355\u8bcd": 92, "\u4f46\u5b83\u5177\u6709\u826f\u597d\u7684\u6027\u80fd": 92, "\u4f46\u6211\u4eec\u53ea\u6709\u4e00\u4e2a\u5355\u8bcd\u5305\u542b\u8fd9\u4e9b\u5b57\u7b26": 92, "\u4f46\u662funicode\u8fd8\u662f\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6269\u5c55\u673a\u5236": 102, "\u4f46\u662f\u5176\u4ed6\u6b27\u6d32\u56fd\u5bb6\u7684\u8bed\u8a00\u6bd4\u5982\u6cd5\u8bed": 102, "\u4f46\u662f\u5b83\u548c\u6c49\u5b57\u7684\u6620\u5c04\u4e0eunicode\u548c\u6c49\u5b57\u7684\u6620\u5c04\u6ca1\u6709\u4efb\u4f55\u5173\u7cfb": 102, "\u4f46\u8fd9\u5e76\u4e0d\u4e00\u5b9a\u662f\u6700\u5408\u7406\u7684\u7f16\u7801\u65b9\u5f0f": 92, "\u4f4e\u7ef4": 94, "\u4f4e\u9891\u5185\u63d2\u7684\u65b9\u5f0f": 94, "\u4f60\u53ef\u80fd\u4e0d\u6e05\u695awordpiece\u662f\u5982\u4f55\u9009\u53d6\u5b50\u8bcd\u7684": 101, "\u4f7f\u7528\u4e0b\u8ff0\u547d\u4ee4\u8fdb\u884c\u8bc4\u4f30": 13, "\u4f8b\u5982\u5b57\u7b26\u4e32": 10, "\u4f8b\u5982\u7f16\u7801\u5e8f\u5217": 92, "\u4fee\u6539\u540e\u663e\u793a\u7ed3\u679c\u5982\u4e0b": 102, "\u5047\u8bbe\u5355\u8bcd\u7684\u5e8f\u5217\u662f": 92, "\u5047\u8bbe\u53e5\u5b50": 101, "\u5047\u8bbe\u6211\u4eec\u6709\u4e00\u4e2a\u8bed\u6599\u5e93": 92, "\u5047\u8bbe\u6211\u4eec\u6709\u9700\u8981\u7f16\u7801": 92, "\u5047\u8bbe\u628a\u76f8\u90bb\u4f4d\u7f6e\u7684x\u548cy\u4e24\u4e2a\u5b50\u8bcd\u8fdb\u884c\u5408\u5e76": 101, "\u5047\u8bbe\u8fd9\u4e9b\u8bcd\u51fa\u73b0\u7684\u9891\u7387\u5982\u4e0b": 92, "\u50cf": 92, "\u5141\u8bb8\u8868\u793a\u4e00\u767e\u591a\u4e07\u4e2a\u5b57\u7b26": 102, "\u5168\u7403\u7edd\u5927\u90e8\u5206\u5b57\u7b26\u7684\u5b57\u7b26\u96c6": 102, "\u5176\u4e2d": [13, 92], "\u5176\u4e2d\u4e0d\u6b62utf": 102, "\u5176\u4e2d\u4f1a\u6d89\u53ca\u5230ascii": 102, "\u5176\u4e2d\u5305\u542b\u5355\u8bcd": 92, "\u5176\u4ed6\u5b57\u8282": 102, "\u5176\u4ed6\u8bed\u8a00": 102, "\u5176\u4ed6\u8bed\u8a00\u53ef\u80fd\u6709\u6240\u4e0d\u540c": 92, "\u51fa\u73b0\u4e86": 92, "\u51fd\u6570": 10, "\u51fd\u6570\u5462": 10, "\u51fd\u6570\u5feb\u901f\u8f6c\u6362\u4e3a\u547d\u4ee4\u884c\u63a5\u53e3": 10, "\u51fd\u6570\u7684\u53c2\u6570\u5217\u8868": 10, "\u5206\u522b\u6765\u81ea": 10, "\u5219\u53e5\u5b50": 101, "\u521d\u59cbtoken\u5c06\u662f\u6240\u6709\u5b57\u7b26\u548c": 92, "\u524d\u9762\u5168\u90e8\u586b\u51450": 102, "\u5269\u4e0b\u7684\u552f\u4e00\u5b57\u8282\u5bf9\u662f": 92, "\u52a0\u4e0a\u63a7\u5236\u7801\u540e\u5bf9\u5e94\u7684utf": 102, "\u5339\u914d": 10, "\u5341\u516d\u8fdb\u5236": 102, "\u5355\u5b57\u8282256\u4e2a\u5b57\u7b26\u80af\u5b9a\u6ca1\u6cd5\u5b8c\u5168\u8868\u793a": 102, "\u5373": [10, 92], "\u538b\u7f29": 92, "\u539f\u672c\u5728\u4f4e\u7ef4\u5ea6\u4e0a": 94, "\u53c2\u6570\u4e3a": 13, "\u53cd\u5411\u6267\u884c\u4ee5\u4e0a\u8fc7\u7a0b\u5c31\u884c\u4e86": 92, "\u53cd\u590d\u8fed\u4ee3\u76f4\u5230\u6ee1\u8db3\u505c\u6b62\u6761\u4ef6": 92, "\u53ea\u9700\u8981\u4e00\u4e2a\u5b57\u8282\u8868\u793a": 102, "\u53ea\u9700\u8981\u4f7f\u7528\u540e\u97627\u4f4d\u5c31\u8db3\u591f\u4e86": 102, "\u53ef\u4ee5\u4f7f\u75281": 102, "\u5404\u4e2a\u56fd\u5bb6\u4fbf\u7740\u624b\u81ea\u5df1\u56fd\u5bb6\u8bed\u8a00\u7684\u7f16\u7801": 102, "\u5408\u5e76\u505c\u6b62token": 92, "\u5408\u5e76\u540e\u4ea7\u751f\u7684\u5b50\u8bcd\u8bb0\u4e3az": 101, "\u5408\u5e76\u5b57\u7b26\u53ef\u4ee5\u8ba9\u4f60": 92, "\u5408\u5e76\u5b83\u4eec": 92, "\u540c\u7406\u8fd8\u6709\u5176\u4ed6\u56fd\u5bb6\u7684\u7279\u6709\u5b57\u7b26\u96c6": 102, "\u548c": [10, 92, 94], "\u548cascii\u7684\u76ee\u7684\u4e00\u6837": 102, "\u548cascii\u7801\u4e00\u81f4": 102, "\u56de\u8f6613\u7b49\u7b49\u4e00\u4e9b\u952e\u76d8\u4e0a\u6240\u89c1\u7684\u7b26\u53f7": 102, "\u56e0\u4e3a": 92, "\u56e0\u4e3a\u4eba\u7c7b\u8bed\u8a00\u4e5f\u7ecf\u5e38\u4ee5\u5355\u8bcd\u4e3a\u5355\u4f4d\u8fdb\u884c\u4ea4\u6d41": 92, "\u56e0\u4e3a\u5b83\u4eec\u5728\u6211\u4eec\u7684\u8bed\u6599\u5e93\u4e2d\u51fa\u73b0\u4e86": 92, "\u56e0\u4e3a\u6211\u4eec\u7edf\u8ba1\u76f8\u90bb\u5b57\u7b26\u5bf9\u65f6\u4e0d\u80fd\u628a\u5206\u522b\u4f4d\u4e8e\u4e24\u4e2a\u5355\u8bcd\u4e2d\u7684\u5b57\u7b26\u5bf9\u7b97\u8fdb\u53bb": 92, "\u56e0\u4e3a\u6ca1\u6709\u51fa\u73b0\u591a\u6b21\u7684\u5b57\u8282\u5bf9": 92, "\u56e0\u6b64": 92, "\u56e0\u6b64bpe\u4e5f\u662f\u5178\u578b\u7684\u57fa\u4e8esubword\u7684tokenization\u7b97\u6cd5": 92, "\u56e0\u6b64\u6211\u4eec\u53ef\u91c7\u7528\u9ad8\u9891\u5916\u63a8": 94, "\u56e0\u6b64\u6211\u4eec\u5c06\u7528\u4e00\u4e2a\u65b0\u5b57\u8282": 92, "\u56e0\u6b64\u6211\u4eec\u6ca1\u6709\u5c06\u5b83\u4eec\u5408\u5e76": 92, "\u5728": 10, "\u5728finest\u548clowest\u4e24\u4e2a\u8bcd\u4e2d": 92, "\u5728unicode\u8bde\u751f": 102, "\u5728\u5b9e\u8df5\u4e2d": 92, "\u5728\u6211\u4eec\u7684\u8bed\u6599\u5e93\u4e2d": 92, "\u5728\u6211\u4eec\u7684\u8bed\u6599\u5e93\u4e2d\u51fa\u73b0\u4e86": 92, "\u5728\u6bcf\u4e2a\u5355\u8bcd\u7684\u672b\u5c3e\u6dfb\u52a0": 92, "\u5728\u8ba1\u7b97\u673a\u4e2d\u6240\u6709\u4fe1\u606f\u90fd\u662f\u4ee5\u4e8c\u8fdb\u5236\u4f4d0\u548c1\u6765\u5b58\u50a8\u7684": 102, "\u5728\u8bed\u8a00\u6a21\u578b\u9884\u6d4b\u7684\u65f6\u5019\u9700\u8981\u5728\u8fd9\u4e2a\u62e5\u6709\u51e0\u4e07\u4e2a\u5355\u8bcd\u7684\u5217\u8868\u4e0a\u8ba1\u7b97\u4e00\u4e2a\u6982\u7387\u5206\u5e03": 92, "\u5728\u8fd9\u91cc": 92, "\u5728\u8fed\u4ee3\u7684\u65f6\u5019\u901a\u8fc7\u6bd4\u8f83token\u7684\u9891\u7387\u5927\u5c0f\u6765\u7a77\u5c3d\u6bcf\u4e00\u79cd\u53ef\u80fd": 92, "\u5927\u5199\u5b57\u6bcda\u5bf9\u5e94\u7684ascii\u7801\u662f65": 102, "\u5927\u5bb6\u90fd\u77e5\u9053\u7535\u8111\u6240\u6709\u4fe1\u606f\u90fd\u662f\u4ee5\u4e8c\u8fdb\u5236\u7684\u65b9\u5f0f\u6765\u8ba1\u7b97\u548c\u5b58\u50a8\u7684": 102, "\u5927\u6982\u5c31\u662f\u8bf4ascii\u662f\u7f8e\u56fd\u4fe1\u606f\u4ea4\u6362\u6807\u51c6\u4ee3\u7801": 102, "\u5927\u90e8\u5206\u662f2\u5b57\u8282": 102, "\u5982\u4f55\u6765\u8868\u793aunicod": 102, "\u5982\u4f55\u77e5\u9053\u5f53\u524d\u5b57\u8282\u5c31\u662f\u8981\u8868\u793a\u4e00\u4e2a\u5b57\u7b26\u8fd8\u662f\u8fde\u7eed\u7684\u4e24\u4e2a\u5b57\u8282\u8868\u793a\u4e00\u4e2a\u5b57\u7b26": 102, "\u5982\u4f55\u8ba9\u8ba1\u7b97\u673a\u8bfb\u61c2unicod": 102, "\u5982\u4f55\u8bfb\u53d6unicode\u5b57\u7b26\u96c6": 102, "\u5982\u4f55\u9009\u62e9\u4e24\u4e2a\u5b50\u8bcd\u8fdb\u884c\u5408\u5e76": 101, "\u5982\u679c\u4f1a\u7559\u4e0b\u51e0\u4e2a\u5b50\u4e32": 92, "\u5982\u679c\u5728\u4f4e\u7ef4\u5ea6\u8fdb\u884c\u5185\u63d2": 94, "\u5982\u679c\u6309\u7167unicode\u7684\u65b9\u5f0f\u7edf\u4e00\u7528\u4e24\u4e2a\u5b57\u8282\u8868\u793a\u4e00\u4e2a\u5b57\u7b26": 102, "\u5982\u679c\u7b97\u6cd5\u770b\u5230token": 92, "\u5b57\u6bcd": 102, "\u5b57\u7b26\u7801\u5c31\u662f\u5bf9\u5e94\u7684unicod": 102, "\u5b57\u7b26\u7801\u7ec4\u6210": 102, "\u5b57\u7b26\u7b49": 92, "\u5b57\u7b26\u96c6\u5373\u662f\u6587\u5b57\u7b26\u53f7\u548c\u4e8c\u8fdb\u5236\u7684\u4e00\u79cd\u6620\u5c04\u5173\u7cfb": 102, "\u5b57\u8282\u957f\u5ea6": 102, "\u5b66\u540c\u4e00\u4e2a": 62, "\u5b83\u4e0d\u80fd\u88ab\u8fdb\u4e00\u6b65\u538b\u7f29": 92, "\u5b83\u4eec\u51fa\u73b0\u4e86": 92, "\u5b83\u4eec\u7ecf\u5e38\u5728\u8bed\u6599\u4e2d\u4ee5\u76f8\u90bb\u65b9\u5f0f\u540c\u65f6\u51fa\u73b0": 101, "\u5b83\u53ea\u6709\u4e00\u4e2a": 92, "\u5b83\u5728": 92, "\u5b83\u5c31\u4f1a\u77e5\u9053\u5b83\u662f": 92, "\u5b83\u7684\u4f5c\u7528\u662f\u628a\u5404\u4e2a\u8bed\u8a00\u7684\u5b57\u7b26\u6620\u5c04\u4e3a\u4e8c\u8fdb\u5236": 102, "\u5b83\u7684\u7279\u70b9\u662f\u53d8\u957f\u7f16\u7801": 102, "\u5b83\u9075\u5faa\u4e00\u79cd\u8d2a\u5a6a\u7684\u7b56\u7565\u6765\u5c3d\u53ef\u80fd\u53d6\u5f97\u6700\u4f18\u7684\u89e3\u51b3\u65b9\u6848": 92, "\u5b8c\u5168\u517c\u5bb9ascii": 102, "\u5bf9\u4e0eascii\u7a7a\u95f4\u6d6a\u8d39": 102, "\u5bf9\u4e8e\u5355\u5b57\u8282\u7684\u5b57\u7b26": 102, "\u5bf9\u4e8e\u53e5\u5b50": 101, "\u5bf9\u4e8e\u5b58\u50a8\u7a7a\u95f4\u662f\u6781\u5927\u7684\u6d6a\u8d39": 102, "\u5bf9\u4e8e\u672a\u77e5": 92, "\u5bf9\u4e8e\u82f1\u6587\u56fd\u5bb6\u6765\u8bf4\u80fd\u6ee1\u8db3\u65e5\u5e38\u4f7f\u7528": 102, "\u5bf9\u4e8e\u9700\u8981n\u4e2a\u5b57\u8282\u7684\u5b57\u7b26": 102, "\u5bf9\u5e94\u7684unicode\u5b57\u7b26\u96c6\u662f4e00": 102, "\u5bf9\u5e94\u7684unicode\u662fu": 102, "\u5bf9\u5e94\u7684\u4e8c\u8fdb\u5236\u4e3a01000001": 102, "\u5bf9\u65b0\u6570\u636e\u8fdb\u884c\u7f16\u7801\u7684\u8fc7\u7a0b\u8fd8\u662f\u6bd4\u8f83\u7b80\u5355\u7684": 92, "\u5bf9\u7528\u4f4e\u7ef4\u533a\u5206\u4e0d\u540c\u4f4d\u7f6e\u95f4\u7684\u80fd\u529b\u5f71\u54cd\u66f4\u5927": 94, "\u5bfb\u627e\u6700\u5e38\u51fa\u73b0\u7684\u5b57\u8282\u5bf9": 92, "\u5c06": 10, "\u5c06\u53c2\u6570\u503c\u8f6c\u6362\u4e3a\u6b63\u786e\u7684\u7c7b\u578b": 10, "\u5c31\u4ee3\u8868\u4e00\u4e2a\u8bed\u8a00\u5355\u4f4d": 92, "\u5c31\u50cf\u5355\u8bcd": 92, "\u5c31\u662f\u4f7f\u7528\u63a7\u5236\u7801": 102, "\u5c31\u80fd\u4ee3\u8868\u4e86\u6240\u6709\u952e\u76d8\u4e0a\u7684\u666e\u901a\u7b26\u53f7": 102, "\u5c31\u8bde\u751f\u4e86utf": 102, "\u5c3d\u7ba1\u8d2a\u5a6a": 92, "\u5e03\u5c14\u503c\u7b49": 10, "\u5e74\u53d1\u8868\u7684\u6587\u7ae0": 92, "\u5e76\u4e00\u6b21\u53c8\u4e00\u6b21\u5730\u6267\u884c\u76f8\u540c\u7684\u8fed\u4ee3": 92, "\u5e76\u4e14\u6211\u4eec\u7684\u5b50\u5b57\u7b26\u4e32\u5c06\u88ab\u66ff\u6362\u4e3a\u6211\u4eectoken\u5217\u8868\u4e2d\u5df2\u7ecf\u5b58\u5728\u7684token\u7ec4\u5408": 92, "\u5e76\u4e14\u7531utf": 102, "\u5e76\u5c06\u5176\u91cd\u547d\u540d\u4e3a": 10, "\u5e76\u5c06\u5176\u9891\u7387\u8bb0\u4e3a": 92, "\u5e76\u5c06\u5b83\u4eec\u6dfb\u52a0\u5230token\u5217\u8868\u4e2d": 92, "\u5e76\u5c06\u65b0\u8bcd\u7684token\u6dfb\u52a0\u5230\u6211\u4eec\u7684token\u5b57\u5178\u4e2d\u4ee5\u5907\u5c06\u6765\u7528\u5230": 92, "\u5e76\u5c1d\u8bd5\u4f7f\u7528\u8fd9\u4e9btoken\u66ff\u6362\u7ed9\u5b9a\u5355\u8bcd\u5e8f\u5217\u4e2d\u7684\u5b50\u5b57\u7b26\u4e32": 92, "\u5e76\u88ab\u4f5c\u4e3a\u673a\u5668\u7ffb\u8bd1\u7b49\u4e3b\u6d41nlp\u4efb\u52a1\u7684\u9996\u9009tokenize\u65b9\u6cd5\u4e4b\u4e00": 92, "\u5e76\u91cd\u65b0\u8ba1\u7b97\u6bcf\u4e2atoken\u51fa\u73b0\u7684\u9891\u7387": 92, "\u5e93": 10, "\u5f00\u5934": 102, "\u5f00\u59cb": 92, "\u5f53\u524d\u5206\u8bcd\u4e0b\u53e5\u5b50s\u7684\u4f3c\u7136\u503c\u53ef\u4ee5\u8868\u793a\u4e3a": 101, "\u5f53\u7136": 92, "\u5f53\u8fd0\u884c\u811a\u672c\u65f6": 10, "\u610f\u5473\u7740\u8fd9\u4e9b\u7ef4\u5ea6\u4e0a\u7684\u4fe1\u53f7\u53d8\u5316\u975e\u5e38\u8fc5\u901f": 94, "\u6211\u4eec\u4f1a\u5c06": 92, "\u6211\u4eec\u5148\u7528\u4e00\u53e5\u8bdd\u6982\u62ec\u5b83\u7684\u6838\u5fc3\u601d\u60f3": 92, "\u6211\u4eec\u53ef\u4ee5\u770b\u5230": 92, "\u6211\u4eec\u53ef\u4ee5\u9012\u5f52\u5730\u4f7f\u7528\u5b57\u8282\u5bf9\u7f16\u7801\u5c06": 92, "\u6211\u4eec\u5c06tokenized\u597d\u7684\u5355\u8bcd\u4fdd\u5b58\u5728\u5b57\u5178\u4e2d": 92, "\u6211\u4eec\u5c06\u4ece\u7b2c\u4e8c\u5e38\u89c1\u7684\u6807\u8bb0": 92, "\u6211\u4eec\u5c06\u5b57\u7b26\u89c6\u4e3a\u4e0e\u5b57\u8282\u7b49\u4ef7": 92, "\u6211\u4eec\u5c06\u5b83\u4eec\u5408\u5e76\u4ee5\u5f62\u6210\u4e00\u4e2a\u65b0\u7684token": 92, "\u6211\u4eec\u5c06\u6bcf\u4e2a\u5355\u8bcd\u62c6\u5206\u4e3a\u5b57\u7b26\u5e76\u8ba1\u7b97\u5b83\u4eec\u7684\u51fa\u73b0\u6b21\u6570": 92, "\u6211\u4eec\u5c06\u7528unknown": 92, "\u6211\u4eec\u5c06\u7ee7\u7eed\u6267\u884c\u6b64\u5408\u5e76\u6b65\u9aa4": 92, "\u6211\u4eec\u5c06\u88ab\u89e3\u7801\u4e3a": 92, "\u6211\u4eec\u5c06\u904d\u5386\u6211\u4eec\u5728\u8bed\u6599\u5e93\u4e2d\u627e\u5230\u7684\u6240\u6709token": 92, "\u6211\u4eec\u5c06\u904d\u5386\u6240\u6709token": 92, "\u6211\u4eec\u5e38\u7528\u7684\u8bed\u8a00\u6a21\u578b\u8bcd\u6c47\u5217\u8868\u662f\u5f88\u5927\u7684": 92, "\u6211\u4eec\u5e94\u7528\u4e0a\u8ff0\u7f16\u7801\u65b9\u6cd5\u5bf9\u65b0\u8bcd\u8fdb\u884ctoken": 92, "\u6211\u4eec\u5fc5\u987b\u7b80\u5355\u5730\u5c06\u6240\u6709token\u8fde\u63a5\u5728\u4e00\u8d77\u4ee5\u83b7\u5f97\u6574\u4e2a\u5355\u8bcd": 92, "\u6211\u4eec\u603b\u5171\u6709": 92, "\u6211\u4eec\u6709\u4e00\u4e2a\u9891\u7387\u4e3a": 92, "\u6211\u4eec\u6765\u67e5\u51fa\u8fd9\u51e0\u4e2a\u5b57\u5bf9\u5e94\u7684utf": 102, "\u6211\u4eec\u73b0\u5728\u5c06\u5408\u5e76token": 92, "\u6211\u4eec\u73b0\u5728\u6709": 92, "\u6211\u4eec\u73b0\u5728\u6709\u4e86": 92, "\u6211\u4eec\u73b0\u5728\u770b\u5230\u5b57\u8282\u5bf9": 92, "\u6211\u4eec\u7684\u6570\u636e\u73b0\u5728\u5df2\u8f6c\u6362\u4e3a": 92, "\u6211\u4eec\u7684\u6a21\u578b\u5728\u8bad\u7ec3\u4e2d\u6ca1\u6709\u770b\u5230\u7684\u8bcd": 92, "\u6211\u4eec\u770b\u5230\u5b57\u8282\u5bf9": 92, "\u6211\u4eec\u77e5\u9053": 92, "\u6211\u4eec\u8ba1\u7b97\u8fd9\u4e9b\u8bcd\u5728\u8bed\u6599\u5e93\u4e2d\u7684\u51fa\u73b0\u9891\u7387": 92, "\u6211\u4eec\u8fd8\u5c06\u4ece\u5355\u4e2atoken": 92, "\u6211\u4eec\u90fd\u4e86\u89e3\u4e00\u79cd\u6700\u57fa\u672c\u7684token": 92, "\u6216": 92, "\u6216\u8005\u53eb": 102, "\u6240\u4ee5": [92, 101], "\u6240\u4ee5ascii\u7684\u5b57\u7b26": 102, "\u6240\u4ee5\u4ed6\u4eec\u5c31\u628a\u7b2c\u4e00\u4e2a\u7a7a\u95f20\u4f7f\u7528\u4e86\u8d77\u6765": 102, "\u6240\u4ee5\u4ed6\u4eec\u628a\u6bcf\u4e2a\u5b57\u8282\u7684\u7b2c\u4e00\u4f4d\u7f6e\u7f6e0": 102, "\u6240\u4ee5\u4f7f\u7528\u4e24\u4e2a\u5b57\u8282\u5df2\u7ecf\u8db3\u591f": 102, "\u6240\u4ee5\u5165\u53e3\u662f": 10, "\u6240\u4ee5\u5bf9\u5e94\u7684utf": 102, "\u6240\u4ee5\u5c31\u51fa\u73b0\u4e86\u4e00\u4e2a\u5168\u7403\u7edf\u4e00\u7684\u7f16\u7801\u89c4\u5219\u53ebunicod": 102, "\u6240\u4ee5\u6211\u4eec\u4e0d\u5bf9\u5b83\u8fdb\u884c\u7f16\u7801": 92, "\u6240\u4ee5\u6211\u4eec\u6709": 92, "\u6240\u4ee5\u8fd9\u5c31\u51fa\u73b0\u4e86\u4e00\u4e2a\u95ee\u9898": 102, "\u628a": 10, "\u628a\u5b83\u653e\u5728\u672c\u5730": 13, "\u63a5\u4e0b\u6765": 92, "\u63a7\u5236\u7801\u5c31\u662f\u544a\u8bc9\u8ba1\u7b97\u673a\u5f53\u524d\u662f\u5355\u5b57\u8282\u8fd8\u662f\u591a\u5b57\u8282": 102, "\u653e\u5165\u7528\u6237\u4e3b\u76ee\u5f55\u4e0b\u7684": 10, "\u6563\u5ea6\u76f4\u63a5\u653e\u5728": 62, "\u6570\u5b57\u548c\u5b57\u6bcd\u90fd\u672a\u4e71\u7801": 102, "\u6570\u636e\u7684\u538b\u7f29": 92, "\u6587\u4ef6": 10, "\u6587\u4ef6\u4e2d\u6709\u5982\u4e0b\u914d\u7f6e": 10, "\u6587\u4ef6\u653e\u5165\u6b64\u76ee\u5f55\u5e76\u91cd\u547d\u540d\u4e3a": 10, "\u659c\u4f53": 102, "\u65b0": 92, "\u65b0\u5efa\u4e00\u4e2ahtml\u8f93\u5165": 102, "\u65cb\u8f6c\u5f97\u8d8a\u591a": 94, "\u65cb\u8f6c\u5f97\u8d8a\u5c11": 94, "\u65cb\u8f6c\u89d2\u5ea6\u8f83\u5927": 94, "\u65e0\u8bba\u5982\u4f55": 92, "\u65e0\u8bba\u662fascii\u7f16\u7801\u8fd8\u662futf": 102, "\u65e5\u6587": 102, "\u65f6": 94, "\u662f\u4e00\u79cd\u7b80\u5355\u7684\u6570\u636e\u538b\u7f29\u7b97\u6cd5": 92, "\u662f\u4ee3\u7801\u7247\u6bb5": 13, "\u662f\u4f7f\u7528": 10, "\u662f\u4f7f\u7528\u6700\u5e7f\u6cdb\u7684sub": 92, "\u662f\u7684": 92, "\u666e\u901a\u7b26\u53f7\u7684\u5b57\u7b26\u96c6": 102, "\u66ff\u6362\u5b83": 92, "\u6700\u5e38\u51fa\u73b0": 92, "\u6700\u5e38\u89c1\u7684\u5e26\u6709": 92, "\u6700\u7ec8": 92, "\u6700\u7ec8\u5bfc\u81f4": 94, "\u6709\u4e00\u79cd\u7f16\u7801\u65b9\u5f0f\u80fd\u5927\u5927\u51cf\u5c0ftoken": 92, "\u6709\u4ec0\u4e48\u7528": 10, "\u6709\u5f88\u591a\u9ad8\u9891\u7ef4\u5ea6\u8f6c\u4e86\u5f88\u591a\u5708": 94, "\u672a\u63d0\u53ca\u7684\u4f4d\u4f7f\u7528\u5bf9\u5e94\u7684unicode\u8865\u5145": 102, "\u6765\u505a\u4e00\u4e2a\u5c0f\u5b9e\u9a8c\u5e76\u4e14\u9a8c\u8bc1\u4e0b": 102, "\u6765\u8bf4": 102, "\u67e5\u770b\u5176\u4ed6token": 92, "\u6807\u8bb0\u4ee5\u6807\u8bc6\u5355\u8bcd\u8fb9\u754c\u80fd\u591f\u8ba9\u7b97\u6cd5\u77e5\u9053\u6bcf\u4e2a\u5355\u8bcd\u7684\u7ed3\u675f\u4f4d\u7f6e": 92, "\u6807\u8bb0\u7684\u96c6\u5408": 92, "\u6a21\u4eff\u663e\u8457\u6027": 39, "\u6b21": 92, "\u6b27\u6d32\u90e8\u5206\u8bed\u8a00\u5b57\u6bcd\u7684\u5b57\u7b26\u96c6": 102, "\u6b64\u65f6\u53e5\u5b50": 101, "\u6bd4\u5982utf": 102, "\u6bd4\u5982\u4e2d\u56fd1980\u5e74\u53d1\u884c\u4e86gb2312\u4ee5\u53ca\u540e\u9762\u51fa\u73b0\u5b83\u7684\u6269\u5c55\u7248\u672cgbk": 102, "\u6bd4\u5982\u5728ascii\u4e2d": 102, "\u6bd4\u5982\u5927\u5199\u5b57\u6bcda": 102, "\u6bd4\u5982\u6c49\u5b57": 102, "\u6bd4\u5982\u6c49\u5b57\u4e00\u5728unicode\u4e2d\u5bf9\u5e94\u7684\u5b57\u7b26\u96c6\u662f4e00": 102, "\u6bd4\u5982\u82f1\u6587\u5b57\u6bcda\u5bf9\u5e94\u7684unicode\u662fu": 102, "\u6bd4\u5982\u8bf4\u4e2d\u6587": 102, "\u6c49\u5b57": 102, "\u6c49\u5b57\u7684\u5b57\u7b26\u96c6": 102, "\u6ca1\u6709": 62, "\u6d4f\u89c8\u5668\u6253\u5f00\u540e\u663e\u793a\u6b63\u5e38": 102, "\u7136\u540e\u5bf9\u5176\u8fdb\u884c\u7f16\u53f7": 92, "\u7136\u540e\u6211\u4eec\u4f7f\u7528chrome\u7684charset\u63d2\u4ef6\u6765\u4fee\u6539\u5f53\u524d\u9875\u9762\u89e3\u7801\u7684\u5b57\u7b26\u96c6": 102, "\u7136\u800c": 92, "\u73b0\u5728\u6211\u4eec\u53d1\u73b0": 92, "\u73b0\u5728\u6211\u4eec\u5c06\u6700\u5e38\u89c1\u7684\u5b57\u8282\u5bf9\u5408\u5e76\u6210\u4e00\u4e2atoken": 92, "\u73b0\u5728\u8ba9\u6211\u4eec\u770b\u770b\u5982\u4f55\u89e3\u7801\u6211\u4eec\u7684\u793a\u4f8b": 92, "\u751f\u6210\u5f85\u8bc4\u4f30\u7684\u6587\u4ef6": 13, "\u7528\u6700\u5c11\u7684token\u6765\u8868\u793a\u8bed\u6599\u5e93": 92, "\u7528\u6765\u8868\u793a\u7535\u8111\u548c\u5176\u4ed6\u7535\u4fe1\u8bbe\u5907\u7684\u6587\u672c": 102, "\u7531": 101, "\u7531m\u4e2a\u5b50\u8bcd\u7ec4\u6210": 101, "\u7531\u4e8eascii\u8868\u793a\u7684\u8303\u56f4\u6709\u9650": 102, "\u7531\u4e8e\u6211\u4eec\u603b\u5171\u6709": 92, "\u7684": [10, 92], "\u7684\u4f18\u52bf": 62, "\u7684\u5b57\u7b26\u91cf\u5df2\u7ecf\u8db3\u4ee5\u7528\u4e8e\u5168\u7403\u4e3b\u8981\u8bed\u8a00\u7684\u5927\u591a\u6570\u5b57\u7b26": 102, "\u7684\u5b57\u8282\u5bf9\u662f": 92, "\u7684\u60c5\u51b5": 62, "\u7684\u6548\u679c": 62, "\u7684\u6570\u636e": 92, "\u7684\u65b0token": 92, "\u7684\u6838\u5fc3\u673a\u5236": 10, "\u7684\u7b2c\u4e00\u5b57\u8282\u5168\u662f0": 102, "\u7684\u8bed\u8a00\u6a21\u578b\u4f3c\u7136\u503c\u7b49\u4ef7\u4e8e\u6240\u6709\u5b50\u8bcd\u6982\u7387\u7684\u4e58\u79ef": 101, "\u7684\u9891\u7387\u4e3a": 92, "\u7684\u9891\u7387\u51cf\u5c11": 92, "\u76ee\u5f55\u4e0b": 13, "\u76f4\u5230\u8fbe\u5230\u6211\u4eec\u9884\u5148\u8bbe\u7f6e\u7684token\u6570\u9650\u5236\u6216\u8fed\u4ee3\u9650\u5236": 92, "\u76f8\u6bd4": 62, "\u76f8\u90bb\u5b57\u8282\u5bf9": 92, "\u76f8\u90bb\u6570\u636e\u5355\u4f4d\u5728bpe\u4e2d\u770b\u4f5c\u76f8\u90bb\u5b57\u8282\u5bf9": 92, "\u7701\u8d44\u6e90": 62, "\u770b\u5230\u8fd9\u91cc": 101, "\u770b\u5b8c\u4e0b\u9762\u5b8c\u6574\u7684\u8fed\u4ee3\u8fc7\u7a0b": 92, "\u770b\u770b\u6211\u4eec\u6700\u7ec8\u7684token\u5217\u8868": 92, "\u771f\u5b9e\u7684": 64, "\u77e5\u9053\u4e86\u7f16\u7801\u7684\u539f\u7406\u540e\u81ea\u7136\u80fd\u60f3\u5230\u7684\u5c31\u662f\u6587\u672c\u7f16\u7801\u548c\u89e3\u7801\u6240\u4f7f\u7528\u7684\u5b57\u7b26\u96c6\u4e0d\u540c": 102, "\u786e\u4fdd\u6700\u5e38\u89c1\u7684\u8bcd\u5728token\u5217\u8868\u4e2d\u8868\u793a\u4e3a\u5355\u4e2atoken": 92, "\u7a0d\u540e\u6211\u4eec\u5c06\u770b\u5230": 92, "\u7a76\u7adf\u8c03\u7528\u7684\u662f\u4ec0\u4e48\u51fd\u6570": 10, "\u7a7a\u95f4\u6781\u5927\u6d6a\u8d39": 102, "\u7b2cn": 102, "\u7b2c\u4e00\u4e2a\u5b57\u8282": 102, "\u7b2c\u4e00\u4e2a\u5b57\u8282\u7684\u7b2c\u4e8c\u4e2a0": 102, "\u7b2c\u4e8c\u9ad8\u9891\u7387\u6807\u8bb0\u662f": 92, "\u7b49\u8bcd\u4e4b\u95f4\u7684\u533a\u522b": 92, "\u7b97\u6cd5\u7684\u4e0b\u4e00\u6b65\u662f\u5bfb\u627e\u6700\u9891\u7e41\u7684\u5b57\u7b26\u5bf9": 92, "\u7b97\u6cd5\u7684\u4e3b\u8981\u76ee\u6807": 92, "\u7c7b\u4f3c\u5730": 10, "\u7f16\u7801\u4e3a": 92, "\u7f16\u7801\u672c\u8eab\u8ba1\u7b97\u590d\u6742\u5ea6\u6bd4\u8f83\u9ad8": 92, "\u7f16\u7801\u7c7b\u578b": 102, "\u8001\u89c4\u77e9": 92, "\u800cwordpiece\u9009\u62e9\u80fd\u591f\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u6982\u7387\u6700\u5927\u7684\u76f8\u90bb\u5b50\u8bcd\u52a0\u5165\u8bcd\u8868": 101, "\u800c\u4e0d\u662f": 92, "\u800c\u4e14\u8fc7\u5927\u7684token\u5217\u8868\u5341\u5206\u5f71\u54cd\u6a21\u578b\u7684\u9884\u6d4b\u51c6\u786e\u5ea6": 92, "\u800c\u5728gb2312\u4e2d\u5bf9\u5e94\u7684\u5b57\u7b26\u96c6\u662fd2bb": 102, "\u800c\u662f\u5c06\u5b83\u4ee5utf": 102, "\u800c\u6c49\u5b57\u4e00\u4e8c\u4e09\u4e71\u7801\u4e86": 102, "\u800c\u7f55\u89c1\u7684\u8bcd\u88ab\u5206\u89e3\u4e3a\u4e24\u4e2a\u6216\u591a\u4e2asubword": 92, "\u80fd\u591f\u6e05\u695a\u5730\u7406\u89e3wordpiece\u5728\u5408\u5e76\u8fd9\u4e00\u6b65\u662f\u5982\u4f55\u4f5c\u51fa\u9009\u62e9\u7684": 101, "\u80fd\u591f\u7cbe\u7ec6\u5730\u533a\u5206\u76f8\u90bb\u4f4d\u7f6e": 94, "\u82e5\u4f7f\u7528\u8fd9\u79cd\u7f16\u7801\u65b9\u5f0f": 92, "\u82f1\u6587\u5b57\u6bcd": 102, "\u83b7\u53d6\u6a21\u578b\u7684": 10, "\u8461\u8404\u7259\u8bed\u7b49\u65e0\u6cd5\u7528127\u4e2a\u5b57\u7b26\u8868\u793a\u5b8c\u5168": 102, "\u8868\u793a\u5b50\u8bcd": 101, "\u8981\u89e3\u7801": 92, "\u89c4\u52191": 102, "\u89c4\u52192": 102, "\u89e3\u6790\u547d\u4ee4\u884c\u53c2\u6570": 10, "\u8ba1\u7b97": 94, "\u8ba1\u7b97\u673a\u5bf9\u6570\u636e\u7684\u8bfb\u53d6\u662f\u6309\u7167\u4e00\u4e2a\u5b57\u8282\u7684\u5927\u5c0f\u6765\u8bfb\u53d6\u8bc6\u522b\u7684": 102, "\u8ba1\u7b97\u673a\u6309\u7167\u5b57\u8282\u6765\u8bfb\u53d6\u6570\u636e\u65f6": 102, "\u8ba1\u7b97\u673a\u662f\u5982\u4f55\u77e5\u9053\u5f53\u524d\u5b57\u8282\u8868\u793a\u7684\u662f\u4ec0\u4e48\u610f\u601d\u5462": 102, "\u8ba9\u6211\u4eec\u5728\u6bcf\u4e2a\u5355\u8bcd\u7684\u672b\u5c3e\u6dfb\u52a0\u4e00\u4e2a\u7279\u6b8a\u7684\u7ed3\u675f\u6807\u8bb0": 92, "\u8ba9\u6211\u4eec\u73b0\u5728\u505c\u6b62\u8fed\u4ee3\u5e76": 92, "\u8ba9\u6211\u4eec\u73b0\u5728\u8003\u8651": 92, "\u8ba9\u6211\u4eec\u7528": 92, "\u8ba9\u6211\u4eec\u901a\u8fc7\u4e00\u4e2a\u5b9e\u9645\u7684\u4f8b\u5b50\u6765\u4e86\u89e3\u4e00\u4e0b\u5b83\u7684nlp\u7248\u672c": 92, "\u8bcd": 92, "\u8bf4\u660e": 102, "\u8bf4\u660eunicode\u548cgbk\u90fd\u5411\u4e0b\u517c\u5bb9\u4e86ascii": 102, "\u8c03\u7528": 10, "\u8d8a\u8fd1": 94, "\u8d8a\u8fdc": 94, "\u8f6c\u6362\u4e3a\u4e8c\u8fdb\u5236\u662f0100": 102, "\u8f6c\u6362\u4e3a\u4e8c\u8fdb\u5236\u662f100": 102, "\u8f6c\u6362\u4e3a\u5341\u8fdb\u5236\u662f19968": 102, "\u8f6c\u6362\u4e3a\u5341\u8fdb\u5236\u662f65": 102, "\u8f93\u51fa\u6587\u4ef6": 10, "\u8fd8\u6709\u5176\u4ed6\u7f16\u7801\u65b9\u5f0f": 102, "\u8fd8\u6709\u7a7a\u683c32": 102, "\u8fd9\u4e00\u95ee\u9898": 102, "\u8fd9\u4e24\u4e2a\u8bcd\u90fd\u6709\u4e00\u4e2a\u5171\u540c\u7684": 92, "\u8fd9\u4e2a\u4e8c\u8fdb\u5236\u670915\u4f4d": 102, "\u8fd9\u4e2a\u8bcd\u7684token": 92, "\u8fd9\u4e5f\u662f": 92, "\u8fd9\u53ea\u662f\u82f1\u8bed\u7684\u7528\u6cd5": 92, "\u8fd9\u5b8c\u5168\u7b49\u540c\u4e8e127\u4f4d\u6700\u521d\u7684ascii\u7801": 102, "\u8fd9\u610f\u5473\u7740\u6211\u4eec\u7684\u9891\u7387\u8ba1\u6570\u5c06\u5728\u6bcf\u4e2a\u5408\u5e76\u6b65\u9aa4\u540e\u53d1\u751f\u53d8\u5316": 92, "\u8fd9\u662f\u66f4\u65b0\u540e\u7684\u8868\u683c": 92, "\u8fd9\u6709\u52a9\u4e8e\u7b97\u6cd5\u67e5\u770b\u6bcf\u4e2a\u5b57\u7b26\u5e76\u627e\u5230\u9891\u7387\u6700\u9ad8\u7684\u5b57\u7b26\u914d\u5bf9": 92, "\u8fd9\u6709\u52a9\u4e8e\u7b97\u6cd5\u7406\u89e3": 92, "\u8fd9\u6837\u7684token\u5c06\u88ab\u4e0d\u540c\u5730\u5904\u7406": 92, "\u8fd9\u79cd\u73b0\u8c61\u79f0\u4e4b\u4e3a": 94, "\u8fd9\u79cd\u7f16\u7801\u65b9\u5f0f\u5341\u5206\u7b26\u5408\u4eba\u7c7b\u8bed\u8a00\u4e60\u60ef": 92, "\u8fd9\u91cc": [10, 101], "\u8fd9\u91cc\u4fee\u6539\u4e3agbk": 102, "\u8fd9\u91cc\u7b80\u5355\u4ecb\u7ecd\u4e0b\u8fd9\u51e0\u79cd\u7f16\u7801\u65b9\u5f0f\u7684\u533a\u522b": 102, "\u8fdc\u7a0b\u8870\u51cf\u66f2\u7ebf\u5982\u4e0b": 94, "\u8fed\u4ee3": 92, "\u901a\u5e38\u6709\u51e0\u4e07\u5230\u51e0\u5341\u4e07\u91cf\u7ea7\u7684\u5355\u8bcd\u6570": 92, "\u901a\u8fc7\u5f62\u5f0f\u5316\u65b9\u6cd5": 101, "\u90a3\u4e48\u4eba\u4eec\u80fd\u8bc6\u522b\u7684\u6587\u5b57\u548c\u8ba1\u7b97\u673a\u4e2d\u7684\u4e8c\u8fdb\u5236\u5fc5\u7136\u5b58\u5728\u4e00\u79cd\u6620\u5c04\u5173\u7cfb": 102, "\u90a3\u4e48\u9762\u5bf9\u5168\u4e16\u754c\u8fd9\u4e48\u591a\u8bed\u8a00": 102, "\u90a3\u5982\u4f55\u628a\u538b\u7f29\u7684\u7f16\u7801\u590d\u539f\u5462": 92, "\u90a3\u5c31\u662f\u672c\u6587\u5373\u5c06\u4ecb\u7ecd\u7684byte": 92, "\u90a3\u6837\u7684\u8ba1\u7b97\u91cf\u662f\u975e\u5e38\u6050\u6016\u7684": 92, "\u90a3\u82f1\u6587\u5b57\u7b26": 102, "\u90e8\u5206\u9891\u7387\u4f4e": 94, "\u90e8\u5206\u9891\u7387\u9ad8": 94, "\u90fd\u4e00\u6837": 102, "\u91cc\u548c\u653e\u5728": 62, "\u91cc\u7684\u533a\u522b": 62, "\u95f4\u76f8\u4e92\u9694\u5f00": 62, "\u963f\u62c9\u4f2f\u6570\u5b570\u5bf9\u5e94\u7684ascii\u7801\u662f48": 102, "\u963f\u62c9\u4f2f\u6570\u5b57\u548c\u82f1\u6587\u5b57\u6bcd": 102, "\u968f\u673a\u6027\u5f88\u5927": 94, "\u968f\u7740\u8ba1\u7b97\u673a\u5728\u5168\u7403\u7684\u53d1\u5c55\u548c\u666e\u53ca": 102, "\u9700": 10, "\u9700\u52a0\u4e0a": 10, "\u9700\u8981\u4ece": 13, "\u9700\u8981\u81f3\u5c112\u4e2a\u5b57\u8282\u8868\u793a": 102, "\u975e\u5e38\u91cd\u8981": 92, "\u9996\u5148\u6765\u660e\u786e\u4e00\u4e0b\u57fa\u7840\u6982\u5ff5": 92, "\u9996\u5148\u770b\u4e0bwiki\u5bf9\u5b83\u7684\u5b9a\u4e49": 102, "\u9ad8\u4f4d\u4ee5": 102, "\u9ad8\u4f4d\u524dn\u4f4d\u4e3a": 102, "\u9ad8\u7684": 62, "\u9ad8\u7ef4": 94, "\u9ad8\u9891\u4fe1\u606f\u7684\u635f\u5931": 94, "\u9ad8\u9891\u7ef4\u5ea6\u5c11\u4f4e\u9891\u7ef4\u5ea6\u591a": 94, "\ud835\udc41": 61}, "titles": ["Base", "Attention Is All You Need", "GPT", "GPT2", "GPT3", "InstructGPT", "Benchmarks", "Alignment Benchmarks", "Code Alpaca", "CRUXEval", "EvalPlus", "General Benchmarks", "HumanEval", "LiveCodeBench", "Magicoder", "Math &amp; Science Benchmarks", "MBPP", "RewardBench", "SELF-INSTRUCT", "TACO", "WizardCoder", "Contents", "Contents", "Data", "AlphaCode", "APPS", "Code Alpaca", "Magicoder", "OpenCoder", "SELF-INSTRUCT", "TACO", "UNICODER", "WizardCoder", "Introduction", "Markdown Files", "Notebooks with MyST Markdown", "GOLD", "Evaluation of LLMs Should Not Ignore Non-Determinism", "Scaling Laws for Neural Language Models", "Weak to Strong Generalization", "Models", "AlphaCode", "AlphaCode 2", "AlphaCodium", "Code Llama", "DeepSeek-Coder-V2", "DeepSeek-V2", "DeepSeek V3", "Llama", "Llama 2", "Llama 3", "Llama3", "Llama 3 Source Code", "Qwen 2.5", "Qwen2.5-Coder", "Preference Optimization", "ArmoRM-MoE", "DAPO", "DeepSeek-GRM", "DPO", "DPOP", "Efficient Exploration for LLMs", "Group Relative Policy Optimization", "Instruct GPT", "Aligning Language Models with Judgments", "OpenRLHF", "PPO", "REINFORCE++", "Constitutional AI: Harmlessness from AI Feedback", "RLAIF vs. RLHF", "RLCD", "RS-DPO", "RSO", "Secrets of RLHF in Large Language Models: Reward Modeling", "Self-Rewarding Language Models", "Self-Taught Evaluators", "West-of-N", "Reasoning", "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models", "DeepCoder", "DeepSeek-R1", "Logic-RL", "s1: Simple test-time scaling", "Training Language Models to Self-Correct via Reinforcement Learning", "Let\u2019s Verify Step by Step", "References", "SFT", "LIMA: Less Is More for Alignment", "RETHINKING DATA SELECTION AT SCALE: RANDOM SELECTION IS ALMOST ALL YOU NEED", "RL on Incorrect Synthetic Data Scales the Efficiency of LLM Math Reasoning by Eight-Fold", "Scaling Relationship on Learning Mathematical Reasoning with Large Language Models", "Techniques", "Byte Pair Encoding (BPE)", "DeepSeekMoE", "Extending context window of LLMs", "Extending context window of large language models via position interpolation", "Multi-Head Latent Attention", "Normalization", "RoPE", "Rotary Positional Embeddings (RoPE)", "SwiGLU", "WordPiece, ULM and SentencePiece", "ASCII,UNICODE,UTF8", "Llama Factory", "LORA", "OpenRLHF"], "titleterms": {"": 84, "1": 56, "16": 102, "2": [42, 44, 49, 53, 56], "2d": [98, 99], "3": [50, 52], "32\u7b49": 102, "5": [53, 54], "500": 15, "8": 102, "A": 54, "AT": 88, "Not": 37, "The": [36, 43, 56, 67, 90], "abil": [74, 90], "ablat": [87, 96], "absolut": 99, "accuraci": 90, "activ": [51, 52, 61, 84], "adapt": 73, "add": 35, "addit": 43, "advantag": 67, "aggreg": 56, "aha": 80, "ai": [68, 69], "algorithm": [36, 61, 67, 72, 74, 80], "align": [7, 37, 45, 46, 64, 74, 87], "all": [1, 80, 88], "almost": 88, "alpaca": [8, 26], "alphacod": [24, 41, 42], "alphacodium": 43, "an": [35, 61], "analyz": 73, "annot": 75, "app": 25, "appendix": 59, "approach": [3, 4, 41, 48, 58, 72, 99], "architectur": [1, 4, 46, 47, 48, 53, 61], "arena": 7, "armorm": 56, "ascii": 102, "assess": 61, "attent": [1, 51, 52, 96], "augment": 90, "averag": 50, "awar": 94, "background": [67, 94, 95, 99], "balanc": 93, "base": [0, 53, 54, 57, 58, 80, 84], "basic": [38, 47], "batch": [67, 97], "batchnorm": 51, "benchmark": [6, 7, 11, 15, 58], "better": 73, "between": 96, "boost": 58, "bpe": 92, "byte": 92, "cach": 96, "capabl": 50, "case": [98, 99], "cell": 35, "chain": 78, "chart": 38, "chat": 50, "chatformat": 51, "citat": 34, "classif": [18, 29], "clip": [57, 67], "cluster": [41, 42], "code": [8, 10, 20, 26, 32, 43, 44, 50, 52], "codecontest": 24, "coder": [45, 54], "cold": [58, 80], "collaps": 83, "collect": [45, 49, 84], "commun": 93, "comparison": [58, 96], "composit": [28, 54], "compress": 96, "comput": 38, "concept": 43, "consider": 93, "constitut": 68, "construct": [46, 47, 75], "content": [21, 22], "context": [44, 47, 94, 95], "contextwindow": 94, "contrast": 64, "control": 50, "correct": [10, 12, 83], "count": [38, 90], "creat": [35, 82], "creation": 74, "critiqu": [58, 68], "cruxev": 9, "curat": 82, "dapo": 57, "data": [8, 18, 23, 26, 28, 29, 38, 45, 46, 47, 48, 49, 50, 54, 73, 82, 84, 87, 88, 89, 90], "dataset": [3, 4, 5, 24, 38, 41, 44, 57, 63], "decod": [1, 2], "decontamin": [28, 54], "decoupl": 96, "deepcod": 79, "deepseek": [45, 46, 47, 58, 80], "deepseekmo": 93, "deriv": 59, "design": 43, "detail": [5, 14, 27, 28, 57, 65], "determin": 37, "devic": 93, "dialog": 50, "differ": [58, 73], "direct": [34, 50, 59, 95], "discuss": 46, "distanc": 94, "diverg": 57, "divers": [73, 87], "dot": 1, "dpo": [59, 60, 71], "dpop": 60, "drop": 93, "dynam": [57, 94], "effect": 37, "effici": [61, 89, 94], "eight": 89, "elicit": 78, "embed": [1, 38, 94, 95, 96, 99], "empir": [38, 61], "encod": [1, 92], "enhanc": 67, "enn": 61, "epistem": 61, "estim": 61, "evalplu": 10, "evalu": [4, 10, 13, 14, 20, 27, 32, 37, 41, 42, 46, 47, 53, 54, 69, 75, 87], "evol": [20, 32], "evolut": 80, "exampl": 35, "experi": [3, 57, 74, 95], "experiment": [5, 37, 45, 61, 67, 74], "expert": [56, 93], "explor": 61, "extend": [94, 95], "extens": [47, 94], "extrapol": 95, "factor": [37, 90], "factori": 103, "failur": 60, "feed": 1, "feedback": [68, 69], "feedforward": [51, 52], "ffn": 100, "file": 34, "filter": [18, 29, 41, 42], "fine": [2, 24, 41, 42, 44, 45, 46, 47, 49, 53, 58, 63, 75, 80, 93], "finetun": [18, 29, 50], "flip": 73, "flow": 43, "fold": 89, "follow": [18, 29, 74], "form": [98, 99], "format": 50, "formul": [98, 99], "forward": 1, "framework": [2, 36], "frequenc": 94, "from": [14, 27, 36, 58, 64, 68, 69], "full": 37, "fullest": 73, "function": [12, 51, 52], "gate": 100, "gb2312": 102, "gbk\u7b49\u5176\u4ed6\u7f16\u7801": 102, "gener": [11, 18, 29, 37, 39, 51, 58, 84, 98, 99], "glu": 100, "gold": 36, "gpqa": 15, "gpt": [2, 63], "gpt2": 3, "gpt3": 4, "gqa": 96, "gradient": [36, 57, 59], "grain": 93, "grm": 58, "group": 62, "grpo": 62, "gsm8k": 15, "hard": 7, "harmless": 68, "head": [1, 96], "high": [5, 94], "higher": 57, "how": [37, 73], "human": [49, 73, 87], "humanev": [10, 12], "hyper": 46, "i": [1, 34, 37, 83, 87, 88], "identif": [18, 29], "ifev": 7, "ignor": 37, "ii": 83, "impact": 73, "implement": [14, 27, 65, 95], "incorpor": 64, "incorrect": 89, "infer": 58, "infil": 44, "infinit": 38, "influenc": 37, "inform": 94, "initi": [74, 75, 83], "input": [2, 3], "insight": 43, "instal": 13, "instanc": [18, 29], "instruct": [14, 18, 20, 27, 28, 29, 31, 32, 44, 53, 54, 63, 74, 75], "instructgpt": 5, "integr": 67, "interpol": [94, 95], "interpret": 56, "introduct": [14, 27, 33, 39, 44, 48, 64, 70, 95], "isol": 93, "iter": [49, 50, 62, 75], "its": 73, "joint": 96, "judgment": [64, 75], "kei": 96, "kl": [57, 67], "kv": 96, "label": [69, 73], "languag": [38, 50, 64, 73, 74, 78, 83, 90, 94, 95, 101], "larg": [41, 73, 78, 84, 90, 94, 95], "latent": 96, "law": 38, "layer": 97, "layernorm": 51, "learn": [34, 45, 46, 47, 53, 61, 63, 64, 68, 69, 80, 83, 84, 90], "less": 87, "let": 84, "level": [5, 57, 67, 93], "leverag": 73, "lima": 87, "limit": 38, "linear": 100, "livecodebench": 13, "llama": [44, 48, 49, 50, 52, 95, 103], "llama3": 51, "llm": [37, 61, 69, 89, 90, 94], "lm": [18, 29], "load": 93, "local": 94, "logic": 81, "long": [44, 47], "lora": 104, "loss": [57, 59, 90, 93, 94], "low": 96, "magicod": [14, 27], "main": [57, 68], "margin": 73, "markdown": [34, 35], "math": [15, 89, 90], "mathemat": 90, "mbpp": [10, 16], "measur": 73, "mechan": 96, "metadata": 35, "method": [5, 68, 75, 84], "methodologi": [5, 39, 63, 69], "mha": 96, "mini": 67, "mixtur": [54, 56, 93], "mla": 96, "mle": 36, "mmlu": 11, "mode": 60, "model": [1, 3, 4, 5, 38, 40, 42, 49, 50, 51, 52, 53, 54, 56, 57, 58, 61, 63, 64, 73, 74, 75, 76, 78, 80, 83, 84, 90, 94, 95, 101], "moe": 56, "moment": 80, "more": [34, 87], "mqa": 96, "multi": [1, 47, 56, 83, 96], "myst": [34, 35], "n": [38, 76], "need": [1, 56, 88], "network": [1, 61], "neural": [38, 61], "nlp\u5b9e\u4f8b": 92, "non": [37, 38], "normal": [51, 67, 97], "notebook": 35, "ntk": 94, "object": 56, "off": 36, "offlin": 53, "onli": 2, "onlin": 53, "ood": 84, "open": [14, 27], "opencod": 28, "openrlhf": [65, 105], "optim": [48, 50, 55, 59, 62], "orient": [43, 80], "orm": 84, "oss": [14, 27], "outcom": [62, 84], "overal": [42, 74], "overfit": 38, "overlong": 57, "overview": 43, "pair": [75, 92], "paramet": [38, 46], "part": 94, "passiv": 61, "pattern": 37, "penalti": 67, "perform": [38, 73, 80], "pi_": 59, "pipelin": 61, "point": 61, "polici": [36, 42, 54, 57, 62], "posit": [1, 94, 95, 96, 99], "post": [28, 47, 50, 53, 54], "postprocess": [18, 29], "potenti": [37, 73], "power": 38, "ppo": [62, 65, 66, 67], "pre": [2, 46, 47, 48, 50, 53, 54, 90], "predict": 47, "prefer": [49, 50, 55, 59, 69, 73, 76], "preliminari": [57, 59, 72, 73, 83, 93, 96, 99], "pretrain": [28, 49], "prevent": 83, "principl": 58, "prm": 84, "pro": 11, "problem": [64, 83], "process": [50, 62, 80, 84], "product": 1, "prompt": [20, 32, 69, 78], "properti": 99, "propos": [43, 99], "qualiti": [50, 58, 87], "quantiti": 87, "queri": 96, "quick": 10, "quickli": 35, "qwen": 53, "qwen2": 54, "r": [59, 71], "r1": 80, "random": 88, "rank": 96, "reason": [77, 78, 80, 82, 89, 90], "recip": 54, "redux": 11, "refer": 85, "reinforc": [45, 46, 47, 53, 63, 67, 68, 69, 80, 83], "reject": [58, 72, 80], "rel": [62, 94], "relat": 76, "relationship": 90, "remov": 57, "represent": 3, "respons": 75, "result": [5, 37, 38, 45, 46, 47, 50, 57, 58, 61, 68, 69, 74, 82], "rethink": 88, "review": 62, "revis": 68, "reward": [36, 49, 50, 56, 57, 58, 61, 63, 67, 73, 74, 80, 83, 84], "rewardbench": 17, "rl": [36, 58, 62, 63, 81, 83, 89], "rlaif": 69, "rlcd": 70, "rlhf": [49, 69, 73], "rm": [58, 63, 73], "rmsnorm": [51, 52, 97], "role": 34, "rope": [51, 52, 94, 95, 98, 99], "rotari": [94, 95, 96, 99], "round": 50, "rso": 72, "rule": [57, 58], "s1": 82, "s1k": 82, "sampl": [34, 41, 42, 57, 72, 80], "scale": [1, 37, 38, 41, 58, 82, 84, 88, 89, 90, 94], "scenario": 80, "scienc": 15, "score": [42, 83], "secret": 73, "segment": 93, "select": [75, 88], "self": [18, 29, 58, 74, 75, 76, 80, 83], "sentencepiec": 101, "set": 64, "setup": [67, 74, 83], "sft": [49, 50, 63, 86, 103, 105], "shape": [57, 83], "share": 93, "should": 37, "simpl": 82, "size": 38, "small": 84, "smooth": 73, "softmax": 1, "sourc": [14, 27, 52], "spct": 58, "special": 44, "specif": 2, "stack": 1, "stage": [28, 43, 56, 83], "standard": 96, "stanford": [8, 26], "start": [10, 58, 80], "statist": 72, "step": 84, "strategi": 93, "strength": 73, "strong": 39, "summar": 3, "supervis": [2, 45, 46, 47, 50, 53, 62, 63, 68, 80, 84, 90], "surfac": 37, "swiglu": [51, 52, 100], "swish": 100, "synthet": [84, 89], "system": 42, "taco": [19, 30], "takeawai": [45, 46, 47, 50, 53, 54, 73, 84, 98], "task": [2, 18, 29], "taught": 75, "techniqu": [69, 91], "temperatur": 37, "templat": 80, "test": 82, "thought": 78, "tiktoken": 51, "time": [38, 58, 82], "token": [47, 51, 53, 57, 67, 93], "train": [2, 3, 4, 20, 28, 32, 38, 46, 47, 48, 50, 53, 54, 57, 61, 74, 75, 76, 80, 83, 87, 90], "transform": [2, 38, 51, 52, 93], "tune": [2, 14, 24, 27, 28, 41, 42, 44, 45, 46, 47, 49, 53, 58, 63, 75, 80], "turn": 83, "two": 28, "ulm": 101, "understand": 58, "unicod": [31, 102], "unigram": 101, "unit": 100, "unpin": 58, "unsupervis": 2, "updat": 67, "utf": 102, "utf8": 102, "v": [69, 84, 90], "v2": [45, 46], "v3": 47, "valu": 96, "variant": 100, "variou": 37, "verifi": 84, "via": [83, 95], "weak": 39, "west": 76, "what": [34, 37], "why": [2, 96, 97], "window": [94, 95], "wise": 1, "wizardcod": [20, 32], "wizardlm": [20, 32], "wordpiec": 101, "work": 76, "yaml": 35, "yarn": 94, "you": [1, 88], "zero": 80, "\u4e3a\u4ec0\u4e48\u4f1a\u4e71\u7801": 102, "\u521d\u8bc6bpe": 92, "\u603b\u7ed3": 102, "\u662f\u4e0d\u662f\u8d2a\u5fc3\u7b97\u6cd5": 92, "\u672c\u5730": [10, 13], "\u7684\u8fdc\u7a0b\u8870\u51cf": 94, "\u7f16\u7801\u548c\u89e3\u7801": 92, "\u9ad8\u9891\u5916\u63a8\u4f4e\u9891\u5185\u63d2": 94}})