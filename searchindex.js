Search.setIndex({"alltitles": {"2D case": [[106, "d-case"], [107, "d-case"]], "A Recipe for Instruction Data": [[61, "a-recipe-for-instruction-data"]], "API-Bank": [[1, "api-bank"]], "APPS": [[32, "apps"]], "ASCII": [[110, "ascii"]], "ASCII,UNICODE,UTF8": [[110, "ascii-unicode-utf8"]], "Ablation of Attention Mechanisms": [[104, "ablation-of-attention-mechanisms"]], "Ablation of MHA, GQA, and MQA": [[104, "ablation-of-mha-gqa-and-mqa"]], "Ablations on Data Diversity, Quality, and Quantity": [[95, "ablations-on-data-diversity-quality-and-quantity"]], "Absolute position embedding": [[107, "absolute-position-embedding"]], "Active Exploration with a Point Estimate": [[69, "active-exploration-with-a-point-estimate"]], "Active Exploration with an ENN": [[69, "active-exploration-with-an-enn"]], "Active Learning": [[92, "active-learning"]], "Adaptive Margin": [[81, "adaptive-margin"]], "Additional insights": [[50, "additional-insights"]], "Advantage Normalization": [[75, "advantage-normalization"]], "Agent": [[0, "agent"]], "Aider Polyglot": [[11, "aider-polyglot"]], "Aligning Language Models with Judgments": [[72, "aligning-language-models-with-judgments"]], "Alignment": [[52, "alignment"], [53, "alignment"]], "Alignment Benchmarks": [[12, "alignment-benchmarks"]], "Alignment Data": [[95, "alignment-data"]], "Alignment Effect on Non-Determinism": [[44, "alignment-effect-on-non-determinism"]], "AlphaCode": [[31, "alphacode"], [48, "alphacode"]], "AlphaCode 2": [[49, "alphacode-2"]], "AlphaCodium": [[50, "alphacodium"]], "An example cell": [[42, "an-example-cell"]], "Analyze and Leverage Diverse Data to its Fullest Potential": [[81, "analyze-and-leverage-diverse-data-to-its-fullest-potential"]], "Appendix": [[67, "appendix"]], "Approach": [[7, "approach"], [8, "approach"], [48, "approach"], [55, "approach"]], "Architecture": [[53, "architecture"], [54, "architecture"], [55, "architecture"]], "Architecture & Tokenizer": [[60, "architecture-tokenizer"]], "Arena-Hard": [[12, "arena-hard"]], "ArmoRM-MoE": [[64, "armorm-moe"]], "Assessment Pipeline": [[69, "assessment-pipeline"]], "Attention": [[5, "attention"], [58, "attention"], [58, "id1"], [59, "attention"], [59, "id1"]], "Attention Is All You Need": [[5, "attention-is-all-you-need"]], "Background": [[107, "background"]], "Background: Rotary Position Embedding (RoPE)": [[102, "background-rotary-position-embedding-rope"], [103, "background-rotary-position-embedding-rope"]], "Background: The REINFORCE Algorithm": [[75, "background-the-reinforce-algorithm"]], "Base": [[4, "base"]], "Base Models": [[60, "base-models"], [61, "base-models"], [92, "base-models"]], "Basic Architecture": [[54, "basic-architecture"]], "Batch Normalization": [[105, "batch-normalization"]], "BatchNorm": [[58, "batchnorm"]], "Benchmark Construction": [[13, "benchmark-construction"], [25, "benchmark-construction"]], "Benchmark Statistics": [[13, "benchmark-statistics"]], "Benchmarking NL-Oriented Instructions to Code Generation": [[13, "benchmarking-nl-oriented-instructions-to-code-generation"]], "Benchmarks": [[10, "benchmarks"]], "BigCodeBench": [[13, "bigcodebench"]], "Boosting Reward Quality with Principles": [[66, "boosting-reward-quality-with-principles"]], "Byte Pair Encoding (BPE)": [[100, "byte-pair-encoding-bpe"]], "CRUXEval": [[15, "cruxeval"]], "Capabilities": [[57, "capabilities"]], "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models": [[86, "chain-of-thought-prompting-elicits-reasoning-in-large-language-models"]], "Charting the Infinite Data Limit and Overfitting": [[45, "charting-the-infinite-data-limit-and-overfitting"]], "Chat Dialog Format": [[57, "chat-dialog-format"]], "ChatFormat": [[58, "chatformat"]], "Citations": [[41, "citations"]], "Classification Task Identification": [[24, "classification-task-identification"], [36, "classification-task-identification"]], "Clip-Higher": [[65, "clip-higher"]], "Clustering": [[48, "clustering"], [49, "clustering"]], "Code": [[57, "code"]], "Code Alpaca": [[14, "code-alpaca"], [14, "id1"], [33, "code-alpaca"], [33, "id1"]], "Code Correctness Evaluation: HumanEval(+) or MBPP(+)": [[16, "code-correctness-evaluation-humaneval-or-mbpp"]], "Code Llama": [[51, "code-llama"]], "Code Llama: Specializing Llama 2 for code": [[51, "code-llama-specializing-llama-2-for-code"]], "Code-Oriented Design Concepts": [[50, "code-oriented-design-concepts"]], "CodeAct": [[2, "codeact"]], "CodeAct Benefits from Multi-turn Interactions and Existing Software Packages": [[2, "codeact-benefits-from-multi-turn-interactions-and-existing-software-packages"]], "CodeAct Gets More Done with Fewer Interactions": [[2, "codeact-gets-more-done-with-fewer-interactions"]], "CodeAct Makes LLMs Better Agents": [[2, "codeact-makes-llms-better-agents"]], "CodeAct Shows the Promise as a Strong Tool Use Framework": [[2, "codeact-shows-the-promise-as-a-strong-tool-use-framework"]], "CodeContests fine-tuning dataset": [[31, "codecontests-fine-tuning-dataset"]], "Cold Start": [[88, "cold-start"]], "Communication Balance Loss": [[101, "communication-balance-loss"]], "Comparison Between MLA and MHA": [[104, "comparison-between-mla-and-mha"]], "Comparison of Key-Value Cache": [[104, "comparison-of-key-value-cache"]], "Comparisons of Different RM approaches": [[66, "comparisons-of-different-rm-approaches"]], "Constitutional AI: Harmlessness from AI Feedback": [[76, "constitutional-ai-harmlessness-from-ai-feedback"]], "Contents": [[28, "contents"], [29, "contents"]], "Create a notebook with MyST Markdown": [[42, "create-a-notebook-with-myst-markdown"]], "Critiques, Revisions, and Supervised Learning": [[76, "critiques-revisions-and-supervised-learning"]], "DAPO": [[65, "dapo"], [65, "id1"]], "DPO": [[67, "dpo"]], "DPOP": [[68, "dpop"], [68, "id1"]], "Data": [[14, "data"], [30, "data"], [33, "data"]], "Data Collection": [[52, "data-collection"], [92, "data-collection"]], "Data Composition": [[35, "data-composition"], [61, "data-composition"]], "Data Construction": [[53, "data-construction"], [54, "data-construction"]], "Data Generation": [[24, "data-generation"], [36, "data-generation"]], "Data Mixture": [[61, "data-mixture"]], "Data Processing and Quality Control": [[57, "data-processing-and-quality-control"]], "Data Synthesis": [[13, "data-synthesis"]], "Dataset": [[9, "dataset"], [51, "dataset"], [65, "dataset"], [71, "dataset"]], "Datasets": [[48, "datasets"]], "Decontamination": [[35, "decontamination"], [61, "decontamination"]], "Decoupled Rotary Position Embedding": [[104, "decoupled-rotary-position-embedding"]], "DeepCoder": [[87, "deepcoder"]], "DeepSeek V3": [[54, "deepseek-v3"]], "DeepSeek-Coder-V2": [[52, "deepseek-coder-v2"]], "DeepSeek-GRM": [[66, "deepseek-grm"]], "DeepSeek-R1": [[88, "deepseek-r1"]], "DeepSeek-R1-Zero: Reinforcement Learning on the Base Model": [[88, "deepseek-r1-zero-reinforcement-learning-on-the-base-model"]], "DeepSeek-R1: Reinforcement Learning with Cold Start": [[88, "deepseek-r1-reinforcement-learning-with-cold-start"]], "DeepSeek-V2": [[53, "deepseek-v2"]], "DeepSeekMoE": [[101, "deepseekmoe"]], "Derivation of \\pi_{r}": [[67, "derivation-of-pi-r"]], "Design Principles of API-Bank": [[1, "design-principles-of-api-bank"]], "Device-Level Balance Loss": [[101, "device-level-balance-loss"]], "Direct Preference Optimization": [[57, "direct-preference-optimization"], [67, "direct-preference-optimization"]], "Direct extrapolation": [[103, "direct-extrapolation"]], "Discussion": [[53, "discussion"]], "Dynamic Sampling": [[65, "dynamic-sampling"]], "Dynamic Scaling - \u201cDynamic NTK\u201d interpolation": [[102, "dynamic-scaling-dynamic-ntk-interpolation"]], "Efficient Exploration for LLMs": [[69, "efficient-exploration-for-llms"]], "Embeddings and Softmax": [[5, "embeddings-and-softmax"]], "Empirical Results": [[69, "empirical-results"]], "Empirical Results and Basic Power Laws": [[45, "empirical-results-and-basic-power-laws"]], "Encoder and Decoder Stacks": [[5, "encoder-and-decoder-stacks"]], "Epistemic Neural Network": [[69, "epistemic-neural-network"]], "EvalPlus": [[16, "evalplus"]], "Evaluation": [[8, "evaluation"], [13, "evaluation"], [20, "evaluation"], [27, "evaluation"], [34, "evaluation"], [39, "evaluation"], [48, "evaluation"], [49, "evaluation"], [60, "evaluation"], [61, "evaluation"], [77, "evaluation"]], "Evaluation Results": [[53, "evaluation-results"], [53, "id3"], [54, "evaluation-results"], [54, "id4"]], "Evaluation System of API-Bank": [[1, "evaluation-system-of-api-bank"]], "Evaluation of LLMs Should Not Ignore Non-Determinism": [[44, "evaluation-of-llms-should-not-ignore-non-determinism"]], "Evol-Instruct": [[27, "evol-instruct"], [39, "evol-instruct"]], "Evol-Instruct Prompts for Code": [[27, "evol-instruct-prompts-for-code"], [39, "evol-instruct-prompts-for-code"]], "Experimental Results": [[44, "experimental-results"], [52, "experimental-results"]], "Experimental Setup": [[25, "experimental-setup"], [75, "experimental-setup"], [82, "experimental-setup"]], "Experimentation Pipeline": [[69, "experimentation-pipeline"]], "Experiments": [[7, "experiments"], [65, "experiments"], [82, "experiments"], [103, "experiments"]], "Expert-Level Balance Loss": [[101, "expert-level-balance-loss"]], "Exploration Algorithms": [[69, "exploration-algorithms"]], "Extending context window of LLMs": [[102, "extending-context-window-of-llms"]], "Extending context window of large language models via position interpolation": [[103, "extending-context-window-of-large-language-models-via-position-interpolation"]], "FFN": [[108, "ffn"]], "Failure Mode of DPO": [[68, "failure-mode-of-dpo"]], "Features of Swe-Bench": [[25, "features-of-swe-bench"]], "FeedForward": [[58, "feedforward"], [59, "feedforward"]], "Filtering": [[48, "filtering"], [49, "filtering"]], "Filtering and Postprocessing": [[24, "filtering-and-postprocessing"], [36, "filtering-and-postprocessing"]], "Fine-Grained Expert Segmentation": [[101, "fine-grained-expert-segmentation"]], "Fine-tuning": [[48, "fine-tuning"]], "Finetuning the LM to Follow Instructions": [[24, "finetuning-the-lm-to-follow-instructions"], [36, "finetuning-the-lm-to-follow-instructions"]], "Flipping the Labels": [[81, "flipping-the-labels"]], "Flow stages": [[50, "flow-stages"]], "Formulation": [[106, "formulation"], [107, "formulation"]], "Framework": [[6, "framework"]], "From MLE to RL framework": [[43, "from-mle-to-rl-framework"]], "Functional Correctness": [[18, "functional-correctness"]], "GB2312\u3001GBK\u7b49\u5176\u4ed6\u7f16\u7801": [[110, "gb2312gbk"]], "GOLD": [[43, "gold"]], "GPQA": [[21, "gpqa"]], "GPT": [[6, "gpt"]], "GPT2": [[7, "gpt2"]], "GPT3": [[8, "gpt3"]], "GRPO": [[70, "id3"]], "GSM8K": [[21, "gsm8k"]], "Gated Linear Units (GLU) and Variants": [[108, "gated-linear-units-glu-and-variants"]], "General Benchmarks": [[17, "general-benchmarks"]], "General form": [[106, "general-form"], [107, "general-form"]], "Generation": [[58, "generation"]], "Generator": [[92, "generator"]], "Gradient of DPO Loss": [[67, "gradient-of-dpo-loss"]], "Group Relative Policy Optimization": [[70, "group-relative-policy-optimization"]], "High-level methodology": [[9, "high-level-methodology"]], "How Various Factors Influence Non-Determinism?": [[44, "how-various-factors-influence-non-determinism"]], "How to Better Model Human Preference?": [[81, "how-to-better-model-human-preference"]], "Human Curation": [[13, "human-curation"]], "Human Evaluation": [[95, "human-evaluation"]], "Human Preference Data Collection": [[56, "human-preference-data-collection"]], "HumanEval": [[18, "humaneval"]], "Hyper-Parameters": [[53, "hyper-parameters"]], "IFEval": [[12, "ifeval"]], "Impacts of Different Data on RM Performance": [[81, "impacts-of-different-data-on-rm-performance"]], "Implementation Details": [[20, "implementation-details"], [34, "implementation-details"]], "Incorporating Judgments for Alignment": [[72, "incorporating-judgments-for-alignment"]], "Inference-Time Scaling with SPCT": [[66, "inference-time-scaling-with-spct"]], "Infilling": [[51, "infilling"]], "Initialization": [[82, "initialization"], [83, "initialization"]], "Input Format": [[25, "input-format"]], "Input Representation": [[7, "input-representation"]], "Installation": [[19, "installation"]], "Instance Generation": [[24, "instance-generation"], [36, "instance-generation"]], "Instruct GPT": [[71, "instruct-gpt"]], "Instruct Models": [[61, "instruct-models"]], "InstructGPT": [[9, "instructgpt"]], "Instruction Following Ability Results": [[82, "instruction-following-ability-results"]], "Instruction Following Training": [[82, "instruction-following-training"]], "Instruction Generation": [[24, "instruction-generation"], [36, "instruction-generation"]], "Instruction Selection": [[83, "instruction-selection"]], "Instruction fine-tuning": [[51, "instruction-fine-tuning"]], "Instruction-tuned Model": [[60, "instruction-tuned-model"]], "Introduction": [[20, "introduction"], [34, "introduction"], [40, "introduction"], [46, "introduction"], [51, "introduction"], [55, "introduction"], [72, "introduction"], [78, "introduction"], [103, "introduction"]], "Iterative Fine-Tuning": [[56, "iterative-fine-tuning"]], "Iterative RL with GRPO": [[70, "iterative-rl-with-grpo"]], "Iterative Rounds": [[57, "iterative-rounds"]], "Iterative Training": [[83, "iterative-training"]], "Judgment Annotation": [[83, "judgment-annotation"]], "LIMA: Less Is More for Alignment": [[95, "lima-less-is-more-for-alignment"]], "LORA": [[112, "lora"]], "Label Smoothing": [[81, "label-smoothing"]], "Large scale sampling": [[48, "large-scale-sampling"]], "Large-scale Supervision": [[92, "large-scale-supervision"]], "Layer Normalization": [[105, "layer-normalization"]], "LayerNorm": [[58, "layernorm"]], "Learn more": [[41, "learn-more"]], "Learning Pipeline": [[69, "learning-pipeline"]], "Learning from Contrasting": [[72, "learning-from-contrasting"]], "Let\u2019s Verify Step by Step": [[92, "lets-verify-step-by-step"]], "LiveCodeBench": [[19, "livecodebench"]], "Llama": [[55, "llama"]], "Llama 2": [[56, "llama-2"]], "Llama 3": [[57, "llama-3"]], "Llama 3 Source Code": [[59, "llama-3-source-code"]], "Llama Factory": [[111, "llama-factory"]], "Llama implementation": [[103, "llama-implementation"]], "Llama3": [[58, "llama3"]], "Load Balance Consideration": [[101, "load-balance-consideration"]], "Logic-RL": [[89, "logic-rl"]], "Long Context Extension": [[54, "long-context-extension"]], "Long context fine-tuning": [[51, "long-context-fine-tuning"]], "Loss": [[67, "loss"]], "Loss of High Frequency information - \u201cNTK-aware\u201d interpolation": [[102, "loss-of-high-frequency-information-ntk-aware-interpolation"]], "Loss of Relative Local Distances - \u201cNTK-by-parts\u201d interpolation": [[102, "loss-of-relative-local-distances-ntk-by-parts-interpolation"]], "Low-Rank Compression for Queries": [[104, "low-rank-compression-for-queries"]], "Low-Rank Key-Value Joint Compression": [[104, "low-rank-key-value-joint-compression"]], "MATH": [[21, "math"]], "MATH 500": [[21, "math-500"]], "MBPP": [[22, "mbpp"]], "MMLU": [[17, "mmlu"]], "MMLU-Pro": [[17, "mmlu-pro"]], "MMLU-Redux": [[17, "mmlu-redux"]], "Magicoder": [[20, "magicoder"], [34, "magicoder"]], "Main Result": [[76, "main-result"]], "Main Results": [[65, "main-results"], [76, "main-results"]], "Markdown Files": [[41, "markdown-files"]], "Math & Science Benchmarks": [[21, "math-science-benchmarks"]], "Measuring the Strength of Preferences": [[81, "measuring-the-strength-of-preferences"]], "Method": [[76, "method"], [76, "id1"], [83, "method"]], "Methodology": [[46, "methodology"], [77, "methodology"]], "Methods": [[92, "methods"]], "Methods and experimental details": [[9, "methods-and-experimental-details"]], "Mini-Batch Updates": [[75, "mini-batch-updates"]], "Model": [[7, "model"], [58, "model"], [59, "model"]], "Model Accuracy VS. Augmented Data Count": [[98, "model-accuracy-vs-augmented-data-count"]], "Model Accuracy VS. Pre-training Loss": [[98, "model-accuracy-vs-pre-training-loss"]], "Model Accuracy VS. Supervised Data Count": [[98, "model-accuracy-vs-supervised-data-count"]], "Model Architecture": [[5, "model-architecture"]], "Model Averaging": [[57, "model-averaging"]], "Model Fine-tuning (Iterative Training)": [[83, "model-fine-tuning-iterative-training"]], "Model and Architectures": [[8, "model-and-architectures"]], "Modeling": [[57, "modeling"]], "Models": [[9, "models"], [47, "models"]], "Multi-Head Attention": [[5, "multi-head-attention"]], "Multi-Head Latent Attention": [[104, "multi-head-latent-attention"]], "Multi-Token Prediction": [[54, "multi-token-prediction"]], "NLP\u5b9e\u4f8b": [[100, "nlp"]], "Normalization": [[58, "normalization"], [105, "normalization"]], "Notebooks with MyST Markdown": [[42, "notebooks-with-myst-markdown"]], "OOD Generalization": [[92, "ood-generalization"]], "OSS-INSTRUCT: Instruction Tuning from Open Source": [[20, "oss-instruct-instruction-tuning-from-open-source"], [34, "oss-instruct-instruction-tuning-from-open-source"]], "Off-policy policy gradient": [[43, "off-policy-policy-gradient"]], "Offline Reinforcement Learning": [[60, "offline-reinforcement-learning"]], "Online Reinforcement Learning": [[60, "online-reinforcement-learning"]], "OpenCoder": [[35, "opencoder"]], "OpenRLHF": [[73, "openrlhf"], [113, "openrlhf"]], "Optimizer": [[55, "optimizer"]], "Outcome Supervision RL with GRPO": [[70, "outcome-supervision-rl-with-grpo"]], "Outcome-supervised Reward Models (ORMs)": [[92, "outcome-supervised-reward-models-orms"]], "Overall Self-Alignment Algorithm": [[82, "overall-self-alignment-algorithm"]], "Overall System": [[49, "overall-system"]], "Overlong Reward Shaping": [[65, "overlong-reward-shaping"]], "Overview": [[50, "overview"]], "PPO": [[74, "ppo"]], "PPO Review": [[70, "ppo-review"]], "PPO implementation detail": [[73, "ppo-implementation-detail"]], "PPO-Clip Integration": [[75, "ppo-clip-integration"]], "Parameter and Compute Scaling of Transformers": [[45, "parameter-and-compute-scaling-of-transformers"]], "Passive Exploration": [[69, "passive-exploration"]], "Performance with Dataset Size and Compute": [[45, "performance-with-dataset-size-and-compute"]], "Performance with Non-Embedding Parameter Count N": [[45, "performance-with-non-embedding-parameter-count-n"]], "Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero": [[88, "performance-self-evolution-process-and-aha-moment-of-deepseek-r1-zero"]], "Point Estimate": [[69, "point-estimate"]], "Policy and Fine-Tuning": [[49, "policy-and-fine-tuning"]], "Position interpolation": [[102, "position-interpolation"]], "Position-wise Feed-Forward Networks": [[5, "position-wise-feed-forward-networks"]], "Positional Encoding": [[5, "positional-encoding"]], "Positional Interpolation": [[103, "positional-interpolation"]], "Post Training": [[35, "post-training"]], "Post-Training": [[54, "post-training"], [57, "post-training"]], "Post-trained Language Model": [[57, "post-trained-language-model"]], "Post-training": [[60, "post-training"], [61, "post-training"]], "Post-training Data": [[57, "post-training-data"]], "Pre-Training": [[53, "pre-training"], [54, "pre-training"]], "Pre-trained Language Model": [[57, "pre-trained-language-model"]], "Pre-training": [[60, "pre-training"], [61, "pre-training"]], "Pre-training data": [[55, "pre-training-data"]], "Preference Data": [[57, "preference-data"]], "Preference Labeling with LLMs": [[77, "preference-labeling-with-llms"]], "Preference Optimization": [[63, "preference-optimization"]], "Preliminaries": [[67, "preliminaries"], [80, "preliminaries"], [81, "preliminaries"]], "Preliminaries and Problem Setup": [[91, "preliminaries-and-problem-setup"]], "Preliminaries: Mixture-of-Experts for Transformers": [[101, "preliminaries-mixture-of-experts-for-transformers"]], "Preliminaries: Standard Multi-Head Attention": [[104, "preliminaries-standard-multi-head-attention"]], "Preliminary": [[65, "preliminary"], [107, "preliminary"]], "Pretrain": [[56, "pretrain"]], "Pretraining": [[35, "pretraining"]], "Pretraining Data": [[35, "pretraining-data"]], "Problem Setting": [[72, "problem-setting"]], "Process Supervision RL with GRPO": [[70, "process-supervision-rl-with-grpo"]], "Process vs Outcome Supervision": [[92, "process-vs-outcome-supervision"]], "Process-supervised Reward Models (PRMs)": [[92, "process-supervised-reward-models-prms"]], "Prompting Techniques": [[77, "prompting-techniques"]], "Properties of RoPE": [[107, "properties-of-rope"]], "Proposed approach": [[107, "proposed-approach"]], "Quick Start": [[16, "quick-start"]], "Quickly add YAML metadata for MyST Notebooks": [[42, "quickly-add-yaml-metadata-for-myst-notebooks"]], "Qwen 2.5": [[60, "qwen-2-5"]], "Qwen2.5-Coder": [[61, "qwen2-5-coder"]], "Qwen3": [[62, "qwen3"]], "REINFORCE++": [[75, "reinforce"]], "REINFORCE++ Enhancements": [[75, "reinforce-enhancements"]], "RETHINKING DATA SELECTION AT SCALE: RANDOM SELECTION IS ALMOST ALL YOU NEED": [[96, "rethinking-data-selection-at-scale-random-selection-is-almost-all-you-need"]], "RL on Incorrect Synthetic Data Scales the Efficiency of LLM Math Reasoning by Eight-Fold": [[97, "rl-on-incorrect-synthetic-data-scales-the-efficiency-of-llm-math-reasoning-by-eight-fold"]], "RLAIF vs. RLHF": [[77, "rlaif-vs-rlhf"], [77, "id1"]], "RLCD": [[78, "rlcd"], [78, "id1"]], "RLHF": [[56, "rlhf"]], "RMSNorm": [[58, "rmsnorm"], [59, "rmsnorm"], [105, "rmsnorm"]], "RS-DPO": [[79, "rs-dpo"]], "RSO": [[80, "rso"]], "RSO APPROACH": [[80, "rso-approach"]], "Reasoning": [[85, "reasoning"]], "Reasoning data curation to create s1K": [[90, "reasoning-data-curation-to-create-s1k"]], "Reasoning-oriented Reinforcement Learning": [[88, "reasoning-oriented-reinforcement-learning"]], "References": [[93, "references"]], "Reinforcement Learning": [[52, "reinforcement-learning"], [53, "reinforcement-learning"], [54, "reinforcement-learning"]], "Reinforcement Learning Algorithm": [[88, "reinforcement-learning-algorithm"]], "Reinforcement Learning for all Scenarios": [[88, "reinforcement-learning-for-all-scenarios"]], "Reinforcement Learning from AI Feedback": [[76, "reinforcement-learning-from-ai-feedback"], [77, "reinforcement-learning-from-ai-feedback"]], "Reinforcement learning (RL)": [[71, "reinforcement-learning-rl"]], "Rejection Sampling and Supervised Fine-Tuning": [[88, "rejection-sampling-and-supervised-fine-tuning"]], "Rejective Fine-Tuning (Cold Start)": [[66, "rejective-fine-tuning-cold-start"]], "Related Work": [[84, "related-work"]], "Removing KL Divergence": [[65, "removing-kl-divergence"]], "Response Pair Construction": [[83, "response-pair-construction"]], "Results": [[9, "results"], [25, "results"], [57, "results"], [77, "results"], [90, "results"]], "Results on Reward Modeling Benchmarks": [[66, "results-on-reward-modeling-benchmarks"]], "Retrieval-Based Approach": [[25, "retrieval-based-approach"]], "Reward": [[43, "reward"]], "Reward Model Architectures and Training": [[69, "reward-model-architectures-and-training"]], "Reward Modeling": [[56, "reward-modeling"], [57, "reward-modeling"], [88, "reward-modeling"]], "Reward Modeling Ability Results": [[82, "reward-modeling-ability-results"]], "Reward Normalization and Clipping": [[75, "reward-normalization-and-clipping"]], "Reward modeling (RM)": [[71, "reward-modeling-rm"]], "RewardBench": [[23, "rewardbench"]], "RoPE": [[58, "rope"], [59, "rope"], [106, "rope"]], "RoPE \u7684\u8fdc\u7a0b\u8870\u51cf": [[102, "rope"]], "Rotary Positional Embeddings (RoPE)": [[107, "rotary-positional-embeddings-rope"]], "Rule-Based RL": [[66, "rule-based-rl"]], "Rule-based Reward Modeling": [[65, "rule-based-reward-modeling"]], "SCoRe: Self-Correction via Multi-Turn Reinforcement Learning": [[91, "score-self-correction-via-multi-turn-reinforcement-learning"]], "SELF-INSTRUCT": [[24, "self-instruct"], [36, "self-instruct"]], "SFT": [[56, "sft"], [94, "sft"], [111, "sft"], [113, "sft"]], "SFT Data": [[57, "sft-data"]], "STATISTICAL REJECTION SAMPLING ALGORITHM": [[80, "statistical-rejection-sampling-algorithm"]], "SWE-Llama: Fine-Tuning Codellama for SWE-bench": [[25, "swe-llama-fine-tuning-codellama-for-swe-bench"]], "SWE-bench": [[25, "swe-bench"], [25, "id1"]], "SWE-bench Lite": [[25, "swe-bench-lite"]], "Sample Roles and Directives": [[41, "sample-roles-and-directives"]], "Sampling": [[49, "sampling"]], "Scaled Dot-Product Attention": [[5, "scaled-dot-product-attention"]], "Scaling Effect on Non-Determinism": [[44, "scaling-effect-on-non-determinism"]], "Scaling Laws for Neural Language Models": [[45, "scaling-laws-for-neural-language-models"]], "Scaling Laws with Model Size and Training Time": [[45, "scaling-laws-with-model-size-and-training-time"]], "Scaling Relationship on Learning Mathematical Reasoning with Large Language Models": [[98, "scaling-relationship-on-learning-mathematical-reasoning-with-large-language-models"]], "Scoring Model": [[49, "scoring-model"]], "Secrets of RLHF in Large Language Models: Reward Modeling": [[81, "secrets-of-rlhf-in-large-language-models-reward-modeling"]], "Self-Instruction Creation": [[82, "self-instruction-creation"]], "Self-Principled Critique Tuning (SPCT)": [[66, "self-principled-critique-tuning-spct"]], "Self-Rewarding Language Models": [[82, "self-rewarding-language-models"]], "Self-Taught Evaluators": [[83, "self-taught-evaluators"]], "Self-Training for Preference Modeling": [[84, "self-training-for-preference-modeling"]], "Semi-Automatic Program Refactoring and Testing Case Generation": [[13, "semi-automatic-program-refactoring-and-testing-case-generation"]], "SentencePiece": [[109, "sentencepiece"]], "Shared Expert Isolation": [[101, "shared-expert-isolation"]], "Small-scale Synthetic Supervision": [[92, "small-scale-synthetic-supervision"]], "Stage I: Training a Model Initialization to Prevent Collapse": [[91, "stage-i-training-a-model-initialization-to-prevent-collapse"]], "Stage II: Multi-Turn RL with Reward Shaping": [[91, "stage-ii-multi-turn-rl-with-reward-shaping"]], "Stage-1: Multi-Objective Reward Modeling": [[64, "stage-1-multi-objective-reward-modeling"]], "Stage-2: Mixture-of-Experts Aggregation of Reward Objectives": [[64, "stage-2-mixture-of-experts-aggregation-of-reward-objectives"]], "Stanford Alpaca": [[14, "stanford-alpaca"], [33, "stanford-alpaca"]], "Summarization": [[7, "summarization"]], "Supervised Fine-Tuning": [[52, "supervised-fine-tuning"], [53, "supervised-fine-tuning"], [54, "supervised-fine-tuning"]], "Supervised Fine-tuning": [[60, "supervised-fine-tuning"]], "Supervised Finetuning": [[57, "supervised-finetuning"]], "Supervised fine-tuning": [[6, "supervised-fine-tuning"]], "Supervised fine-tuning (SFT)": [[71, "supervised-fine-tuning-sft"]], "Surface Patterns in Non-Determinism Generation?": [[44, "surface-patterns-in-non-determinism-generation"]], "SwiGLU": [[108, "swiglu"]], "SwiGLU activation function": [[58, "swiglu-activation-function"], [59, "swiglu-activation-function"]], "Swish": [[108, "swish"]], "TACO": [[26, "taco"], [37, "taco"]], "Takeaway": [[52, "takeaway"], [53, "takeaway"], [54, "takeaway"], [57, "takeaway"], [60, "takeaway"], [61, "takeaway"], [106, "takeaway"]], "Takeaways": [[81, "takeaways"], [92, "takeaways"]], "Task-specific input transformations": [[6, "task-specific-input-transformations"]], "Techniques": [[99, "techniques"]], "Temperature Effect on Non-Determinism": [[44, "temperature-effect-on-non-determinism"]], "Test-time scaling": [[90, "test-time-scaling"]], "The Factors of Math Reasoning Ability in Supervised LLM": [[98, "the-factors-of-math-reasoning-ability-in-supervised-llm"]], "The GOLD algorithm": [[43, "the-gold-algorithm"]], "The Need for Interpretable Reward Models": [[64, "the-need-for-interpretable-reward-models"]], "The Proposed Flow": [[50, "the-proposed-flow"]], "Tiktoken": [[58, "tiktoken"]], "Token-Dropping Strategy": [[101, "token-dropping-strategy"]], "Token-Level KL Penalty": [[75, "token-level-kl-penalty"]], "Token-Level Policy Gradient Loss": [[65, "token-level-policy-gradient-loss"]], "Tokenizer": [[58, "tokenizer"], [58, "id2"]], "Training Dataset": [[7, "training-dataset"], [8, "training-dataset"]], "Training Details": [[35, "training-details"], [65, "training-details"]], "Training LIMA": [[95, "training-lima"]], "Training Language Models to Self-Correct via Reinforcement Learning": [[91, "training-language-models-to-self-correct-via-reinforcement-learning"]], "Training Policy": [[61, "training-policy"], [61, "id8"]], "Training Template": [[88, "training-template"]], "Training WizardCoder": [[27, "training-wizardcoder"], [39, "training-wizardcoder"]], "Transformer": [[58, "transformer"], [59, "transformer"]], "Two-stage Instruction-Tuning": [[35, "two-stage-instruction-tuning"]], "UNICODER": [[38, "unicoder"], [38, "id2"]], "UNICODER-INSTRUCT": [[38, "unicoder-instruct"]], "UTF-16\u3001UTF-32\u7b49": [[110, "utf-16utf-32"]], "UTF-8": [[110, "utf-8"]], "Unigram Language Model (ULM)": [[109, "unigram-language-model-ulm"]], "Unpinning Principles from Understanding to Generation": [[66, "unpinning-principles-from-understanding-to-generation"]], "Unsupervised pre-training": [[6, "unsupervised-pre-training"]], "Weak to Strong Generalization": [[46, "weak-to-strong-generalization"]], "West-of-N": [[84, "west-of-n"]], "West-of-N Self-Training": [[84, "west-of-n-self-training"]], "What is CodeAct?": [[2, "what-is-codeact"]], "What is MyST?": [[41, "what-is-myst"]], "What is the Full Potential of Non-Determinism?": [[44, "what-is-the-full-potential-of-non-determinism"]], "Why KV cache": [[104, "why-kv-cache"]], "Why Layer Normalization": [[105, "why-layer-normalization"]], "Why decoder-only": [[6, "why-decoder-only"]], "WizardCoder": [[27, "wizardcoder"], [39, "wizardcoder"]], "WizardLM": [[27, "wizardlm"], [39, "wizardlm"]], "WordPiece": [[109, "wordpiece"]], "WordPiece, ULM and SentencePiece": [[109, "wordpiece-ulm-and-sentencepiece"]], "YaRN": [[102, "id5"]], "YaRN: Efficient ContextWindow Extension of Large Language Models": [[102, "yarn-efficient-contextwindow-extension-of-large-language-models"]], "methodology": [[71, "methodology"]], "s1: Simple test-time scaling": [[90, "s1-simple-test-time-scaling"]], "unicode": [[110, "unicode"]], "\u4e3a\u4ec0\u4e48\u4f1a\u4e71\u7801": [[110, "id1"]], "\u521d\u8bc6BPE": [[100, "bpe"]], "\u603b\u7ed3": [[110, "id2"]], "\u662f\u4e0d\u662f\u8d2a\u5fc3\u7b97\u6cd5\uff1f": [[100, "id2"]], "\u672c\u5730 Evaluate": [[16, "evaluate"], [19, "evaluate"]], "\u7f16\u7801\u548c\u89e3\u7801": [[100, "id1"]], "\u9ad8\u9891\u5916\u63a8\u4f4e\u9891\u5185\u63d2": [[102, "id4"]]}, "docnames": ["agent/0", "agent/api-bank", "agent/code-act", "agent/swe-agent", "base/0", "base/attention", "base/gpt", "base/gpt2", "base/gpt3", "base/instruct-gpt", "bench/0", "bench/aider", "bench/alignment", "bench/bigcodebench", "bench/code-alpaca", "bench/cruxeval", "bench/evalplus", "bench/general", "bench/humaneval", "bench/livecodebench", "bench/magic", "bench/math-science", "bench/mbpp", "bench/reward-bench", "bench/self-instruct", "bench/swe", "bench/taco", "bench/wizard", "content", "content-Copy1", "data/0", "data/alphacode", "data/apps", "data/code-alpaca", "data/magic", "data/opencoder", "data/self-instruct", "data/taco", "data/unicoder", "data/wizard", "intro", "markdown", "markdown-notebooks", "methodology/gold", "methodology/non-determin", "methodology/scaling-law", "methodology/weak-to-strong", "models/0", "models/alphacode", "models/alphacode2", "models/alphacodium", "models/codellama", "models/deepseek-coder-v2", "models/deepseek-v2", "models/deepseek-v3", "models/llama", "models/llama2", "models/llama3", "models/llama3-code copy", "models/llama3-source-code", "models/qwen25", "models/qwen25-coder", "models/qwen3", "preference/0", "preference/armo", "preference/dapo", "preference/deepseek-grm", "preference/dpo", "preference/dpop", "preference/ee", "preference/grpo", "preference/instruct-gpt", "preference/judge", "preference/openrlhf", "preference/ppo", "preference/reinforce++", "preference/rlaif-1", "preference/rlaif-2", "preference/rlcd", "preference/rs-dpo", "preference/rso", "preference/secret-rm", "preference/self-reward", "preference/self-taught", "preference/west-of-n", "reasoning/0", "reasoning/cot", "reasoning/deepcoder", "reasoning/deepseek-r1", "reasoning/logic-rl", "reasoning/s1", "reasoning/self-correct-rl", "reasoning/verify", "reference", "sft/0", "sft/lima", "sft/random", "sft/rl-eight-fold", "sft/rs", "techniques/0", "techniques/bpe", "techniques/deepseek-moe", "techniques/extending", "techniques/extending-old", "techniques/mla", "techniques/norm", "techniques/rope", "techniques/rope-old", "techniques/swiglu", "techniques/tokenize", "techniques/unicode-utf8", "tools/llama-factory", "tools/lora", "tools/openrlhf"], "envversion": {"sphinx": 61, "sphinx.domains.c": 3, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 9, "sphinx.domains.index": 1, "sphinx.domains.javascript": 3, "sphinx.domains.math": 2, "sphinx.domains.python": 4, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.intersphinx": 1, "sphinxcontrib.bibtex": 9}, "filenames": ["agent/0.ipynb", "agent/api-bank.ipynb", "agent/code-act.ipynb", "agent/swe-agent.ipynb", "base/0.ipynb", "base/attention.ipynb", "base/gpt.ipynb", "base/gpt2.ipynb", "base/gpt3.ipynb", "base/instruct-gpt.ipynb", "bench/0.ipynb", "bench/aider.ipynb", "bench/alignment.ipynb", "bench/bigcodebench.ipynb", "bench/code-alpaca.ipynb", "bench/cruxeval.ipynb", "bench/evalplus.ipynb", "bench/general.ipynb", "bench/humaneval.ipynb", "bench/livecodebench.ipynb", "bench/magic.ipynb", "bench/math-science.ipynb", "bench/mbpp.ipynb", "bench/reward-bench.ipynb", "bench/self-instruct.ipynb", "bench/swe.ipynb", "bench/taco.ipynb", "bench/wizard.ipynb", "content.ipynb", "content-Copy1.ipynb", "data/0.ipynb", "data/alphacode.ipynb", "data/apps.ipynb", "data/code-alpaca.ipynb", "data/magic.ipynb", "data/opencoder.ipynb", "data/self-instruct.ipynb", "data/taco.ipynb", "data/unicoder.ipynb", "data/wizard.ipynb", "intro.md", "markdown.md", "markdown-notebooks.md", "methodology/gold.ipynb", "methodology/non-determin.ipynb", "methodology/scaling-law.ipynb", "methodology/weak-to-strong.ipynb", "models/0.ipynb", "models/alphacode.ipynb", "models/alphacode2.ipynb", "models/alphacodium.ipynb", "models/codellama.ipynb", "models/deepseek-coder-v2.ipynb", "models/deepseek-v2.ipynb", "models/deepseek-v3.ipynb", "models/llama.ipynb", "models/llama2.ipynb", "models/llama3.ipynb", "models/llama3-code copy.ipynb", "models/llama3-source-code.ipynb", "models/qwen25.ipynb", "models/qwen25-coder.ipynb", "models/qwen3.ipynb", "preference/0.ipynb", "preference/armo.ipynb", "preference/dapo.ipynb", "preference/deepseek-grm.ipynb", "preference/dpo.ipynb", "preference/dpop.ipynb", "preference/ee.ipynb", "preference/grpo.ipynb", "preference/instruct-gpt.ipynb", "preference/judge.ipynb", "preference/openrlhf.ipynb", "preference/ppo.ipynb", "preference/reinforce++.ipynb", "preference/rlaif-1.ipynb", "preference/rlaif-2.ipynb", "preference/rlcd.ipynb", "preference/rs-dpo.ipynb", "preference/rso.ipynb", "preference/secret-rm.ipynb", "preference/self-reward.ipynb", "preference/self-taught.ipynb", "preference/west-of-n.ipynb", "reasoning/0.ipynb", "reasoning/cot.ipynb", "reasoning/deepcoder.ipynb", "reasoning/deepseek-r1.ipynb", "reasoning/logic-rl.ipynb", "reasoning/s1.ipynb", "reasoning/self-correct-rl.ipynb", "reasoning/verify.ipynb", "reference.ipynb", "sft/0.ipynb", "sft/lima.ipynb", "sft/random.ipynb", "sft/rl-eight-fold.ipynb", "sft/rs.ipynb", "techniques/0.ipynb", "techniques/bpe.ipynb", "techniques/deepseek-moe.ipynb", "techniques/extending.ipynb", "techniques/extending-old.ipynb", "techniques/mla.ipynb", "techniques/norm.ipynb", "techniques/rope.ipynb", "techniques/rope-old.ipynb", "techniques/swiglu.ipynb", "techniques/tokenize.ipynb", "techniques/unicode-utf8.ipynb", "tools/llama-factory.ipynb", "tools/lora.ipynb", "tools/openrlhf.ipynb"], "indexentries": {}, "objects": {}, "objnames": {}, "objtypes": {}, "terms": {"": [6, 7, 9, 13, 16, 17, 21, 35, 41, 42, 43, 45, 46, 50, 51, 53, 56, 57, 58, 60, 61, 64, 65, 67, 68, 69, 70, 71, 75, 76, 77, 78, 80, 81, 83, 90, 91, 93, 98, 100, 101, 102, 103, 109], "0": [5, 6, 16, 18, 19, 24, 27, 35, 36, 39, 43, 44, 45, 48, 49, 52, 53, 54, 55, 56, 57, 58, 59, 64, 65, 67, 68, 70, 72, 73, 76, 77, 80, 81, 82, 84, 95, 98, 101, 102, 103, 105, 106, 107, 108, 110, 111, 112], "000": [1, 17, 21, 25, 26, 32, 37, 48, 51, 90, 95], "0000": [103, 110], "0000j": 103, "0001": 110, "0010": 110, "003": [14, 33], "0041": 110, "005": 18, "007f": 110, "0080": 110, "01": [65, 73], "0100j": 103, "012": 65, "01825": [41, 93], "02120": [20, 34, 41, 93], "02155": [41, 93], "02954": [41, 93], "03": 55, "03065": [41, 93], "0314": 12, "03300": [41, 93], "03341": [41, 93], "03374": [41, 93], "03762": [41, 93], "04434": [41, 93], "0461": 103, "04805": [41, 93], "0596": 103, "0596j": 103, "06": 55, "0674": 103, "0674j": 103, "07436": [41, 93], "076": 45, "07911": [41, 93], "07974": [41, 93], "07ff": 110, "08": 65, "0800": 110, "08083": [41, 93], "08361": [41, 93], "08568": [27, 39], "096": [51, 65], "09864": [41, 93], "0xxxxxxx": 110, "1": [1, 5, 6, 9, 12, 14, 16, 18, 19, 20, 24, 25, 26, 27, 29, 33, 34, 35, 36, 37, 38, 39, 41, 43, 44, 45, 48, 49, 51, 52, 53, 54, 55, 57, 58, 59, 60, 61, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 80, 81, 82, 83, 84, 90, 91, 92, 93, 95, 98, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111], "10": [2, 16, 19, 21, 32, 35, 44, 45, 49, 51, 52, 53, 55, 57, 61, 65, 70, 77, 81, 100, 101, 110, 111], "100": [7, 14, 17, 18, 33, 35, 49, 51, 61, 64, 69, 92], "1000": [27, 39, 54, 92, 102, 103], "10000": [5, 49, 58, 59, 102, 103, 106, 107], "100000": 73, "10000000": 110, "100k": 53, "10111000": 110, "1024": [61, 73], "102400": 53, "1048576000": 53, "105": 19, "10509": [41, 93], "10560": [24, 36], "106": 19, "107": 19, "10k": 53, "10x": 8, "10xxxxxx": 110, "11": [19, 107], "1106": [20, 34], "110k": [20, 34], "110xxxxx": 110, "110\u8868\u793a\u9700\u8981\u4e24\u4e2a\u5b57\u8282\u8868\u793a\u5f53\u524d\u5b57\u7b26": 110, "1110": 110, "1110xxxx": 110, "1110\u8868\u793a\u9700\u8981\u4e09\u4e2a\u5b57\u8282": 110, "11110xxx": 110, "11110\u8868\u793a\u9700\u8981\u56db\u4e2a\u5b57\u8282": 110, "117": 19, "12": [17, 19, 21, 25, 45, 49, 53, 100, 107], "12000": 111, "12122": [41, 93], "12186": [41, 93], "12288": 53, "123abc\u4e00\u4e8c\u4e09": 110, "125": [8, 19], "128": [53, 73], "128k": [53, 54, 57], "128\u4f4dascii\u7801\u662f\u6570\u5b57": 110, "12b": 18, "12k": 92, "12n_": 45, "13": [19, 45, 58, 100], "131": 32, "13245": [41, 93], "138": 1, "13b": 51, "13k": 9, "14": [17, 19, 51, 54, 104], "14165": [41, 93], "14168": [41, 93], "14187": [41, 93], "14858": [41, 93], "149225472": 53, "15": [19, 20, 34, 55, 56, 95], "151": 60, "15115": [41, 93], "1536": 53, "15b": [27, 39], "16": [17, 19, 55, 65, 71, 73, 76, 90, 98], "160": 53, "1609": 58, "1612": [41, 93], "164": 18, "16441": [41, 93], "16609": [41, 93], "16k": 57, "17": [18, 19], "1706": [41, 93], "175": [8, 24, 36], "17k": 65, "18": [19, 60], "1810": [41, 93], "18290": [41, 93], "185b": 52, "188743680": 53, "19": [7, 8, 25, 41, 93], "1904": [41, 93], "1909": [41, 93], "195": 25, "198": 21, "1990\u5e74\u5f00\u59cb\u7814\u53d1": 110, "1994": 100, "1_gnu": 19, "1e": [58, 59, 95, 105, 111], "1k": 21, "1l": 79, "1m": 35, "1qvx610cu7": [41, 93], "1t": [53, 57], "1w": 79, "1\u4f4d\u4e3a": 110, "2": [1, 2, 5, 6, 7, 8, 9, 13, 14, 16, 17, 19, 21, 24, 25, 29, 33, 36, 42, 45, 46, 48, 50, 52, 53, 54, 55, 57, 58, 59, 61, 65, 66, 67, 68, 69, 70, 71, 72, 73, 76, 77, 80, 81, 82, 84, 90, 91, 92, 98, 100, 101, 102, 103, 104, 105, 106, 107, 108, 110], "20": [8, 9, 14, 19, 33, 41, 65, 81, 92, 93], "200": [9, 14, 18, 33, 92], "2000": 55, "2001": [41, 93], "2005": [41, 93], "20050": [41, 93], "2009": [41, 93], "2017": [41, 93], "2019": [41, 93], "2020": [41, 93], "2021": [41, 93], "2022": [41, 93], "2023": [41, 52, 93], "2024": [19, 41, 61, 65, 93], "20240602": 19, "2025": [41, 93], "2048": [58, 59, 95, 102, 103], "20k": [14, 20, 27, 33, 34, 39, 52], "21": [17, 18, 19, 20, 21, 34, 41, 93, 98, 107], "2104": [41, 93], "2107": [41, 93], "2110": [41, 93], "21326725120": 53, "21b": 53, "22": [9, 19, 41, 93, 107], "2203": [41, 93], "2212": [24, 36], "2294": 58, "23": [5, 6, 12, 19, 21, 41, 54, 55, 60, 93, 98, 100, 102, 104, 106], "2305": [41, 93], "2306": [27, 39], "2308": [41, 93], "2309": [41, 93], "2311": [41, 93], "2312": [20, 34, 41, 93], "232": 32, "235692359680": 53, "236b": 53, "24": [15, 17, 19, 35, 38, 41, 52, 53, 54, 57, 60, 61, 93, 101, 104], "2401": [41, 93], "2403": [41, 93], "2405": [41, 93], "2406": [41, 93], "2409": [41, 93], "2412": [41, 93], "25": [12, 19, 26, 37, 41, 60, 61, 81, 93, 104], "250": 95, "256": [58, 59, 82], "256\u4f4dascii\u6269\u5c55\u7801\u662fascii": 110, "26": [19, 90], "27": [56, 90], "28": 65, "29": 19, "2900": 103, "290k": 35, "2d": 102, "2d_": 45, "2e": 35, "2i": [5, 51, 107], "2j": [58, 59, 102, 103], "2m": 53, "2n": 45, "2n_": [45, 104], "2t": 107, "2\u62164\u5b57\u8282\u53d8\u957f": 110, "3": [6, 7, 8, 9, 12, 19, 20, 25, 34, 44, 45, 48, 50, 51, 52, 53, 54, 58, 71, 73, 76, 82, 88, 90, 98, 100, 101, 102, 103, 105, 108], "30": [17, 52, 57], "300": 25, "3000": 103, "300m": 52, "30k": 52, "314": 1, "31k": 9, "32": [19, 58, 59, 65, 69, 95, 98, 110], "3200": 69, "32768": [102, 103], "32b": [65, 90], "32k": 54, "33": 17, "338": 52, "33k": 9, "33t": 104, "34": [19, 21], "34b": [15, 51, 73], "35x": 16, "37": 25, "374": 58, "37b": 54, "38": 19, "3822059520": 53, "384": 65, "39": 21, "3m": 53, "4": [9, 12, 17, 19, 20, 21, 34, 45, 46, 51, 55, 57, 58, 59, 65, 68, 69, 71, 73, 77, 82, 92, 98, 100, 103, 104, 105, 110, 111], "40": [64, 76, 81], "400": 12, "405b": 57, "4096": [35, 58, 59, 111], "40k": 52, "41": 19, "421": 32, "426": 22, "43": 19, "438k": 25, "443": [26, 37], "448": 21, "45": [7, 19, 55], "4d": [58, 59], "4e00": 110, "4e00\u57280800": 110, "4e00\u5bf9\u5e94\u7684\u4e8c\u8fdb\u5236\u4e3a100": 110, "4k": 54, "4t": 55, "4\u4e2a\u5b57\u8282\u8868\u793a\u4e00\u4e2a\u5b57\u7b26": 110, "4\u5b57\u8282\u53d8\u957f": 110, "4\u5b57\u8282\u8868\u793a": 110, "5": [17, 19, 20, 21, 28, 29, 32, 34, 35, 41, 48, 51, 52, 53, 54, 55, 58, 59, 64, 65, 67, 68, 69, 81, 82, 84, 90, 93, 95, 100, 103, 111], "50": [49, 51, 65, 90], "500": 12, "500000": [58, 59], "500b": 51, "512": [5, 35, 53, 65], "5120": 53, "52": 51, "52k": [14, 24, 33, 36], "54": [19, 48], "540": 56, "5403": 103, "55m": [26, 37], "57": 17, "5963": 58, "59k": 90, "5b": 92, "5e": [35, 73], "5k": 21, "5m": 53, "5pm": [14, 33], "6": [5, 16, 19, 20, 24, 34, 36, 45, 50, 51, 52, 53, 58, 59, 65, 73, 77, 92, 95, 103, 105, 111], "60": [13, 52, 53, 64, 76], "62": [19, 51], "63": 19, "64": [19, 53, 55, 95], "643": 60, "65": 21, "65b": [55, 95], "66": 19, "67": 55, "671b": 54, "67b": 53, "6n": 45, "6nb": 45, "6w": 110, "7": [18, 19, 21, 24, 36, 46, 49, 56, 60, 61, 65, 73, 98, 100, 103], "70": 19, "70b": [51, 56, 57, 73, 82], "72": 19, "73": 1, "750": 95, "753": 1, "75k": [20, 34, 92], "77": 49, "777": 32, "788m": 54, "7b": [14, 16, 20, 33, 34, 51, 55, 56, 73, 90, 104, 111], "8": [8, 19, 21, 24, 36, 41, 45, 50, 51, 53, 54, 60, 73, 76, 93, 98, 103, 104], "80": [46, 92], "800": 15, "8000": 49, "800k": 92, "80gb": 73, "80k": [20, 34], "80x": 16, "821b": 52, "82k": [24, 36], "83": 58, "8415j": 103, "85": [49, 57], "888": 1, "8b": [44, 57, 73], "8binstruct": 44, "8\u4e2abit\u4f4d\u4e2d\u9ad8\u4f4d\u5fc5\u987b\u7b49\u4e8e0": 110, "8\u4e2abit\u4f4d\u662f\u4e00\u4e2a\u5b57\u8282": 110, "8\u4e3a11100100": 110, "8\u4e3a\u4e09\u5b57\u8282": 110, "8\u4f4d\u53ef\u8868\u793a256\u4e2a\u5b57\u7b26": 110, "8\u548cgbk\u7f16\u7801": 110, "8\u6765\u5b9e\u73b0\u7f16\u7801": 110, "8\u7684\u65b9\u5f0f\u6765\u7f16\u7801\u89e3\u7801\u4f7f\u7528": 110, "8\u7684\u6620\u5c04\u5173\u7cfb\u5982\u4e0b": 110, "8\u7684\u7f16\u7801\u65b9\u5f0f": 110, "8\u7684\u89c4\u5219\u5f88\u7b80\u5355": 110, "8\u7b49": 110, "8\u7f16\u7801": 110, "9": [9, 18, 19, 41, 55, 58, 65, 71, 78, 93, 95, 100, 104], "90": 25, "92": 61, "9297": 103, "95": [19, 55, 95], "97": 13, "974": 22, "9901": 111, "9999": 103, "9e": 73, "A": [7, 14, 21, 24, 33, 35, 36, 41, 43, 44, 49, 56, 58, 59, 60, 64, 65, 69, 70, 75, 76, 77, 80, 83, 84, 88, 90, 93, 100, 101, 102, 107, 108, 112], "And": [70, 98], "As": [43, 46, 49, 50, 52, 56, 65, 68, 69, 70, 84, 88, 92, 101, 104], "At": [5, 7, 24, 36, 43, 48, 54, 56, 83, 92, 102, 103], "By": [8, 52, 54, 57, 58, 65, 69, 81, 102], "FOR": 76, "For": [1, 2, 6, 9, 14, 20, 22, 24, 25, 33, 34, 35, 36, 41, 44, 45, 46, 49, 50, 51, 53, 54, 56, 57, 60, 61, 64, 65, 66, 68, 69, 70, 72, 76, 78, 79, 80, 81, 88, 90, 92, 98, 101, 102, 103, 104, 112], "If": [12, 42, 43, 46, 50, 58, 59, 61, 69, 70, 76, 80, 83, 101, 103, 108], "In": [1, 2, 5, 6, 9, 14, 20, 24, 27, 33, 34, 35, 36, 38, 39, 41, 43, 44, 48, 50, 51, 52, 53, 54, 56, 57, 58, 60, 61, 65, 66, 67, 68, 69, 70, 71, 72, 76, 77, 80, 81, 82, 83, 84, 86, 88, 93, 95, 98, 101, 102, 103, 104, 106, 107], "It": [9, 12, 13, 14, 17, 27, 33, 39, 41, 45, 46, 50, 51, 53, 56, 57, 58, 67, 70, 76, 90, 91, 101, 103], "Its": 49, "No": [44, 54, 58, 61], "Not": [14, 33], "OF": 76, "Of": [24, 36], "On": [5, 46, 54, 69], "One": [12, 43, 48, 57, 69, 72, 76, 77, 80, 102, 103, 107], "Or": [16, 57], "Such": [41, 65, 83, 93], "That": [5, 42], "The": [2, 5, 6, 7, 8, 9, 13, 14, 15, 17, 18, 20, 21, 22, 24, 27, 31, 32, 33, 34, 35, 36, 38, 39, 41, 42, 44, 45, 48, 49, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 65, 66, 67, 68, 69, 70, 71, 72, 73, 76, 77, 78, 80, 81, 82, 83, 88, 92, 95, 101, 102, 103, 104, 105, 107, 108], "Their": 92, "Then": [9, 20, 25, 34, 57, 68, 69, 70, 76, 77, 80, 90, 101, 104, 106], "There": [13, 102], "Thes": 13, "These": [5, 6, 14, 21, 26, 27, 33, 35, 39, 57, 60, 76, 80, 82, 83], "To": [1, 2, 5, 7, 8, 9, 13, 17, 20, 21, 24, 25, 27, 31, 34, 35, 36, 38, 39, 43, 44, 45, 48, 49, 51, 52, 54, 55, 56, 57, 58, 60, 61, 64, 65, 66, 70, 71, 72, 75, 77, 80, 82, 84, 88, 90, 92, 101, 102, 105, 108], "With": [17, 21, 42, 50, 66, 102], "_": [5, 9, 24, 27, 36, 38, 39, 43, 51, 54, 56, 58, 59, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 79, 80, 81, 84, 91, 92, 98, 101, 102, 104, 106, 107, 108], "_1": [5, 91], "__init__": [53, 58, 59, 105], "__main__": 16, "__name__": 16, "_bsz": [58, 59], "_h": 5, "_i": 98, "_libgcc_mutex": 19, "_mergeable_rank": 58, "_norm": [58, 59, 105], "_openmp_mutex": 19, "_t": [75, 104], "a100": 73, "a_": [43, 70, 75, 90, 103, 107], "a_i": [38, 98], "a_t": 75, "aa": 100, "aaabdaaabac": 100, "ab": [41, 93, 100, 103], "abbrevi": [21, 32, 110], "abil": [1, 7, 17, 20, 21, 34, 46, 52, 53, 60, 65, 68, 70, 80, 86, 88, 92], "abl": [14, 15, 21, 33, 46, 56], "ablat": [35, 90, 92], "about": [5, 9, 14, 20, 25, 33, 34, 41, 42, 46, 49, 50, 51, 53, 69, 81, 90, 101, 103], "abov": [6, 25, 46, 49, 51, 57, 65, 66, 75, 80, 81, 83, 84, 88, 91, 101, 102], "absenc": 54, "absolut": [5, 55, 64, 106], "absorb": 104, "abstract": 7, "abstractset": 58, "ac": 100, "academ": 17, "acceler": [6, 104], "accept": [41, 80], "access": [1, 32, 46, 56, 69, 80, 82, 83, 84, 91], "accommod": [57, 92], "accompani": 72, "accomplish": [44, 57], "accord": [1, 49, 54, 69, 81, 84, 90, 92, 101, 103], "accordingli": 65, "account": [45, 51], "accumul": 50, "accur": [17, 22, 26, 35, 56, 60, 66, 70, 81], "accuraci": [1, 17, 21, 54, 60, 65, 77, 81, 88, 90], "achiam": [41, 93], "achiev": [2, 8, 17, 21, 35, 48, 52, 54, 55, 56, 57, 60, 65, 66, 67, 68, 72, 77, 80, 81, 84, 90, 104], "acquir": [17, 53, 56, 88, 101], "across": [17, 19, 46, 49, 57, 60, 69, 90, 92, 101, 102, 104, 105], "action": [2, 14, 33, 43, 65, 75], "activ": [5, 6, 53, 54, 55, 60, 101, 104, 108], "actor": [70, 73], "actor_learning_r": 73, "actual": [9, 70, 91], "ad": [6, 7, 24, 27, 36, 39, 48, 57, 65, 70, 81, 91, 107], "adam_offload": 73, "adamw": [55, 65, 95], "adapt": [6, 27, 39, 66, 72, 101, 102, 103, 112], "add": [5, 7, 9, 13, 27, 39, 50, 51, 54, 55, 56, 58, 59, 64, 71, 81, 82], "addit": [5, 7, 9, 25, 43, 46, 48, 49, 51, 52, 54, 58, 59, 65, 69, 71, 72, 78, 82, 88, 91, 92, 95, 101, 102, 104, 108, 110], "addition": [6, 17, 24, 35, 36, 54, 69, 101], "additionali": 70, "address": [1, 21, 27, 39, 50, 57, 61, 65, 70, 77, 80], "adher": [46, 60, 61, 65, 88], "aditya": [41, 93], "adjust": [53, 57, 64, 80, 88, 104], "adopt": [14, 20, 25, 31, 33, 34, 44, 48, 53, 54, 61, 66, 88, 104], "advanc": [1, 17, 35, 49, 60], "advantag": [46, 70, 72, 73, 104], "advis": 76, "affect": [28, 35, 44, 101], "affin": [54, 58, 101], "aforement": 77, "after": [6, 7, 24, 27, 36, 39, 44, 48, 49, 52, 54, 55, 56, 57, 58, 61, 65, 68, 69, 70, 71, 77, 80, 81, 82, 88, 90, 92, 101, 102, 103], "again": [5, 77, 108], "against": [9, 12, 31, 50, 61, 76, 78, 90], "agarw": [41, 93], "agent": [61, 69, 75, 80], "aggreg": 49, "aggress": [14, 33], "agnost": [8, 72], "ahm": [41, 93], "ai": [1, 12, 19, 21, 41, 50, 57, 65, 66, 82, 84, 93], "aidan": [41, 93], "aif": 82, "aift": 82, "aim": [13, 21, 43, 44, 65, 70, 80, 82, 83, 91, 101], "aime24": 90, "ainsli": [41, 93], "aiohttp": 19, "aiosign": 19, "aixin": [41, 93], "ajudg": 61, "alec": [41, 93], "alethea": [41, 93], "alex": [41, 93], "algorithm": [9, 18, 24, 26, 36, 37, 48, 49, 50, 52, 53, 55, 56, 57, 60, 61, 65, 67, 70, 100], "alibi": 103, "align": [5, 6, 9, 21, 28, 38, 45, 46, 54, 56, 57, 58, 59, 61, 64, 65, 66, 67, 68, 69, 70, 71, 75, 77, 80, 101, 102, 103, 104, 106, 107, 108], "align_n": 72, "alignmentrelev": 46, "alik": 51, "all": [2, 7, 9, 14, 18, 22, 27, 31, 33, 38, 39, 41, 42, 43, 45, 48, 49, 52, 53, 56, 57, 58, 61, 65, 67, 68, 69, 70, 71, 75, 76, 77, 79, 80, 81, 91, 92, 93, 95, 98, 101, 102, 103, 104, 105, 108], "allclos": [58, 105], "allevi": 101, "alloc": [61, 65, 88], "allow": [2, 5, 7, 8, 25, 41, 43, 48, 49, 54, 57, 58, 75, 76, 83, 101, 103, 107], "allowed_speci": 58, "allowed_token": 58, "almeida": [41, 93], "almost": [81, 95], "alon": [77, 81, 82, 95], "along": [6, 31, 48, 50, 51], "alongsid": [54, 70], "alpaca": [1, 20, 27, 28, 29, 34, 39], "alpacaev": [44, 82], "alpha": [72, 75, 81, 91, 102], "alpha_": [45, 101], "alphacod": 51, "alreadi": [46, 48, 52, 58, 69, 82], "also": [5, 9, 14, 17, 19, 21, 22, 24, 25, 33, 35, 36, 41, 42, 44, 45, 46, 48, 49, 50, 51, 52, 53, 54, 56, 57, 58, 59, 60, 61, 64, 67, 68, 70, 71, 76, 82, 83, 84, 90, 101, 102, 104, 108], "altdj": [41, 60, 93, 104], "alter": 50, "altern": [8, 56, 57, 67, 69, 72, 77], "although": [12, 43, 52, 54, 88, 101, 110], "alwai": [48, 57, 72, 81, 101, 102, 103], "amanda": [41, 93], "ambigu": 81, "amc": 21, "american": 110, "amodei": [41, 93], "among": [1, 12, 15, 51, 53, 61, 69, 80, 101, 104], "amount": [35, 45, 49, 53, 57, 82, 88, 90, 92, 98, 102, 104, 108], "an": [1, 2, 5, 6, 7, 8, 9, 12, 13, 14, 17, 18, 20, 24, 25, 27, 33, 34, 35, 36, 39, 41, 43, 44, 46, 48, 49, 50, 51, 52, 53, 54, 56, 57, 58, 59, 61, 64, 65, 67, 68, 70, 72, 75, 76, 77, 78, 80, 81, 82, 83, 84, 88, 91, 92, 93, 98, 101, 102, 103, 104], "anaconda3": 19, "analogi": 46, "analysi": [1, 25, 50, 53, 57, 61, 72, 77], "analyz": [35, 61, 98], "anchor": 50, "andi": [41, 93], "andrew": [41, 93], "angela": [41, 93], "angl": [103, 106], "ani": [5, 7, 8, 13, 14, 18, 20, 24, 31, 33, 34, 35, 36, 42, 43, 46, 50, 54, 57, 58, 61, 71, 76, 84, 88, 92, 102, 103, 104, 106, 107], "anneal": 35, "annot": [1, 17, 19, 51, 54, 56, 57, 65, 77, 82], "anoth": [14, 33, 43, 48, 50, 56, 70, 72, 77, 81, 103], "answer": [1, 2, 6, 7, 16, 17, 21, 26, 35, 37, 38, 44, 51, 54, 56, 60, 61, 65, 76, 83, 88, 90, 91, 92, 95, 98, 103], "answer_1": 57, "anthrop": [19, 69], "anticip": 61, "anyio": 19, "anywher": [9, 71], "aop": 65, "api": [2, 9, 13, 19, 35, 60, 71], "app": 76, "appear": [31, 45, 50, 65, 103], "append": [24, 35, 36, 58, 59, 73, 76, 80, 90], "appli": [5, 6, 9, 20, 27, 34, 39, 44, 49, 50, 51, 52, 54, 57, 58, 59, 64, 65, 67, 69, 70, 71, 72, 75, 81, 83, 84, 88, 91, 98, 101, 102, 103, 108], "applic": [8, 9, 21, 51, 56, 102, 103], "apply_chat_templ": 73, "apply_rotary_emb": [58, 59, 103], "approach": [20, 24, 34, 35, 36, 43, 45, 49, 51, 53, 54, 56, 57, 61, 64, 65, 67, 72, 75, 77, 78, 82, 83, 84, 88, 90, 91, 101, 102, 103], "appropri": [1, 5, 14, 33, 51, 75], "approx": [45, 54, 102, 106], "approxim": [7, 8, 12, 43, 52, 53, 57, 69, 81, 84, 92, 95, 98, 101, 103], "aptitud": 21, "ar": [1, 2, 5, 6, 7, 8, 9, 12, 13, 14, 15, 18, 20, 21, 24, 25, 27, 32, 33, 34, 35, 36, 39, 41, 42, 43, 44, 45, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 80, 81, 82, 83, 86, 92, 93, 95, 98, 101, 102, 103, 104, 105, 107, 110], "arang": [18, 58, 59, 67, 103], "arbitrari": [9, 103], "archit": [41, 93], "architectur": [6, 7, 45, 48, 51, 61, 101, 103, 104, 105, 107, 112], "archiv": 7, "area": [17, 60], "arg": [56, 58, 59, 66, 73, 80, 103], "argmax": 58, "argu": [53, 80], "ariel": [41, 93], "aris": [76, 77], "arithmet": 21, "armando": [41, 93], "armo": 29, "armorm": 44, "around": [5, 12, 104], "arrang": 19, "art": [8, 21, 46, 55, 65], "articl": 7, "artifici": 66, "arvind": [41, 93], "arxiv": [20, 24, 27, 34, 36, 39, 41, 55, 93], "ascent": 75, "ascertain": 69, "ascii\u5c31\u662f\u6700\u521d\u7684\u4e00\u79cd\u6620\u5c04\u5173\u7cfb": 110, "ascii\u8d77\u521d\u53ea\u89c4\u5b9a\u4e86127\u4e2a\u5b57\u7b26": 110, "ashish": [41, 93], "ask": [9, 13, 14, 22, 24, 33, 36, 50, 56, 57, 71, 77, 83], "askel": [41, 93], "aspect": [50, 60, 72, 82], "assert": [51, 58, 59, 103], "assertionerror": [58, 103], "assess": [1, 17, 18, 21, 27, 39, 57, 61, 79, 90], "assign": [5, 43, 48, 56, 57, 64, 65, 69, 70, 72, 75, 80, 81, 84, 91, 92, 101], "assist": [14, 33, 46, 51, 58, 76, 78, 82, 83], "associ": [51, 65, 80], "assum": [6, 43, 68, 72, 80, 82, 83, 84, 91, 105, 109], "ast": [66, 67, 80, 91, 107], "asymptot": 21, "async": 19, "atcod": [19, 32], "atol": [58, 105], "atom": 2, "att": 101, "attach": 64, "attain": 69, "attempt": [46, 90, 91, 92], "attend": 5, "attent": [6, 7, 8, 28, 29, 41, 45, 53, 54, 60, 68, 93, 101, 102, 103, 105, 106, 107], "attention_bia": 53, "attention_norm": [58, 59], "attn": 45, "attr": 19, "attract": 103, "attribut": [25, 49, 78], "audio": [14, 33], "augment": [1, 82], "auli": [41, 93], "auth": 19, "authent": 72, "author": [67, 82], "auto": [5, 12], "autom": [24, 32, 35, 36, 60], "automat": [2, 12, 20, 24, 27, 34, 36, 39, 57, 61, 64, 78, 101], "autonom": 88, "autoregress": [6, 8, 43, 51, 56, 102], "auxiliari": [6, 46, 48, 54, 91], "avail": [1, 26, 51, 55, 57, 60], "avenu": 9, "averag": [2, 18, 25, 48, 49, 58, 59, 70, 77, 90, 101], "avg": 65, "avoid": [9, 43, 46, 48, 49, 50, 65, 68, 70, 75, 80, 81, 102], "await": [27, 39], "ax": 57, "axi": [58, 105], "b": [6, 41, 43, 45, 58, 59, 69, 76, 83, 90, 91, 93, 102, 108, 112], "b_": [58, 59, 108], "b_1": 5, "b_2": 5, "b_i": 54, "b_j": 54, "babuschkin": [41, 93], "backbon": 64, "backend": 16, "background": 18, "backpropag": 56, "backtransl": 57, "backward": 45, "bad": [43, 46, 57], "bag": 56, "bai": [41, 93], "baker": [41, 93], "balaji": [41, 93], "balanc": [5, 54, 57, 61, 65, 83], "band": 8, "bank": 2, "bao": [41, 93], "baosong": [41, 93], "baptist": [41, 93], "bar": 102, "barn": [41, 93], "basart": [41, 93], "base": [1, 5, 6, 7, 9, 13, 14, 20, 21, 24, 27, 29, 33, 34, 35, 36, 39, 42, 43, 46, 48, 49, 50, 51, 53, 54, 55, 56, 57, 58, 64, 67, 69, 70, 71, 72, 76, 80, 81, 82, 83, 84, 90, 91, 98, 102, 103, 106, 107, 110, 111], "baselin": [12, 21, 44, 65, 82, 83, 92], "basi": [58, 103], "basic": [21, 22, 28, 50, 60, 61], "basu": [41, 93], "batch": [9, 14, 33, 35, 45, 51, 55, 56, 57, 58, 65, 69, 71, 95, 104], "batchnorm1d": [58, 105], "batchnorm2d": 58, "bavarian": [41, 93], "bax": 112, "bbc": [41, 60, 93], "bbpe": [53, 60], "becaus": [5, 14, 24, 33, 36, 50, 57, 64, 76, 88, 105], "becom": [68, 77, 91, 101, 103, 104], "been": [2, 25, 43, 49, 50, 53, 54, 58, 60, 69, 70, 81, 84], "befor": [1, 24, 31, 36, 46, 48, 52, 56, 61, 78, 102, 103], "begin": [1, 5, 6, 7, 9, 45, 51, 54, 56, 57, 58, 59, 65, 66, 67, 68, 69, 70, 71, 76, 80, 84, 88, 101, 102, 103, 104, 106, 107, 108], "begin_of_text": 58, "behav": [46, 49, 103], "behavior": [7, 9, 21, 43, 46, 49, 50, 61, 65, 66, 76, 88, 103, 105], "behaviour": 48, "behind": 103, "bei": [41, 93], "beichen": [41, 93], "being": [5, 8, 20, 34, 41, 48, 57, 84], "believ": 64, "belong": 101, "below": [14, 20, 33, 34, 46, 51, 68, 75, 81], "bench": 28, "benchmark": [1, 2, 7, 8, 15, 26, 27, 37, 39, 41, 46, 52, 53, 57, 90, 93, 104], "benefici": [5, 50, 81], "benefit": [7, 56, 81, 98], "benfeng": [41, 93], "benjamin": [41, 93], "berner": [41, 93], "bert": [7, 41, 93], "besid": [65, 66], "best": [2, 9, 21, 35, 44, 45, 48, 49, 50, 55, 56, 57, 61, 66, 69, 71, 81, 82, 84, 90, 91, 92], "bestof": 44, "beta": [9, 56, 58, 67, 68, 70, 71, 75, 80, 102, 105, 108], "beta_": [55, 91], "beta_1": 95, "beta_2": 95, "better": [9, 17, 46, 49, 50, 52, 54, 55, 56, 57, 60, 61, 68, 75, 76, 77, 80, 83, 90, 92, 98, 104], "between": [5, 9, 14, 17, 20, 21, 33, 34, 38, 43, 44, 46, 48, 49, 51, 53, 56, 57, 58, 59, 65, 67, 68, 70, 71, 75, 76, 77, 80, 81, 82, 88, 91, 92, 98, 101, 102, 105, 106, 107, 108], "beyond": [19, 46, 77, 101, 103], "bf16": [73, 111], "bi": [41, 93], "bia": [53, 54, 58, 59, 60, 64, 77, 83, 91, 108], "bias": [24, 36, 43], "bib": 41, "bibliographi": 41, "bibtex": 41, "bidirect": [41, 93], "bigger": [9, 102], "biggest": 49, "bilinear": 108, "billion": 8, "bin": [41, 93], "binari": [56, 57, 66, 77, 81], "bing": [41, 93], "bingxuan": [41, 93], "binom": [9, 18, 71, 90], "binyuan": [41, 93], "biologi": [17, 21], "bit": [69, 102], "black": 45, "bleu": 43, "blind": 17, "block": [6, 7, 16, 42, 51, 101], "blog": [9, 41, 61, 70, 74, 93], "blue": [27, 39], "bm25": 25, "bmr": [8, 9, 41, 93], "bn": [58, 105], "bo": [41, 58, 93], "bob": [41, 93], "bodi": [18, 110], "bofei": [41, 93], "boltzmann": 69, "bonu": 91, "book": [7, 41, 42, 55], "bool": 58, "bootstrap": [20, 24, 34, 36, 46], "bos_id": 58, "both": [1, 5, 8, 12, 15, 17, 20, 21, 34, 41, 44, 48, 51, 54, 56, 57, 60, 61, 70, 71, 72, 76, 77, 78, 82, 84, 91, 92, 102, 103, 104, 105], "boto3": 19, "botocor": 19, "bottleneck": 104, "bottom": [5, 81], "bound": [44, 75, 101, 103], "boundari": [20, 24, 34, 36, 102], "bowen": [41, 93], "box": [21, 41, 54, 88], "boyang": [41, 93], "bpe": [7, 51, 55], "bpe\u6bcf\u4e00\u6b65\u90fd\u5c06\u6700\u5e38\u89c1\u7684\u4e00\u5bf9\u76f8\u90bb\u6570\u636e\u5355\u4f4d\u66ff\u6362\u4e3a\u8be5\u6570\u636e\u4e2d\u6ca1\u6709\u51fa\u73b0\u8fc7\u7684\u4e00\u4e2a\u65b0\u5355\u4f4d": 100, "bpe\u9009\u62e9\u9891\u6570\u6700\u9ad8\u7684\u76f8\u90bb\u5b50\u8bcd\u5408\u5e76": 109, "bradlei": [9, 64, 67, 80, 81], "brahma": [41, 93], "brainstorm": 83, "branch": 57, "breadth": [17, 27, 39], "break": [24, 36, 58, 80], "breviti": [101, 104], "bridg": [17, 43, 61], "brief": [8, 13], "bright": 21, "bring": 70, "broad": [24, 36], "broadcast": 103, "broader": [17, 19], "broadli": 8, "brockman": [41, 93], "brook": [41, 93], "brown": [41, 93], "brundag": [41, 93], "brute": 50, "bsz": [58, 59], "bt": 80, "bucket": 57, "budget": [48, 49, 51, 55, 60, 90, 101], "buffer": [69, 73], "bug": [13, 25, 50, 73], "build": [7, 17, 19, 41, 44, 52, 57, 58, 60, 61, 64, 78, 82, 83], "built": [42, 53, 56, 61], "bullet": 50, "burda": [41, 93], "burden": 70, "burn": [41, 93], "busi": 17, "byte": [7, 41, 51, 53, 55, 60, 93], "bzip2": 19, "c": [6, 15, 18, 45, 48, 49, 53, 56, 57, 58, 59, 61, 66, 69, 81, 102, 103, 104, 105, 108], "c4": 55, "c_": [45, 53], "ca": 19, "cach": [16, 58, 59, 60, 65], "cache_k": [58, 59], "cache_len": [58, 59], "cache_v": [58, 59], "cachecontrol": 19, "cachetool": 19, "cai": [41, 76, 93], "caichat": 73, "calcul": [1, 18, 21, 53, 58, 59, 65, 69, 70, 75, 76, 77, 81, 98, 101, 103, 105], "calibr": [44, 76], "call": [1, 2, 5, 7, 8, 9, 13, 18, 27, 39, 41, 43, 46, 50, 54, 56, 57, 58, 71, 72, 76, 80, 86, 92, 103, 104, 108], "can": [1, 2, 5, 6, 8, 9, 13, 18, 19, 20, 21, 24, 25, 34, 35, 36, 41, 42, 43, 44, 45, 46, 48, 49, 50, 51, 52, 53, 54, 56, 57, 60, 61, 64, 65, 67, 68, 69, 70, 71, 72, 75, 76, 77, 80, 81, 82, 83, 84, 88, 90, 92, 93, 98, 101, 102, 103, 104, 105, 107, 108, 109], "cancel": 67, "candid": [25, 48, 49, 57, 61, 69, 77, 80, 82, 84, 98], "cannot": [13, 14, 21, 33, 43, 46, 103, 104], "canon": 77, "capabl": [1, 12, 13, 19, 20, 21, 27, 34, 35, 38, 39, 44, 46, 51, 54, 56, 60, 61, 72, 82, 88, 103], "capac": [6, 69, 101], "captur": [81, 88, 101], "cardin": [49, 69], "care": [46, 57, 90], "carefulli": [17, 53, 57, 60, 95], "carlo": [43, 70], "carr": [41, 93], "carri": [54, 104], "carrol": [41, 93], "cascad": 51, "case": [9, 14, 20, 22, 24, 31, 32, 33, 34, 35, 36, 43, 48, 49, 50, 51, 52, 54, 56, 61, 65, 66, 68, 72, 76, 84, 88, 90, 101, 102], "cast": 58, "catastroph": 103, "catch": 57, "categor": [49, 57, 61, 81, 83], "categori": [12, 83], "caus": [17, 58, 67, 68, 78, 101], "causal": [6, 51], "cd": [16, 19], "cdot": [5, 53, 65, 67, 68, 72, 80, 91, 101, 102, 107, 108], "ce": 69, "ceas": 45, "ceil": 46, "cell": 58, "central": 81, "centroid": 101, "certain": [6, 43, 54, 57, 65, 83, 84, 101, 102, 103], "certif": 19, "certifi": 19, "cffi": 19, "cfg": 16, "cgrs19": [8, 41, 93], "chai": [41, 93], "chain": [17, 54, 76, 77, 83, 98], "chainof": 98, "challeng": [1, 12, 13, 14, 17, 21, 26, 33, 37, 46, 50, 57, 60, 61, 65, 67, 81, 83, 88, 92], "chan": [41, 93], "chanc": [17, 50], "chang": [14, 19, 25, 33, 41, 66, 78, 91, 93], "changhan": [41, 93], "changyu": [41, 93], "channel": [19, 58], "chantzi": [41, 93], "charact": [50, 51, 110], "character": [45, 53, 60], "characterist": [27, 39, 53], "charset": [19, 110], "chat": [13, 41, 52, 53, 56, 58, 93], "chat_complet": 58, "chatbot": 12, "chatgpt": [20, 34, 46], "cheaper": 12, "check": [9, 13, 22, 32, 35, 51, 57, 60, 61, 90, 91, 92], "checklist": 61, "checkpoint": [41, 52, 54, 56, 57, 58, 60, 73, 88, 93], "chelsea": [41, 93], "chemistri": [17, 21], "chen": [41, 93], "chenggang": [41, 93], "chengpeng": [41, 93], "chengqi": [41, 93], "chengqiang": [41, 93], "chengyuan": [41, 93], "chess": [41, 46, 93], "child": [41, 93], "chines": 53, "cho": [41, 93], "choic": [6, 17, 21, 35, 43, 56, 68, 69, 76, 80, 92, 107], "chong": [41, 93], "choos": [25, 27, 39, 48, 50, 56, 76, 81, 83, 90, 92, 102], "chose": 5, "chosen": [45, 56, 57, 64, 81], "chosen_1": 57, "chosen_2": 57, "christiano": [41, 93], "christoph": [41, 93], "chu": [41, 93], "chuanqi": [41, 93], "chunqiu": [41, 93], "ci": 103, "cite": 41, "ckb": [21, 41, 93, 98], "ckpt_dir": 58, "ckpt_path": 58, "cl": [20, 34], "clamp": 76, "clarifi": 45, "clariti": [54, 61], "clark": [41, 93], "class": [19, 24, 36, 53, 58, 59, 68, 105], "classic": 75, "classif": [6, 14, 33, 46, 57, 81, 90, 92], "classifi": [19, 48, 57, 61, 72, 83, 90], "claud": [52, 90], "clean": [35, 52, 56, 57, 61], "clear": [61, 73, 81, 103], "clearli": [50, 52], "clemen": [41, 93], "cleo": 19, "clever": 67, "cli": [16, 73], "client": 19, "clip": [55, 70], "clone": [16, 19], "close": [20, 34, 43, 49, 50, 52, 54, 58, 76], "closer": 80, "closest": 50, "cluster": 57, "cly": [41, 61, 93], "cnn": 7, "co": [5, 51, 58, 59, 102, 103, 106, 107], "coars": [57, 61], "cobb": [41, 93], "code": [2, 15, 18, 19, 20, 25, 26, 28, 29, 32, 34, 35, 37, 38, 41, 42, 44, 46, 48, 49, 52, 53, 54, 55, 60, 61, 83, 88, 93, 110], "code1": 19, "code2": 19, "code_alpaca_20k": [14, 33], "code_generation_lit": 19, "code_list": 19, "codealpaca": [14, 20, 28, 29, 33, 34], "codebas": 25, "codebert": 61, "codecontest": [48, 49], "codeexecut": 19, "codeforc": [19, 31, 32, 48, 49], "codegen": 16, "codellama": [20, 34], "codenet": 31, "codeqwen1": 61, "coder": [20, 28, 29, 34, 35, 41, 54, 60, 93, 111], "codewar": 32, "codex": [18, 28, 51], "coeffici": [9, 64, 67, 71, 75, 103], "cognit": 88, "coher": [19, 77], "collabor": 61, "collaps": 65, "collect": [7, 9, 19, 20, 24, 25, 34, 35, 36, 51, 53, 55, 57, 58, 61, 71, 72, 73, 76, 83, 84, 88, 98], "collin": [41, 93], "colon": 78, "com": [16, 19], "combin": [7, 9, 14, 20, 33, 34, 35, 49, 50, 51, 54, 56, 57, 61, 65, 71, 72, 77, 79, 80, 88, 101, 102], "come": [5, 9, 14, 24, 33, 36, 52, 102, 103], "command": [19, 42], "commbal": 101, "comment": [51, 57, 61], "common": [7, 9, 19, 43, 50, 52, 57, 61, 64, 69, 101], "commoncrawl": 55, "commonli": [1, 20, 34, 35], "commonmark": 41, "commun": [35, 54, 88, 95, 110], "commut": 104, "compar": [2, 12, 17, 35, 43, 44, 52, 54, 57, 60, 61, 65, 68, 69, 70, 72, 82, 84, 90, 91, 92, 108], "comparison": [9, 44, 51, 52, 53, 59, 71, 72, 76, 80, 81], "compat": [5, 51, 103, 105], "compet": [8, 57], "competit": [5, 8, 19, 21, 31, 48, 49, 54, 65, 90], "competitor": 9, "compil": [52, 53, 54, 57, 88], "complementari": 54, "complet": [9, 13, 14, 18, 27, 33, 35, 39, 44, 50, 51, 54, 56, 57, 58, 61, 67, 68, 71], "complex": [2, 6, 13, 17, 20, 27, 34, 39, 46, 51, 57, 58, 59, 60, 61, 70, 75, 86, 88, 102, 103, 107], "complex64": [58, 59, 103], "compli": 49, "complic": [27, 39, 46, 70], "compon": [35, 49, 56, 58, 59, 60, 81, 82, 108], "compos": [5, 21, 101], "composit": [2, 57], "comprehens": [1, 7, 13, 18, 27, 39, 54, 60, 72, 75], "compress": [53, 100], "compris": [17, 53, 54, 57, 60, 61, 69, 101], "comput": [5, 15, 17, 18, 35, 46, 49, 51, 55, 56, 58, 59, 60, 61, 66, 69, 70, 75, 76, 77, 79, 88, 90, 92, 101, 102, 103, 104, 105, 107, 108, 110], "concat": 5, "concaten": [5, 24, 36, 56, 77, 91], "concept": [51, 61], "concis": [54, 60, 69], "conclud": 53, "conclus": [53, 104], "concret": [27, 39, 65, 91], "concurr": 50, "conda": 19, "condit": [1, 6, 24, 36, 43, 48, 72], "conduct": [49, 53, 57, 66, 72, 77, 80, 92, 103], "conduct_rejection_sampl": 80, "confer": [41, 72, 93], "confid": [46, 76, 84], "config": [53, 101], "configur": [44, 53], "confin": 56, "conform": 57, "confus": 65, "conjug": 107, "connect": 5, "consecut": [20, 34, 49], "consequ": [57, 72], "consid": [2, 18, 25, 43, 46, 57, 61, 64, 65, 68, 69, 72, 76, 77, 80, 81, 83, 102, 103], "consider": [52, 65, 72], "consist": [1, 5, 6, 9, 15, 17, 20, 21, 25, 31, 32, 34, 46, 51, 52, 53, 61, 64, 65, 67, 71, 75, 84, 88, 98, 101, 105], "console_script": 16, "consolid": [2, 101], "constant": [1, 43, 65, 101, 102, 107, 108], "constitut": 66, "constrain": [2, 65, 67, 75, 80, 90], "constraint": [27, 39, 50, 56], "construct": [1, 12, 15, 27, 38, 39, 51, 52, 61, 66, 72, 78, 80, 82, 88, 101], "consum": [5, 84], "contain": [1, 2, 5, 7, 9, 12, 13, 14, 16, 17, 20, 21, 22, 24, 25, 33, 34, 35, 36, 46, 48, 49, 50, 51, 53, 55, 56, 57, 58, 61, 65, 66, 78, 90, 92, 98, 103, 104], "container": 57, "contamin": [19, 25, 41, 93], "content": [14, 33, 35, 41, 42, 58, 72, 76], "contest": [19, 49], "context": [6, 7, 13, 14, 24, 25, 33, 35, 36, 43, 45, 53, 57, 64, 70, 72, 76, 77, 78, 101, 107], "context_messag": 73, "contextu": 35, "contextwindow": 54, "contigu": [6, 58, 59], "continu": [1, 5, 9, 19, 20, 21, 25, 34, 50, 57, 61, 64, 70, 71, 76, 80, 81], "contrast": [8, 17, 43, 50, 78, 84, 103], "contribut": [25, 35, 65, 75, 81], "control": [2, 9, 46, 58, 67, 71, 77, 90], "convei": 72, "convent": 104, "converg": [6, 101, 105], "convers": [45, 57, 58, 65, 76, 103], "convert": [5, 42, 77, 80], "convinc": 92, "convolut": [5, 41, 93], "cookbook": 35, "coordin": 102, "copi": 18, "core": [7, 12, 19, 46, 104], "corpora": [35, 61], "corpu": [6, 8, 20, 34, 52, 53, 54, 61, 69, 103], "corr": 64, "correct": [1, 2, 13, 22, 28, 29, 35, 41, 43, 48, 49, 50, 51, 54, 57, 60, 61, 64, 65, 66, 68, 72, 83, 88, 90, 92, 93, 98], "correctli": [21, 50, 84, 91, 103], "correl": [8, 12, 64, 72, 82, 98], "correspond": [5, 9, 13, 21, 24, 27, 35, 36, 39, 43, 49, 50, 51, 52, 54, 58, 59, 64, 66, 67, 70, 72, 80, 103, 106, 107], "correspondingli": 70, "cosin": [5, 35, 55, 57, 111], "cost": [14, 33, 52, 54, 101], "costli": 101, "cot": [17, 28, 29, 65, 76, 77, 88], "could": [44, 46, 48, 49, 57, 64, 66, 75, 81, 84, 90, 103], "count": [18, 35, 90], "counteract": [5, 57], "counterclockwis": 106, "counterpart": [56, 57], "coupl": [54, 91], "cover": [17, 50, 57, 61, 82, 101], "coverag": [24, 36, 52, 60, 77, 84], "cpu": 58, "cr": 69, "crack": 29, "craft": 69, "crashtest": 19, "crawl": [7, 52, 61], "creat": [7, 9, 14, 19, 21, 24, 25, 27, 33, 35, 36, 39, 46, 51, 52, 57, 60, 61, 78, 82], "creation": 50, "creativ": [46, 54], "credit": [43, 75], "criteria": [56, 57, 60, 78, 90], "critic": [48, 52, 60, 70, 73, 75, 91], "critic_learning_r": 73, "critiqu": [28, 29, 72], "crmsnorm": [41, 93], "cross": [13, 25, 45, 48, 57, 66, 69, 77], "cross_entropi": 58, "crowd": 22, "crowdwork": 76, "crucial": [35, 52, 53, 61], "crux": 29, "cruxev": [41, 93], "cryptographi": 19, "ctj": [18, 41, 93], "ctx": 45, "cu12": 19, "cubla": 19, "cuda": [19, 58], "cudnn": 19, "cufft": 19, "cui": [41, 93], "cum": [41, 93], "cumbersom": 69, "cumsum": 58, "cumul": [58, 75], "cup": 98, "cupti": 19, "cur_po": 58, "curand": 19, "curat": [2, 17, 31, 32, 35, 48, 53, 54, 57, 61, 64, 88, 95], "curiou": [12, 81], "current": [1, 7, 8, 9, 14, 26, 27, 33, 35, 39, 43, 60, 69, 70, 71, 80, 83, 90, 92, 102, 104], "curv": [45, 56], "cusolv": 19, "cuspars": 19, "custom": 9, "custom_evalu": 19, "custom_output_fil": 19, "cut": [58, 72], "cutoff_len": 111, "cycl": 57, "d": [9, 16, 27, 39, 41, 42, 43, 45, 51, 56, 58, 59, 64, 65, 67, 68, 69, 70, 71, 77, 78, 79, 80, 81, 84, 91, 93, 98, 100, 101, 102, 103, 104, 106, 107, 112], "d_": [5, 9, 38, 45, 53, 56, 71, 80, 81, 104, 108], "d_c": [53, 104], "d_h": [53, 104], "d_k": 5, "d_v": 5, "dab": [41, 53, 93], "dai": [41, 93], "daili": [1, 7], "dalf": [41, 52, 53, 54, 93, 101, 104], "damai": [41, 93], "dan": [41, 93], "dang": [41, 93], "danger": 76, "daniel": [41, 93], "dario": [41, 93], "data": [2, 6, 7, 8, 9, 20, 25, 27, 28, 29, 31, 32, 34, 39, 43, 44, 46, 51, 60, 64, 66, 67, 69, 70, 71, 72, 76, 78, 79, 80, 82, 83, 84, 88, 91, 100, 103, 105], "dataclass": [58, 59], "datalabel": 92, "dataset": [1, 6, 14, 16, 17, 18, 19, 20, 21, 22, 26, 27, 29, 33, 34, 35, 37, 38, 39, 46, 49, 52, 53, 54, 55, 56, 57, 60, 61, 64, 69, 70, 72, 79, 80, 81, 82, 84, 88, 90, 91, 92, 98, 111], "date": 35, "dateutil": 19, "dauphin": [41, 93], "dave": [41, 93], "david": [41, 93], "davinci": [14, 33], "dawn": [41, 93], "daya": [41, 93], "dayiheng": [41, 93], "dclt19": [7, 41, 93], "ddot": [106, 107], "de": [41, 51, 93], "deal": 83, "debat": 53, "debias": 60, "debug": [27, 39, 57, 61], "debugg": 64, "decad": 21, "decai": [55, 95, 103, 107], "decid": [52, 61, 82, 88, 92, 101], "decis": [43, 50, 64], "declin": [27, 39, 53], "decod": [7, 14, 20, 33, 34, 44, 48, 58, 60, 64, 77, 90, 98], "decompos": 56, "decomposit": 112, "decoupl": [53, 54, 65], "decreas": [17, 44, 45, 54, 65, 67, 68, 72, 80, 101], "dedic": 101, "deduc": 91, "dedupl": [9, 35, 51, 52, 57, 98], "deem": 83, "deep": [41, 93], "deepen": [27, 39], "deepseek": [20, 28, 29, 34, 41, 65, 70, 93, 101, 104, 111], "deepseekcod": [54, 111], "deepseekmath": 52, "deepseekmo": [53, 54], "deepseekv2attent": 53, "deepseekv2config": 53, "deepseekv2forcausallm": 53, "deepseekv2mlp": 53, "deepseekv2model": 53, "deepseekv2pretrainedmodel": 53, "deepseekv2rmsnorm": 53, "deepspe": 111, "def": [16, 18, 53, 58, 59, 80, 103, 105], "default": [12, 19, 42, 58, 65, 103], "defin": [1, 2, 24, 35, 36, 42, 43, 45, 46, 56, 57, 58, 59, 60, 61, 66, 67, 75, 82, 98, 102, 103, 106, 107, 108], "definit": [38, 102], "degener": 67, "degrad": [54, 57], "degre": [8, 56], "dejian": [41, 93], "deli": [41, 93], "deliber": 92, "delimit": [6, 90], "deliv": 57, "delta": [43, 56, 112], "delta_": [70, 75], "delta_t": 75, "demonstr": [1, 2, 6, 7, 8, 9, 13, 17, 43, 44, 54, 60, 61, 65, 69, 71, 72, 81, 86, 88, 95, 102, 103, 104], "deng": [41, 93], "dengr": [41, 93], "denni": [41, 93], "denomin": 43, "denot": [38, 43, 45, 48, 51, 64, 66, 68, 69, 75, 80, 81, 91, 101, 104, 105, 107], "dens": [8, 57, 60, 104], "densifi": 54, "densiti": 80, "depend": [6, 8, 19, 35, 41, 45, 58, 59, 67, 69, 72, 84, 90, 102, 103, 105, 106, 107], "depict": [7, 76, 88], "deploi": [57, 101], "deploy": 104, "depth": [27, 39], "deriv": [43, 51, 54, 80, 82, 103], "descend": 58, "descent": 6, "describ": [5, 6, 14, 25, 33, 50, 57, 77, 82, 102], "descript": [22, 25, 38, 48, 49, 50, 57, 78], "description2cod": 31, "design": [13, 17, 22, 26, 35, 37, 38, 54, 60, 68, 76, 78, 88, 92, 101, 104], "desir": [9, 43, 57, 72, 78, 80], "despit": [8, 46, 54, 71, 72, 75], "destabil": 75, "detail": [7, 42, 49, 50, 57, 59, 60, 61, 70, 72, 74, 75], "detect": [24, 36, 48, 57, 60, 72], "determin": [54, 57, 60, 61, 66, 69, 84], "determinist": [44, 54, 88], "detoken": 109, "devbal": 101, "develop": [1, 35, 46, 51, 54, 60, 66, 76, 80, 83, 88, 90, 91], "deviat": [44, 58, 67, 70, 75, 81], "devic": [54, 58, 59, 103, 110], "devis": [20, 21, 34], "devlin": [41, 93], "dfag17": [41, 60, 93], "dhariw": [41, 93], "diagon": [51, 58, 59], "dialog": 58, "dialogu": [1, 51, 56, 57], "diamond": 21, "dict": 58, "dictionari": [14, 33, 80], "did": [61, 77], "differ": [2, 5, 8, 9, 14, 15, 17, 24, 33, 36, 41, 43, 44, 46, 48, 49, 50, 51, 53, 55, 56, 57, 59, 60, 61, 65, 67, 68, 69, 70, 71, 72, 73, 76, 78, 80, 92, 98, 101, 102, 104, 106, 107], "differenti": 78, "difficult": [21, 46, 50, 98], "difficulti": [17, 21, 26, 27, 35, 37, 39, 49, 57, 61, 90], "dill": 19, "dim": [58, 59, 103, 105], "dimens": [5, 45, 51, 53, 58, 59, 64, 68, 101, 102, 103, 104, 105, 107, 108], "dimension": [5, 64, 106, 107], "diminish": [72, 98], "ding": [41, 93], "diogo": [41, 93], "direct": [8, 17, 21, 27, 39, 42, 50, 60, 61, 72, 77, 78, 79, 80, 88, 93], "directli": [6, 13, 18, 20, 34, 44, 51, 52, 57, 65, 67, 70, 75, 77, 80, 90, 91, 102, 103, 107, 109], "directori": 58, "disagr": 57, "disagre": 83, "disallowed_speci": 58, "disallowed_token": 58, "disanalogi": 46, "discard": [14, 33, 57, 61, 83], "discontinu": [27, 39], "discount": 75, "discrep": [43, 48, 56, 81], "discret": 56, "discrimin": [6, 48], "discuss": [51, 76, 92, 107], "disjoint": 25, "displai": [42, 72], "dispref": [67, 68], "disproportion": [57, 65], "distanc": [68, 103], "distil": [20, 34, 56, 78, 84], "distinct": [1, 53, 54, 60, 69, 72, 81, 98], "distinguish": [66, 69, 80], "distlib": 19, "distort": 105, "distract": 75, "distribut": [6, 8, 9, 27, 35, 39, 43, 48, 51, 54, 56, 58, 64, 65, 67, 69, 70, 71, 75, 76, 77, 80, 81, 83, 84, 88, 90, 91, 92, 101], "distro": 19, "div_": 58, "diverg": [9, 56, 67, 70, 75], "divers": [6, 7, 9, 13, 14, 17, 24, 25, 26, 27, 33, 35, 36, 37, 39, 44, 48, 49, 50, 51, 54, 57, 60, 61, 82, 83, 88, 90], "divid": [1, 5, 48, 50, 60, 70, 106, 107], "divis": 49, "dkv": 104, "do": [8, 14, 15, 20, 24, 25, 33, 34, 36, 41, 43, 46, 49, 50, 52, 56, 57, 58, 59, 71, 83, 88, 90, 91, 92, 102, 104], "do_train": 111, "docstr": [13, 18, 51], "doctyp": 110, "document": [6, 7, 20, 25, 34, 41, 42, 51, 57, 61, 103], "doe": [9, 24, 36, 51, 54, 58, 61, 67, 75, 81, 98, 103, 104, 105], "doesn": 103, "domain": [1, 7, 17, 21, 27, 39, 43, 54, 60, 66, 88, 90], "domin": 43, "don": 75, "done": [27, 39, 61], "dong": [41, 93], "dongji": [41, 93], "dot": [6, 43, 58, 59, 61, 68, 70, 72, 79, 82, 83, 101, 102, 103, 104, 106, 107, 109], "doubl": [50, 69, 98], "down": [49, 102, 103, 105], "down_proj": 53, "downstream": [8, 57, 112], "dpo": [28, 29, 44, 57, 60, 61, 73, 80, 82, 83], "dpop": [28, 57], "dq": 104, "dr": 7, "draw": [20, 34, 80], "drawback": 88, "drawn": [60, 69], "drive": 48, "drop": [17, 54], "drop_last": 73, "dschat": 73, "dtype": [58, 59], "du": [41, 93], "duan": [41, 93], "due": [35, 50, 54, 61, 64, 65, 101, 103, 105], "dulwich": 19, "duplic": 51, "dure": [2, 5, 6, 14, 17, 26, 33, 35, 43, 45, 48, 51, 52, 54, 56, 57, 60, 61, 65, 67, 75, 76, 88, 92, 95, 98, 101, 103, 104, 105], "dynam": [57, 72], "dz": 69, "e": [1, 2, 6, 9, 14, 15, 16, 18, 19, 20, 24, 33, 34, 36, 43, 46, 54, 56, 57, 58, 59, 61, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 80, 81, 82, 83, 88, 90, 91, 100, 101, 102, 103, 105], "e501": 58, "e_": 83, "e_j": 98, "each": [2, 5, 6, 7, 9, 12, 14, 17, 18, 20, 21, 22, 24, 25, 26, 27, 32, 33, 34, 35, 36, 37, 39, 43, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 60, 61, 64, 65, 66, 68, 69, 70, 71, 75, 76, 77, 78, 79, 80, 81, 82, 83, 90, 91, 92, 98, 101, 102, 103, 104, 105, 112], "earli": [76, 90], "earlier": [9, 56, 57, 76, 90], "easi": [12, 13, 43, 46, 50, 51, 61, 73, 80], "easier": [6, 46, 50, 65, 80, 103], "easili": [5, 15, 20, 34, 48], "eason": 15, "echo": [58, 98], "econom": [17, 41, 53, 54, 93], "ecosystem": 41, "ecut": 15, "edg": 61, "edit": [14, 22, 25, 33, 50, 57, 68, 91], "editor": 51, "educ": [35, 61], "edward": [41, 93], "ee": [28, 29], "effect": [1, 5, 20, 26, 34, 35, 46, 50, 52, 54, 60, 65, 66, 68, 70, 75, 77, 80, 81, 82, 95, 98, 103, 106, 107], "effici": [6, 41, 49, 53, 54, 60, 70, 75, 92, 93, 101, 104], "effort": [35, 52, 53, 54, 84], "eft": 82, "either": [2, 14, 33, 49, 53, 56, 57, 72, 75, 91, 92], "electron": 110, "element": [5, 9, 51, 71, 103, 104, 106, 107], "elementari": [17, 21], "elev": 53, "elicit": [46, 69, 76, 77], "elimin": [25, 27, 39, 75], "elizabeth": [41, 93], "elment": 58, "elo": 76, "els": [53, 58, 59, 76, 84, 103], "embed": [6, 41, 51, 53, 55, 58, 59, 60, 93, 106], "embed_token": 53, "emerg": 86, "emit": [2, 68], "emphas": [7, 91], "empir": [7, 43, 46, 98, 102, 103], "emploi": [1, 2, 5, 9, 12, 20, 27, 34, 35, 39, 44, 51, 52, 53, 54, 57, 58, 60, 61, 64, 70, 81, 88, 101], "empow": [27, 39, 41, 93], "empti": [14, 33, 101], "emptyset": 84, "enabl": [1, 2, 8, 21, 50, 51, 54, 66, 72, 88, 101, 102, 103, 106, 107], "encod": [6, 7, 24, 36, 48, 51, 53, 55, 58, 60, 102, 103, 106, 107, 110], "encode_dialog_prompt": 58, "encode_head": 58, "encode_messag": 58, "encoding_for_model": 58, "encompass": 52, "encount": [5, 57, 101], "encourag": [7, 24, 25, 36, 43, 46, 49, 57, 66, 78, 90], "end": [5, 6, 9, 12, 14, 20, 24, 33, 34, 36, 45, 49, 51, 54, 56, 58, 59, 65, 66, 67, 68, 70, 71, 76, 77, 78, 80, 84, 90, 91, 92, 95, 101, 102, 103, 104, 106, 107, 108], "end_header_id": 58, "end_of_text": 58, "enforc": [88, 90, 103], "engin": [1, 17, 25, 35, 61, 73], "english": [14, 33, 53], "enhanc": [1, 13, 17, 20, 34, 35, 38, 41, 44, 53, 54, 60, 61, 65, 81, 93], "enlarg": 98, "enlighten": [20, 34], "enlist": 54, "enough": [8, 15, 46, 53, 56, 68, 80, 104], "enrich": 13, "ensembl": [44, 81], "ensur": [1, 5, 9, 15, 21, 22, 26, 27, 39, 48, 54, 56, 57, 60, 61, 68, 75, 84, 92, 101, 105], "entail": 6, "entir": [31, 55, 91, 92, 105], "entri": [20, 22, 34, 35, 51, 58, 59], "entropi": [45, 48, 57, 65, 66, 69, 77], "entry_point": 16, "enumer": [58, 59, 103], "env": 19, "environ": [2, 9, 19, 43, 57, 61, 71, 75], "eo": [58, 75], "eos_id": 58, "eos_idx": 58, "eos_reach": 58, "eot_id": 58, "ep": [58, 59, 105], "episod": [9, 71, 73], "epoch": [27, 35, 39, 53, 55, 56, 69, 71, 73, 88, 92, 95], "epsilon": [58, 59, 65, 70, 75, 105], "epsilon_": 65, "equal": [43, 55, 65, 102, 104, 106], "equat": [80, 91, 98, 107], "equip": [90, 91, 104, 110], "equival": [7, 41, 48, 50, 78, 80, 93, 101], "erhang": [41, 93], "eric": [41, 93], "ermon": [41, 93], "error": [1, 2, 24, 36, 46, 50, 57, 58, 61, 70, 75, 108], "especi": [24, 36, 98, 105], "essenti": [48, 50, 72], "est": 100, "establish": [54, 80, 90], "estat": 100, "estim": [18, 43, 45, 49, 56, 67, 70, 75, 80, 100], "estrang": 100, "etc": [14, 24, 33, 36, 53], "ethic": [17, 76], "eval": [12, 15, 28, 29, 69], "eval_step": 73, "evalperf": 16, "evalplu": [20, 29, 34], "evalu": [2, 12, 14, 17, 18, 25, 26, 33, 37, 41, 43, 45, 46, 52, 56, 65, 76, 79, 81, 82, 88, 90, 91, 92, 93, 98, 104], "evan": [41, 93], "evas": 76, "even": [2, 8, 27, 39, 43, 46, 54, 57, 61, 67, 68, 80, 103, 104, 106, 107], "evenli": [32, 49], "event": [9, 69, 70], "everi": [1, 5, 8, 21, 24, 36, 77, 102, 105], "evid": [46, 103], "evol": [20, 34, 35], "evolut": [27, 39], "evolutionari": [27, 39], "evolv": [27, 39], "exact": [2, 21, 51], "exactli": [25, 43, 75], "exam": 17, "examin": [13, 66], "exampl": [8, 9, 13, 14, 18, 20, 24, 25, 33, 34, 35, 36, 41, 43, 46, 48, 50, 51, 53, 56, 57, 60, 64, 67, 68, 72, 76, 77, 78, 81, 82, 83, 84, 88, 90, 95, 102, 103, 105], "exce": [14, 33, 44, 58, 90], "exceed": [102, 103], "excel": 54, "except": [8, 27, 39, 44, 53, 54, 57, 58, 68, 104], "exceptiongroup": 19, "excess": [54, 65, 75], "exchang": 95, "exclud": 65, "exclus": [17, 55, 92], "execut": [2, 13, 15, 19, 25, 35, 41, 42, 48, 49, 51, 57, 60, 61, 93, 103], "executor": 61, "exemplar": [77, 86], "exemplifi": 78, "exhibit": [53, 65, 72, 88, 91], "exist": [1, 17, 18, 20, 24, 31, 34, 36, 48, 57, 61, 83, 90, 102, 103, 109], "exit": 90, "exp": [9, 58, 59, 64, 67, 80, 107, 108], "expand": [54, 60], "expans": 103, "expbal": 101, "expect": [9, 24, 36, 43, 49, 54, 57, 61, 65, 70, 72, 75, 76, 98, 103], "expens": [46, 51, 67, 84], "experi": [5, 6, 9, 21, 27, 39, 53, 56, 57, 66, 67, 69, 71, 73, 76, 77, 81, 92, 95], "experience_mak": 73, "experiment": [17, 35], "expert": [21, 41, 53, 54, 57, 61, 93], "expert1": 101, "expert2": 101, "expert3": 101, "expertis": 57, "explain": [50, 57, 61, 64, 77], "explan": [57, 77], "explicit": [7, 54, 102, 103, 106, 107], "explicitli": [14, 33, 56, 76, 78, 91], "exploit": [8, 65], "explor": [50, 56, 65, 66, 70, 72, 86, 88], "exponenti": 103, "export": 19, "express": [2, 8, 18, 67, 69], "extend": [29, 51, 53, 54, 57, 58, 60, 73, 84, 88], "extens": [2, 13, 19, 41, 50, 60, 72, 103], "extern": [1, 35, 88], "extra": [6, 15, 50, 90, 102, 103], "extract": [20, 34, 57, 64, 66, 77], "extractor": 64, "extrapol": [5, 102], "extrem": [5, 21, 43, 46, 54], "f": [16, 58, 59, 90, 100, 102, 103, 108], "f_": [64, 66, 84, 101, 106, 107], "face": [81, 88], "facilit": [5, 48, 72, 75], "fact": 103, "factor": [5, 45, 61, 64, 75, 101, 102, 103, 108], "factual": [60, 88], "fail": [25, 50, 57, 60, 61], "failur": [46, 57, 61], "fair": 104, "faith": 57, "faithfulli": [46, 72], "fake": 72, "fals": [53, 58, 59, 105], "famili": [46, 49, 51, 103], "fan": [41, 93], "fangyun": [41, 93], "fanjia": [41, 93], "far": [57, 65, 69, 75], "fashion": [24, 36, 91], "fast": 58, "fastavro": 19, "faster": [12, 65], "fastjsonschema": 19, "faulti": 57, "favor": [64, 90], "featur": [46, 51, 58, 64, 73, 105], "februari": 61, "fed": [6, 61], "federico": [41, 93], "feed": [45, 58, 59, 60, 101, 108], "feed_forward": [58, 59], "feedback": [9, 41, 46, 51, 52, 53, 54, 57, 60, 61, 67, 69, 72, 80, 82, 84, 88, 92, 93], "feedforward": 6, "fei": [41, 93], "felip": [41, 93], "feng": [41, 93], "few": [7, 8, 9, 17, 24, 36, 41, 48, 49, 51, 76, 77, 82, 84, 86, 92, 93], "fewer": [50, 53, 102, 103], "ff": [45, 108], "ffff": 110, "ffff\u7684\u8303\u56f4": 110, "ffn": [5, 53, 54, 58, 59, 60, 101], "ffn_norm": [58, 59], "fiction": 7, "field": [1, 14, 21, 24, 26, 33, 36, 37, 61], "fifo": 69, "fig": 2, "figur": [5, 45, 52, 54, 56, 69, 72, 76, 88, 98], "file": [14, 19, 25, 33, 35, 42, 51, 58, 61], "filelock": 19, "fill": [2, 51, 54, 56], "filter": [9, 15, 20, 25, 34, 35, 52, 53, 57, 60, 61, 65, 66, 83, 84, 90, 92, 98], "fim": [54, 61], "final": [5, 6, 7, 9, 21, 24, 25, 27, 36, 38, 39, 43, 49, 52, 53, 54, 55, 56, 57, 61, 64, 65, 66, 67, 71, 72, 75, 76, 77, 78, 79, 80, 82, 83, 84, 88, 90, 92, 101], "find": [14, 17, 25, 33, 44, 45, 46, 48, 49, 53, 56, 57, 65, 71, 76, 77, 80, 82, 88, 90, 91, 92, 98, 104], "fine": [8, 9, 14, 26, 27, 33, 35, 37, 39, 57, 61, 67, 70, 72, 78, 79, 80, 82, 91, 92, 95, 98, 102, 103], "finer": 101, "finest": 100, "finetun": [8, 20, 27, 34, 39, 46, 60, 76, 84, 90, 92], "finetuning_typ": 111, "finn": [41, 93], "fire": 16, "first": [1, 2, 5, 6, 7, 9, 15, 20, 24, 34, 35, 36, 43, 49, 50, 51, 52, 53, 54, 56, 57, 58, 59, 61, 65, 66, 67, 68, 69, 71, 72, 73, 77, 80, 81, 82, 83, 88, 90, 91, 92, 98, 101, 104, 107, 108], "first_k_dense_replac": 53, "firstli": 50, "fit": [25, 45, 69, 73, 80, 103], "five": [27, 39, 61], "fix": [5, 9, 25, 45, 50, 57, 69, 71, 92, 102, 105], "flag": 58, "flagopen": 26, "flash": [28, 29], "flash_attn": 73, "flatten": [58, 59, 103], "flavor": 41, "flaw": 61, "flexibl": [2, 66, 101], "flexibli": 101, "flip": [72, 91], "float": [58, 59, 80, 103, 105], "float32": [58, 59], "flow": 2, "fluenci": 43, "focu": [14, 25, 33, 35, 43, 55, 60, 69, 92], "focus": [12, 17, 19, 26, 35, 37, 52, 54, 60, 72, 88], "fold": 13, "follow": [1, 5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 19, 20, 21, 25, 27, 33, 34, 39, 41, 42, 51, 52, 54, 55, 56, 57, 58, 59, 60, 64, 65, 67, 68, 70, 71, 72, 75, 76, 77, 78, 80, 81, 83, 88, 93, 95, 101, 102, 103, 104], "fool": 92, "foral": [6, 66], "forc": [21, 48, 50, 90, 91], "forcefulli": 90, "forecast": 1, "forgo": 69, "form": [5, 27, 39, 49, 51, 54, 67, 69, 75, 78, 82, 92, 102, 103], "formal": [66, 70], "format": [2, 19, 21, 24, 27, 36, 39, 51, 54, 76, 88], "formatt": 58, "former": 66, "formul": [43, 66, 67, 75, 81, 101], "fortun": 67, "forum": [41, 93, 95], "forward": [45, 58, 59, 60, 101, 102, 105, 108, 112], "foster": 66, "fotio": [41, 93], "found": [5, 6, 9, 17, 24, 36, 48, 49, 50, 56, 61, 66, 76, 102, 103], "foundat": [20, 27, 34, 39, 51, 55, 57, 61, 75], "four": [7, 17, 27, 39, 51, 56, 57, 81, 88, 104], "frac": [5, 9, 18, 43, 45, 58, 59, 64, 65, 67, 68, 70, 71, 72, 75, 80, 81, 90, 101, 102, 103, 104, 105, 106, 107, 108, 109], "fraction": 18, "framework": [16, 24, 36, 53, 54, 60, 61, 72], "fraser": [41, 93], "free": [19, 41, 54, 61, 83, 93], "freez": [64, 112], "freq": [58, 59, 103], "freqs_ci": [58, 59, 103], "frequenc": [5, 51, 101, 103, 106, 107], "frequent": [102, 103], "fresh": 92, "friendli": 76, "from": [1, 5, 7, 8, 9, 12, 13, 14, 16, 17, 18, 19, 21, 24, 25, 27, 28, 31, 32, 33, 35, 36, 38, 39, 41, 44, 46, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 64, 65, 67, 68, 69, 70, 71, 75, 78, 79, 80, 81, 82, 83, 84, 88, 90, 91, 92, 93, 95, 98, 101, 102, 103, 104, 105, 107, 109, 110], "frontier": 21, "frozen": 65, "frozenlist": 19, "fsdp": 90, "fsfairx": 44, "fsspec": 19, "fu": [41, 93], "fulfil": [1, 72], "fuli": [41, 93], "full": [46, 50, 51, 52, 54, 58, 59, 73, 111], "fulli": [2, 5, 14, 33, 38, 46, 54, 61, 65], "function": [2, 5, 8, 9, 13, 15, 22, 25, 41, 43, 46, 50, 51, 55, 56, 60, 61, 65, 66, 67, 68, 69, 70, 71, 72, 75, 80, 81, 90, 91, 102, 103, 105, 106, 107, 108], "fundament": [8, 46], "further": [14, 20, 33, 34, 35, 44, 49, 51, 52, 53, 56, 57, 60, 61, 66, 69, 70, 84, 101, 103, 107], "furthermor": [54, 65, 76], "futur": [1, 46, 53, 54], "g": [1, 2, 19, 20, 24, 34, 36, 54, 56, 57, 61, 64, 65, 70, 77, 78, 80, 83, 88, 90, 91, 102, 106, 107], "g_": [54, 64, 101], "g_t": 75, "gabriel": [41, 93], "gae": [70, 75], "gain": [6, 8, 20, 34, 50, 56, 61, 72, 77], "gamma": [9, 43, 54, 58, 59, 70, 71, 72, 75, 102, 105], "gao": [41, 93], "gap": [2, 17, 20, 34, 43, 44, 46, 57, 61, 79], "gate": [41, 54, 64, 93, 101], "gate_proj": 53, "gather": [56, 58, 60], "gating_dim": 101, "gaussian": 108, "gave": [14, 33], "gb2312\u56e0\u4e3a\u53ea\u8868\u793a\u666e\u901a\u6570\u5b57": 110, "gbk\u662fascii": 110, "ge": [18, 24, 36, 41, 64, 66, 68, 70, 80, 93], "geglu": 108, "gelu": 108, "gemini": [49, 52, 69], "gen": 58, "gener": [1, 2, 5, 6, 7, 8, 9, 14, 15, 18, 19, 20, 25, 26, 27, 33, 34, 35, 37, 38, 39, 41, 43, 48, 49, 50, 51, 52, 53, 54, 56, 57, 60, 61, 65, 67, 68, 69, 70, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 86, 88, 90, 91, 93, 98, 102, 104], "generalist": 66, "generate_kwarg": 73, "generate_max_len": 73, "generation_logprob": 58, "generation_token": 58, "generativeai": 19, "generativelanguag": 19, "geometr": [5, 107], "geometri": 57, "get": [21, 41, 42, 43, 51, 61, 64, 80], "get_unique_el": 51, "gg": 45, "gibb": 67, "gibberish": [43, 65], "girish": [41, 93], "git": [16, 19], "github": [14, 16, 19, 25, 26, 33, 35, 48, 52, 55, 58, 59, 60, 61], "give": [6, 82, 83, 103], "given": [1, 5, 6, 9, 13, 14, 24, 25, 27, 33, 36, 39, 43, 45, 46, 48, 50, 51, 55, 56, 57, 58, 59, 60, 61, 64, 66, 67, 68, 69, 70, 71, 77, 78, 80, 82, 83, 84, 91, 92, 98, 102, 103], "glob": 58, "glu": [58, 59], "go": [73, 75], "goal": [6, 50, 61, 65, 67, 80, 83, 90, 91], "gold": [28, 48, 49], "gomez": [41, 93], "good": [15, 28, 43, 46, 48, 54, 57, 70, 80, 83], "googl": [16, 19], "googleapi": 19, "google\u7684bert\u6a21\u578b\u5728\u5206\u8bcd\u7684\u65f6\u5019\u4f7f\u7528\u7684\u662fwordpiece\u7b97\u6cd5": 109, "gpt": [7, 8, 9, 12, 14, 20, 21, 28, 29, 33, 34, 41, 46, 57, 58, 82, 92, 93, 98], "gpt2": 28, "gpt3": 28, "gpt4": 52, "gpu": [54, 90, 111], "gqa": [41, 60, 93], "grade": 21, "gradient": [5, 6, 9, 55, 68, 69, 71, 75], "gradient_accumulation_step": 111, "gradient_checkpoint": 73, "gradual": 51, "grai": [41, 93], "grain": [26, 37, 57, 72], "gram": [35, 61], "grammar": [24, 36], "grammat": [24, 36], "grangier": [41, 93], "granular": 17, "graphic": 21, "great": 58, "greater": 17, "greatli": [8, 112], "greedi": [7, 16, 20, 34, 44, 57, 58, 98], "greg": [41, 93], "gretchen": [41, 93], "grid": 103, "grl": [15, 41, 93], "ground": [2, 13, 22, 32, 46, 52, 53, 54, 61, 65, 66, 67, 80, 83], "group": [48, 49, 52, 53, 54, 58, 60, 61, 65, 81, 88, 101, 104], "grow": [5, 8, 69], "grpcio": 19, "grpo": [28, 29, 52, 60, 65, 66], "gsm8k": [28, 44, 61, 98], "gu": [41, 93], "guan": [41, 93], "guangbo": [41, 93], "guant": [41, 93], "guarante": [18, 66, 68, 70, 101], "guard": 31, "guess": 17, "guid": [24, 36, 44, 66, 80, 81, 88], "guo": [41, 93], "guowei": [41, 93], "guss": [41, 93], "h": [5, 41, 53, 56, 58, 59, 93, 101, 102, 104, 112], "h06a4308_0": 19, "h100": 90, "h11": 19, "h1181459_1": 19, "h1234567_1": 19, "h39e8969_0": 19, "h5eee18b_0": 19, "h5eee18b_1": 19, "h5eee18b_6": 19, "h6a678d5_0": 19, "h6a678d5_1": 19, "h800": 54, "h955ad1f_1": 19, "h_": [6, 103], "h_j": 103, "h_n": 6, "ha": [1, 2, 5, 7, 8, 9, 12, 21, 24, 36, 43, 46, 52, 53, 56, 57, 58, 59, 60, 65, 69, 70, 81, 84, 91, 98, 102, 104, 108], "hack": [54, 56, 64, 65, 76, 88], "had": [9, 22, 48], "half": [46, 48, 51], "hallucin": 61, "hallucinatori": 53, "halv": 5, "ham": 68, "han": [41, 93], "hand": [18, 22, 45, 46, 54, 69], "handl": [6, 61], "handwritten": 18, "hanq": [41, 93], "hanwei": [41, 93], "hao": [41, 93], "haoran": [41, 93], "haowei": [41, 93], "haoyu": [41, 93], "happen": 103, "har": 54, "hard": [49, 50, 67, 78, 104], "harder": 49, "harm": [76, 78], "harmless": [60, 77, 78, 82], "harri": [41, 93], "hat": [43, 65, 67, 70, 75, 81, 91], "have": [1, 5, 6, 7, 18, 20, 21, 24, 25, 34, 36, 42, 43, 45, 46, 49, 50, 52, 54, 56, 57, 60, 64, 65, 67, 68, 76, 80, 81, 82, 83, 90, 95, 98, 101, 102, 103, 105, 108], "hbb": [17, 41, 93], "hd_": 5, "he": [41, 93], "head": [6, 41, 45, 53, 54, 58, 59, 82, 93, 102, 103, 110], "head_dim": [58, 59], "health": 17, "heart": 83, "heavi": [45, 91, 104], "hebgen": [41, 93], "heewoo": [41, 93], "heidi": [41, 93], "height": 58, "held": 8, "help": [6, 9, 21, 41, 50, 51, 53, 54, 56, 57, 60, 61, 64, 69, 71, 76, 78, 82, 102], "helpfulli": 71, "helpsteer2": 28, "henc": [2, 52, 67, 82], "hendryck": [41, 93], "henighan": [41, 93], "henriqu": [41, 93], "herbert": [41, 93], "herd": 57, "here": [5, 8, 14, 27, 33, 39, 41, 43, 45, 55, 56, 57, 98, 102, 103], "hess": [41, 93], "heurist": [9, 20, 21, 24, 34, 36], "hf": 80, "hh": 76, "hidden": [53, 58, 59, 64, 101, 102, 108], "hidden_dim": [58, 59], "hidden_s": [53, 101], "high": [6, 13, 18, 20, 21, 25, 34, 35, 43, 44, 48, 54, 56, 57, 60, 61, 64, 65, 66, 68, 69, 75, 81, 82, 83, 84, 91, 95, 100, 106, 107], "highconfid": 84, "higher": [2, 26, 37, 44, 49, 50, 54, 57, 60, 61, 69, 72, 81, 101, 102], "highest": [12, 14, 21, 27, 33, 39, 44, 54, 57, 69, 81, 82, 100, 101, 106], "highli": [21, 46, 61, 64, 83, 84, 92], "highlight": [1, 35], "highqual": 54, "hilton": [41, 93], "hinder": [75, 81], "hing": 67, "histor": 70, "histori": [17, 43], "hoc": 77, "hold": [45, 92], "holdgraf_evidence_2014": 41, "holist": [19, 41, 93], "home": 19, "homepag": 65, "honesti": 64, "hongcheng": [41, 93], "honghui": [41, 93], "hongyi": [41, 93], "hood": [20, 34], "hook": 19, "hope": 21, "hotfix": 19, "hou": [41, 93], "hour": 54, "hous": 52, "how": [1, 9, 12, 17, 21, 25, 42, 43, 46, 49, 57, 61, 64, 66, 67, 70, 77, 78, 86, 88, 92, 98, 103], "howev": [1, 18, 24, 36, 43, 44, 46, 53, 56, 57, 64, 65, 67, 71, 72, 78, 80, 81, 90, 101, 102, 103, 105, 107], "hstack": [58, 59], "html": 110, "http": [16, 19, 20, 24, 27, 34, 36, 39, 41, 93], "httpcore": 19, "httplib2": 19, "httpx": 19, "hu": [41, 93], "huajian": [41, 93], "huan": [41, 93], "huang": [41, 93], "huazuo": [41, 93], "hub": 19, "huge": [50, 103], "huggingfac": [19, 73], "hugh": [41, 93], "hui": [41, 93], "human": [8, 9, 12, 15, 17, 21, 22, 24, 27, 32, 36, 39, 41, 43, 46, 51, 53, 54, 57, 60, 61, 67, 69, 71, 72, 76, 77, 78, 79, 80, 82, 83, 84, 88, 92, 93], "humanev": [13, 27, 29, 35, 39, 44, 61], "humanevalplu": 16, "humanevalplus_releas": 16, "hundr": [54, 103], "hunter": [41, 93], "hurt": 61, "hybridengin": 73, "hyc": [41, 61, 93], "hyper": [54, 55, 58, 59, 60, 65, 70, 72], "hyperparamet": [45, 49, 57, 60, 65, 68, 69, 72, 75, 95], "hypothes": 5, "hypothesi": 8, "i": [1, 6, 7, 8, 9, 12, 13, 14, 15, 16, 17, 18, 20, 21, 24, 25, 26, 27, 28, 29, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 45, 46, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 80, 81, 82, 83, 84, 88, 90, 92, 93, 98, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110], "i_": [24, 36], "i_t": [24, 36], "icl": 98, "id": [9, 41, 58, 93], "id1": 19, "id2": 19, "idea": [72, 84, 102, 103], "ideal": [1, 14, 17, 33, 103], "ident": [5, 60, 65, 67, 69, 83, 92, 101], "identif": 61, "identifi": [1, 9, 12, 13, 17, 24, 36, 44, 60, 61, 65, 66, 69, 72, 76, 78, 81, 84], "idna": 19, "ifev": 53, "ifstat": 2, "ift": 82, "ignor": [43, 48, 58, 103], "ignore_index": 58, "igor": [41, 93], "ij": 98, "ik_": [58, 59, 102, 103], "illeg": 76, "illia": [41, 93], "illustr": [46, 52, 54, 56, 58, 59, 88], "ilya": [41, 93], "im": [58, 59, 102, 103], "imag": [1, 45, 90], "imaginari": [102, 103], "imbal": [54, 101], "imit": 46, "impact": [35, 44, 49, 61, 65, 92, 102], "imped": 65, "imper": [14, 33], "imperfect": 84, "implement": [1, 9, 13, 18, 27, 39, 53, 54, 55, 60, 61, 75, 84], "impli": 65, "implicit": 67, "implicitli": [46, 67, 91], "import": [2, 13, 16, 18, 35, 43, 46, 56, 58, 59, 67, 80, 82, 83, 95, 103, 105], "importantli": [61, 67, 90, 104], "importlib": 19, "impress": 61, "improv": [6, 8, 20, 34, 38, 41, 43, 46, 48, 50, 51, 53, 54, 55, 56, 57, 58, 60, 61, 66, 68, 70, 77, 81, 82, 83, 84, 86, 88, 90, 91, 92, 93, 98, 105], "inabl": 65, "inappropri": 72, "incentiv": 91, "includ": [1, 6, 14, 17, 18, 21, 25, 26, 27, 31, 32, 33, 35, 37, 39, 42, 44, 45, 46, 48, 49, 51, 53, 54, 56, 57, 58, 60, 61, 67, 76, 79, 80, 102, 103, 105, 111], "inclus": 61, "incorpor": [5, 35, 54, 56, 57, 60, 65, 70, 75, 106, 107], "incorrect": [48, 50, 81, 84, 91, 92], "incorrectli": [21, 67], "increas": [17, 20, 27, 34, 35, 39, 49, 51, 54, 57, 65, 67, 68, 72, 98, 101, 103, 107], "increasingli": 88, "increment": 102, "inde": 72, "indent": 61, "independ": [24, 36, 51, 90, 105, 109], "index": [58, 59, 69, 70, 102, 103, 104], "indic": [9, 17, 49, 58, 64, 69, 72, 75, 78, 90, 98, 102, 103, 104, 106], "individu": [45, 65, 66, 105, 106, 107], "induc": [7, 54, 80], "inequ": 67, "inf": [58, 59], "infer": [14, 33, 35, 43, 51, 53, 54, 55, 57, 58, 65, 72, 73, 77, 80, 83, 101, 103, 104, 105], "inference_mod": [58, 59], "infin": 35, "inflat": 61, "influenc": [65, 66, 98], "infomax": 69, "inform": [1, 5, 9, 14, 21, 33, 35, 41, 42, 48, 56, 57, 60, 70, 81, 93, 101, 103, 106, 107, 110], "infrastructur": 103, "infti": 45, "inher": [6, 9], "inherit": 64, "init": 42, "init_kl_coef": 73, "initi": [1, 9, 20, 24, 27, 34, 36, 39, 50, 51, 53, 56, 57, 58, 65, 67, 69, 71, 76, 78, 84, 88, 95], "inject": [5, 102, 103, 112], "inlin": [41, 67], "inner": [106, 107], "innov": [53, 104], "input": [1, 5, 9, 14, 15, 20, 24, 27, 33, 34, 36, 39, 41, 48, 49, 50, 51, 55, 56, 58, 59, 61, 66, 68, 69, 71, 72, 75, 77, 80, 83, 95, 102, 103, 104, 105, 106, 107, 108, 109], "input_kei": 73, "input_text_mask": 58, "inputgen": 16, "inputoutput": 50, "insert": [5, 41, 48, 58, 69], "insid": 70, "insight": 67, "inspect": 22, "inspir": [20, 34, 66], "inst": 51, "instabl": [18, 75, 105], "instag": 57, "instal": 16, "instanc": [2, 6, 14, 25, 33, 53, 54, 58, 64, 76, 88], "instead": [5, 7, 9, 14, 18, 27, 33, 39, 46, 49, 51, 55, 58, 59, 65, 69, 70, 72, 75, 76, 84, 102, 103, 105, 107], "instruciton": [14, 33], "instruct": [1, 2, 8, 9, 12, 14, 25, 28, 29, 33, 41, 42, 44, 46, 52, 53, 56, 57, 64, 66, 72, 76, 77, 81, 88, 90, 91, 93, 95], "instructgpt": 71, "instruction_prefix": 16, "instructionfollow": 82, "int": [53, 58, 59, 80, 103, 105], "int_": 69, "integ": [58, 65], "integr": [17, 54, 60, 61, 66], "intend": 65, "intens": [53, 61], "intent": [9, 13, 81], "intention": 72, "interact": [43, 57, 58, 59, 69, 72, 75, 105], "interc": [64, 101, 102, 103, 104, 106], "interchang": 110, "interdepend": 105, "interest": [46, 57, 80], "interesting": 43, "interestingli": 61, "interfac": 9, "interfer": 105, "interleav": 91, "interlm2": 28, "intermedi": [38, 45, 46, 52, 53, 54, 86, 88, 92, 101], "intermediate_s": 53, "intern": [46, 54, 57], "internet": 71, "interpol": 51, "interpret": [9, 35, 56, 70], "interv": 98, "intervent": 90, "interview": [1, 51], "intric": 60, "intrigu": 88, "intrins": 88, "introduc": [1, 2, 12, 17, 20, 21, 24, 34, 35, 36, 38, 43, 49, 54, 57, 58, 60, 61, 65, 66, 70, 77, 80, 88, 101, 102, 103, 105], "introduct": 77, "intuit": [43, 46, 67, 107], "invas": 76, "invest": [53, 69, 102, 103], "investig": [54, 65, 66, 72, 92, 95, 98], "invok": 1, "involv": [14, 21, 33, 45, 54, 57, 101], "ion": [41, 93], "ip": 77, "ipo": 67, "ipynb": 41, "iq_": [58, 59, 102, 103], "irrelev": 35, "is_safeti": 56, "ise": 16, "isequival": 65, "isin": 58, "isol": 61, "issu": [7, 13, 25, 52, 54, 57, 64, 76, 77, 88, 91, 101, 103, 105], "item": [73, 80], "iter": [9, 24, 27, 36, 39, 50, 58, 60, 69, 71, 72, 80, 81, 82, 91], "itertool": 19, "its": [2, 8, 20, 22, 24, 25, 34, 36, 46, 48, 49, 51, 53, 54, 57, 58, 59, 60, 61, 64, 65, 67, 69, 72, 75, 76, 80, 82, 88, 90, 91, 92, 98, 101, 102, 103, 104, 105, 107, 108], "itself": [24, 36, 52, 82, 91], "ix_": [58, 59, 102, 103], "j": [41, 43, 54, 58, 59, 65, 66, 68, 70, 72, 75, 83, 93, 101, 102, 103, 104, 107], "j_": [70, 83], "j_1": 72, "j_q": 72, "jack": [41, 93], "jacob": [41, 93], "jain": [41, 93], "jakob": [41, 93], "jame": [41, 93], "jan": [41, 93], "jaraco": 19, "jare": [41, 93], "java": 61, "javascript": 61, "jeepnei": 19, "jeff": [41, 93], "jeffrei": [41, 93], "jerri": [41, 93], "jgzp23": [41, 60, 93], "jhg": [19, 41, 93], "ji": [41, 93], "jiaheng": [41, 93], "jiajun": [41, 93], "jian": [41, 93], "jiang": [41, 93], "jianhong": [41, 93], "jianlin": [41, 93], "jianwei": [41, 93], "jianxin": [41, 93], "jianzhong": [41, 93], "jiaqi": [41, 93], "jiashi": [41, 93], "jiatao": [41, 93], "jiawei": [41, 93], "jiaxi": [41, 93], "jie": [41, 93], "jin": [41, 93], "jingren": [41, 93], "jingxiang": [41, 93], "jingyang": [41, 93], "jinja2": 19, "jinz": [41, 93], "jmespath": 19, "joblib": 19, "john": [41, 93], "jointli": 5, "jone": [41, 93], "jong": [41, 93], "joseph": [41, 93], "josh": [41, 93], "joshua": [41, 93], "json": [2, 14, 33, 53, 58, 111], "jsonl": 16, "jsonlin": 19, "judg": [12, 29, 49, 50, 57, 72, 82, 83], "judgement": 72, "judgment": [43, 61], "jun": [41, 93], "junji": [41, 93], "junxiao": [41, 93], "junyang": [41, 93], "jupyt": [41, 42], "jupyterbook": 41, "jupytext": 42, "just": [17, 19, 41], "k": [5, 6, 7, 9, 18, 21, 27, 39, 41, 48, 54, 56, 57, 58, 59, 64, 66, 68, 69, 71, 75, 79, 93, 98, 101, 102, 103, 104, 106, 107, 112], "k_": [54, 58, 59, 66, 70, 101, 102, 103], "k_1": 70, "k_i": 70, "k_r": [54, 101], "kai": [41, 93], "kaig": [41, 93], "kaiser": [41, 93], "kang": [41, 93], "kaplan": [41, 93], "karl": [41, 93], "karma": 7, "katarina": [41, 93], "kati": [41, 93], "katti": 32, "ke": [41, 93], "keep": [49, 53, 54, 56, 57, 61, 65, 91, 101, 108], "keepdim": [58, 59, 105], "kei": [1, 5, 21, 35, 46, 49, 50, 51, 53, 54, 58, 59, 60, 61, 65, 72, 75, 80, 98, 102, 103, 105, 106, 107], "kelton": [41, 93], "keme": [41, 93], "kenton": [41, 93], "keqin": [41, 93], "kernel": 42, "kexin": [41, 93], "keyr": 19, "keyword": 12, "khlaaf": [41, 93], "kind": [9, 41, 72], "king": [41, 93], "kl": [9, 56, 67, 70, 71, 80], "kmh": [8, 41, 93], "knew": 43, "knight": [41, 93], "know": [43, 46, 98], "knowledg": [2, 17, 20, 34, 35, 50, 56, 95, 101, 106, 107], "known": [1, 43, 58, 59, 81, 103], "kosaraju": [41, 93], "koushik": [41, 93], "kr": 104, "kristina": [41, 93], "krueger": [41, 93], "kto": [28, 44], "kv": [53, 60], "kv_a_layernorm": 53, "kv_a_proj_with_mqa": 53, "kv_b_proj": 53, "kv_lora_rank": 53, "kw_": 5, "kyunghyun": [41, 93], "l": [6, 9, 24, 36, 38, 41, 43, 45, 56, 58, 59, 66, 67, 68, 69, 70, 71, 75, 80, 81, 82, 83, 84, 91, 93, 100, 101, 102, 103, 104, 105], "l_": [6, 65, 67, 72, 81], "l_1": 72, "l_2": 72, "label": [6, 8, 9, 24, 26, 36, 37, 46, 52, 53, 56, 57, 60, 71, 76, 78, 80, 83, 84, 92, 98], "labor": 61, "lack": [65, 80], "lambda": [6, 68, 69, 70, 72, 75, 102, 103], "lambda_": [64, 102], "land": 76, "langl": [58, 59, 102, 103, 107], "languag": [6, 7, 8, 9, 12, 13, 14, 17, 18, 21, 24, 25, 33, 35, 36, 40, 41, 46, 48, 49, 50, 51, 52, 53, 54, 55, 56, 58, 61, 67, 69, 71, 84, 88, 90, 92, 93, 95, 101, 106, 107], "larg": [5, 6, 7, 8, 9, 12, 13, 15, 18, 24, 25, 35, 36, 40, 41, 43, 46, 49, 53, 54, 56, 57, 58, 59, 61, 65, 67, 68, 69, 71, 75, 84, 88, 93, 95, 101, 104, 107], "larger": [7, 25, 26, 37, 56, 57, 90, 92, 103, 107], "largest": [7, 26, 48, 49, 55, 56, 57], "last": [5, 8, 48, 57, 58, 64, 70, 76, 92], "latent": [53, 54], "later": [24, 36, 57, 90], "latest": [16, 56, 57], "latex": 21, "law": [8, 17, 41, 46, 60, 93, 104], "layer": [5, 6, 7, 8, 9, 45, 48, 53, 55, 58, 59, 60, 64, 71, 73, 101, 102, 103, 104, 108, 112], "layer_id": [58, 59], "layer_idx": 53, "layernorm": [5, 105], "lcb": 19, "lcb_runner": 19, "lcft": 57, "ld_impl_linux": 19, "le": [18, 27, 39, 41, 54, 65, 68, 90, 93, 101, 103, 108], "lead": [35, 43, 46, 50, 51, 54, 55, 57, 65, 68, 76, 92, 102, 103, 105], "leakag": [31, 46, 61], "lean": [41, 93], "learn": [2, 5, 6, 7, 8, 9, 17, 21, 35, 43, 44, 45, 46, 48, 51, 55, 58, 59, 61, 65, 66, 67, 70, 75, 78, 80, 81, 82, 93, 95, 101, 108], "learnabl": 58, "learner": [7, 8, 41, 93], "learning_r": 111, "least": [7, 8, 12, 25, 49, 76], "leather": [41, 93], "leav": [48, 50, 65], "lebr\u00f3n": [41, 93], "lecong": [41, 93], "led": [56, 57, 76], "lee": [41, 93], "leed": 101, "leetcod": [19, 29, 52, 54, 88], "left": [5, 9, 18, 43, 45, 49, 51, 58, 59, 64, 65, 67, 68, 70, 71, 72, 75, 77, 80, 91, 102, 103, 104, 106, 107, 109], "leftarrow": [75, 104], "legal": 76, "lei": [41, 93], "leik": [41, 93], "len": [58, 73, 80], "length": [5, 45, 51, 53, 54, 56, 57, 58, 64, 65, 68, 72, 77, 90, 101, 102, 103, 104, 105], "lengthen": 90, "lengthi": 50, "less": [5, 9, 14, 24, 28, 29, 33, 36, 44, 52, 57, 65, 69, 70, 76, 81, 92, 98, 102], "let": [21, 41, 42, 43, 53, 64, 66, 68, 75, 76, 80, 84, 93, 102, 104, 106, 107], "level": [7, 13, 17, 21, 22, 26, 27, 35, 37, 39, 41, 43, 46, 49, 51, 53, 54, 56, 57, 60, 61, 69, 92, 93], "leverag": [1, 20, 24, 34, 36, 50, 54, 56, 60, 88, 103, 106, 107], "lex": 103, "leyi": [41, 93], "lezama": [41, 93], "li": [41, 93], "liang": [41, 93], "libffi": 19, "libgcc": 19, "libgomp": 19, "librari": [2, 13, 35, 51], "libstdcxx": 19, "libuuid": 19, "lie": [76, 104], "lightman": [41, 93], "like": [1, 6, 13, 14, 17, 33, 41, 42, 43, 46, 50, 54, 60, 61, 68, 69, 76, 78, 83, 88, 92], "likelihood": [6, 43, 44, 48, 67, 68, 77, 80, 81, 84, 92], "limit": [2, 8, 9, 24, 36, 46, 48, 49, 52, 54, 56, 57, 65, 69, 77, 80, 95, 101, 102, 103, 104], "lin": [41, 93], "line": [20, 24, 25, 27, 34, 36, 39, 41, 42, 45, 46, 51, 58], "linear": [5, 6, 51, 53, 55, 56, 58, 59, 60, 64, 65, 90, 98, 103, 107], "linearli": [5, 49, 95], "lingm": [41, 93], "link": 7, "linter": 57, "linzheng": [41, 93], "liqun": [41, 93], "list": [14, 27, 33, 39, 49, 50, 51, 58, 73, 80, 98, 100], "liter": 58, "littl": 61, "litwin": [41, 93], "liu": [41, 93], "livecodebench": [28, 29, 41, 93], "liyu": [41, 93], "lkb": [21, 41, 93], "ll": [41, 58, 104, 112], "llama": [14, 15, 28, 33, 44, 58, 73, 82, 95, 98, 102, 105], "llama2": [28, 56, 98], "llama3": [28, 29, 59], "llion": [41, 93], "llm": [1, 12, 13, 19, 20, 27, 34, 35, 38, 39, 40, 41, 50, 51, 56, 57, 61, 64, 66, 67, 68, 70, 72, 75, 78, 82, 83, 88, 91, 93, 103], "llm4code": 16, "lm": [7, 25, 71], "lm_head": 53, "ln": [41, 58, 69, 93, 103, 105], "load": [54, 58], "load_checkpoint": 73, "load_state_dict": 58, "load_tiktoken_bp": 58, "local": [8, 91], "localhost": 111, "locat": [51, 103], "log": [6, 7, 9, 43, 49, 56, 58, 67, 68, 70, 71, 72, 75, 76, 77, 80, 81, 91, 92, 98, 109], "logging_step": [73, 111], "logic": [50, 54, 60, 61, 88], "logist": 81, "logit": [48, 56, 58, 67, 68, 102], "logprob": 58, "logprobs_i": 58, "long": [6, 9, 14, 25, 33, 41, 57, 58, 61, 65, 68, 88, 90, 93, 102, 103, 107], "longer": [5, 51, 53, 64, 65, 68, 77, 90, 95, 102, 103], "longterm": [41, 93], "look": 50, "loop": 2, "loos": 103, "lora": [28, 29], "lose": 82, "loss": [8, 9, 43, 45, 46, 48, 53, 54, 56, 57, 64, 66, 68, 69, 70, 71, 72, 77, 81, 95], "lot": [41, 102], "low": [15, 24, 35, 36, 41, 43, 56, 61, 65, 66, 68, 70, 84, 90, 93, 102, 107, 112], "lower": [2, 13, 14, 33, 65, 68, 98, 102], "lowest": [81, 82, 100, 101, 102], "lr": 35, "lr_scheduler_typ": 111, "lu": [41, 93], "luan": [41, 93], "lukasz": [41, 93], "luke": [41, 93], "luo": [41, 93], "lxwz23": [16, 41, 93], "lynx": 1, "m": [6, 19, 24, 36, 41, 54, 56, 58, 59, 66, 68, 69, 72, 80, 81, 93, 101, 102, 103, 106, 107], "m3toolev": 2, "m_": 80, "m_0": 82, "m_1": 82, "m_2": 82, "m_3": 82, "m_t": 82, "ma": [41, 93], "machin": [5, 7, 41, 51, 93], "maddi": [41, 93], "made": [27, 39, 60, 69, 77, 81], "magic": [28, 29], "magicod": [16, 35, 41, 57, 61, 93], "magnitud": [5, 7, 8, 46, 48, 103, 104], "mai": [5, 13, 35, 44, 46, 50, 52, 54, 57, 65, 68, 70, 76, 77, 78, 81, 88, 101, 103], "mail": 7, "main": [16, 19, 44, 46, 49, 50, 51, 55, 56, 57, 66], "mainli": [46, 50, 54, 61, 66, 81, 88], "mainstream": 61, "maintain": [52, 53, 54, 56, 60, 61, 75, 101], "major": [21, 57, 61, 78, 90, 92], "make": [1, 5, 6, 9, 14, 17, 24, 25, 26, 33, 36, 43, 46, 49, 51, 58, 59, 60, 64, 65, 67, 69, 72, 73, 76, 82, 88, 90, 91, 103, 105, 106, 107], "make_experience_list": 73, "man": [41, 93], "manag": 19, "mani": [7, 8, 21, 24, 25, 36, 41, 42, 51, 61, 71, 103, 110], "mann": [41, 93], "manner": [82, 83], "manta": [41, 93], "manual": [22, 24, 36, 65, 95], "map": 5, "map_loc": 58, "margin": [27, 39, 44, 56, 57], "mark": [41, 51, 57, 93], "markdown": [16, 52], "markdownfil": 42, "markedli": 41, "markup": 41, "markupsaf": 19, "mask": [5, 48, 51, 57, 58, 59, 65], "mass": [27, 39, 43, 58], "massiv": [17, 41, 60, 61, 93], "master_port": 111, "match": [2, 21, 26, 37, 54, 58, 60, 91, 102, 103, 104], "mateusz": [41, 93], "math": [17, 41, 52, 53, 54, 58, 59, 60, 61, 65, 68, 70, 88, 90, 92, 93], "mathbb": [5, 9, 18, 43, 56, 64, 65, 67, 68, 70, 71, 72, 75, 80, 81, 90, 91, 101, 102, 104, 106, 107, 112], "mathbf": [5, 43, 51, 54, 58, 59, 66, 70, 72, 75, 101, 102, 103, 104, 105, 106, 107], "mathcal": [6, 38, 43, 56, 64, 65, 67, 68, 69, 79, 80, 81, 83, 84, 90, 91, 98, 101, 104, 108], "mathemat": [17, 18, 21, 41, 52, 53, 54, 57, 60, 61, 67, 70, 90, 93], "mathmix": 92, "mathrm": 79, "matmul": [58, 59], "matplotlib": 67, "matric": [5, 58, 59, 104, 108, 112], "matrix": [5, 6, 51, 58, 59, 104, 106, 107, 108], "matthew": [41, 93], "matthia": [41, 93], "max": [5, 56, 58, 59, 65, 68, 80, 84, 90, 91, 102, 108], "max_": [66, 67, 80, 90, 103], "max_batch_s": [58, 59], "max_epoch": 73, "max_gen_len": 58, "max_prompt_len": 58, "max_reward": 80, "max_sampl": 73, "max_seq_len": [58, 59], "maxim": [6, 9, 14, 20, 27, 33, 34, 39, 43, 49, 55, 65, 67, 69, 70, 71, 80, 84, 91, 92, 102, 103], "maximum": [2, 43, 48, 51, 56, 57, 58, 65, 66, 67, 69, 80, 90, 102, 103, 104], "mayer": [41, 93], "mazeika": [41, 93], "mbox": [75, 107], "mbpp": [27, 28, 29, 35, 39, 61], "mbppplu": 16, "mbppplus_releas": 16, "mccandlish": [41, 93], "mceval": [35, 41, 61, 93], "mcgrew": [41, 93], "md": [41, 42], "me": 76, "mean": [5, 56, 58, 59, 65, 68, 69, 70, 75, 81, 82, 101, 105, 107], "meaning": 50, "meansquar": [58, 59, 105], "meanwhil": [102, 103, 106, 107], "measur": [9, 17, 41, 43, 48, 57, 70, 77, 84, 90, 93, 95], "mechan": [5, 49, 54, 60, 61, 65, 70, 75, 101, 105, 106, 107], "media": 7, "median": 56, "medium": 51, "mei": [41, 93], "melani": [41, 93], "memori": [5, 6, 15, 70, 104], "men": [41, 93], "meng": [41, 93], "mention": 12, "merg": [25, 27, 39], "mergeable_rank": 58, "messag": [50, 58], "met": 57, "meta": [58, 66, 110], "metadata": [19, 49], "method": [8, 20, 25, 27, 34, 35, 39, 44, 54, 56, 57, 58, 60, 61, 66, 67, 69, 72, 75, 78, 81, 86, 90, 102, 103, 106, 107], "methodologi": [54, 88], "meticul": [53, 61], "metric": [1, 18, 27, 39, 43, 46, 48, 64, 77, 90, 98], "miao": [41, 93], "miaojun": [41, 93], "michael": [41, 93], "michiel": [41, 93], "micro_rollout_batch_s": 73, "micro_train_batch_s": 73, "middl": [21, 24, 36, 51, 54], "might": [12, 57, 95, 105], "mikhail": [41, 93], "mile": [41, 93], "miller": [41, 93], "million": [7, 8, 46, 48, 49, 51, 57, 60, 61], "min": [58, 64, 65, 70, 75, 80, 90, 112], "min_": 67, "min_prompt_len": 58, "mine": 80, "ming": [41, 93], "mingchuan": [41, 93], "mingfeng": [41, 93], "minghua": [41, 93], "minghui": [41, 93], "mingm": [41, 93], "mini": [58, 65, 105], "minim": [54, 67, 69, 90, 102, 103], "minimis": [48, 68], "minimum": [67, 75, 90], "minor": 91, "minu": 58, "minut": [15, 90], "mira": [41, 93], "misalign": 72, "mishkin": [41, 93], "mishra": [41, 93], "mismatch": [68, 91], "misra": [41, 93], "miss": 51, "mistak": [50, 72, 91], "mistralai": 19, "mitchel": [41, 93], "mitig": [9, 35, 53, 54, 57, 61, 64, 70, 71, 75, 77, 81, 84, 91, 101, 103], "mix": [9, 48, 51, 52, 57, 61, 71, 80, 88], "mixtral": 73, "mixtur": [41, 53, 54, 60, 73, 93], "mk": 101, "ml": [8, 103], "mla": [29, 53, 54], "mle": [67, 72, 80], "mlp": 64, "mmlu": 44, "mn": 101, "mode": [46, 67], "model": [1, 2, 6, 12, 13, 14, 15, 16, 17, 18, 20, 21, 24, 25, 26, 27, 28, 29, 33, 34, 35, 36, 37, 39, 40, 41, 43, 44, 46, 48, 50, 51, 52, 53, 54, 55, 67, 68, 70, 73, 75, 76, 77, 78, 79, 80, 90, 93, 95, 101, 104, 105, 106, 107, 112], "model_arg": 58, "model_name_or_path": 111, "modelarg": [58, 59], "modern": [46, 110], "modest": 92, "modif": [6, 7, 14, 27, 33, 39, 48, 57, 102], "modifi": [5, 14, 33, 51, 56, 57, 78, 82, 83, 91, 103, 112], "modul": [19, 35, 53, 58, 59, 61, 101, 105], "modular": 50, "modulelist": [58, 59], "modulenotfounderror": 58, "moe": [29, 53, 54, 60, 101, 104], "moe_intermediate_s": 53, "moegat": 101, "mohammad": [41, 93], "monoton": 80, "mont": [43, 70], "month": 56, "more": [5, 6, 7, 8, 9, 12, 13, 14, 16, 17, 19, 20, 21, 24, 25, 26, 27, 28, 29, 33, 34, 35, 36, 37, 39, 42, 46, 48, 49, 50, 51, 52, 53, 55, 56, 57, 59, 60, 61, 64, 65, 66, 68, 69, 70, 71, 72, 74, 76, 78, 80, 81, 88, 92, 98, 101, 102, 104], "moreov": [2, 41, 53, 65, 91], "morikawa": [41, 93], "most": [2, 5, 7, 8, 21, 35, 41, 44, 51, 54, 56, 57, 58, 61, 64, 72, 82, 92, 101, 103, 110], "mostli": [22, 57], "motiv": [7, 67, 80, 84, 90], "move": [7, 51, 75], "mpmath": 19, "msc": 90, "msgpack": 19, "mt": 101, "mtp": 54, "mu_": 75, "mu_a": 75, "much": [13, 14, 25, 33, 46, 51, 57, 65, 67, 76, 92, 102, 103, 107], "multi": [6, 28, 41, 45, 51, 53, 55, 57, 58, 59, 61, 93], "multidict": 19, "multihead": 5, "multilingu": [41, 57, 60, 61, 93], "multinomi": 58, "multipl": [1, 2, 9, 17, 21, 24, 35, 36, 38, 44, 50, 54, 57, 58, 59, 61, 69, 73, 76, 82, 90, 91, 101, 102, 104, 107], "multiple_of": [58, 59], "multipli": [5, 54, 64, 106, 107], "multiprocess": 19, "multistag": 60, "multitask": [7, 17, 41, 51, 93], "murati": [41, 93], "murtadha": [41, 93], "must": [1, 5, 41, 43, 45, 51, 68, 90, 92, 102, 103], "my": 76, "n": [5, 6, 16, 18, 19, 27, 28, 29, 38, 39, 41, 43, 44, 48, 51, 54, 58, 59, 66, 69, 72, 80, 82, 83, 91, 92, 93, 100, 101, 102, 103, 105, 106, 107, 108, 109], "n_": [24, 36, 45, 66, 104], "n_h": [53, 104], "n_head": [58, 59], "n_layer": [58, 59], "n_routed_expert": [53, 101], "n_shared_expert": 53, "n_t": [24, 36], "n_vocab": 58, "n_word": 58, "nabla_": [43, 67, 68, 75], "naiv": [46, 65, 67, 98, 101], "nakano": [41, 93], "naman": [41, 93], "name": [19, 27, 31, 39, 48, 50, 58, 61, 67, 108], "nano": 69, "narrow": [8, 20, 34], "nativ": 57, "natur": [7, 8, 12, 13, 24, 36, 46, 48, 50, 51, 52, 57, 58, 61, 77, 86, 88, 102, 103, 105], "nccl": 19, "ncurs": 19, "nderstand": 15, "ndim": [58, 59, 103], "ne": [66, 68, 98], "nearbi": 106, "nearli": [5, 7, 46, 61, 65, 69], "necess": 53, "necessari": [64, 65, 82, 95], "necessit": 101, "necssari": [14, 33], "need": [1, 8, 13, 24, 35, 36, 41, 42, 46, 50, 52, 53, 56, 57, 58, 59, 70, 75, 81, 93, 98, 103, 104], "neelakantan": [41, 93], "neg": [43, 60, 61, 64, 68, 72, 78, 81, 84, 91, 92, 98], "negligibli": 56, "neighbor": 76, "net": [41, 93], "network": [6, 41, 55, 58, 59, 60, 64, 75, 93, 101, 108], "networkx": 19, "neural": [5, 6, 41, 58, 59, 88, 93, 108, 109], "neutral": 92, "never": 101, "new": [5, 7, 8, 9, 14, 17, 19, 20, 24, 27, 31, 33, 34, 36, 39, 46, 48, 49, 51, 56, 57, 58, 59, 60, 64, 68, 69, 70, 71, 75, 82, 90, 98, 100, 108], "newli": [27, 31, 39], "newlygener": [24, 36], "next": [5, 6, 19, 24, 36, 43, 48, 50, 57, 69, 71, 76, 82, 83, 103, 107], "next_token": 58, "ng": 19, "ni": [41, 93], "nichol": [41, 93], "nichola": [41, 93], "nick": [41, 93], "nie": [41, 93], "niki": [41, 93], "nikola": [41, 93], "ning": [41, 93], "nl": 79, "nll": 57, "nlp": [6, 8, 9, 46, 71, 100, 105], "nn": [53, 58, 59, 101, 103, 105], "noam": [41, 93], "node": [54, 101], "nois": [65, 81], "noisi": [52, 81, 83, 84], "non": [8, 14, 21, 24, 33, 36, 54, 55, 60, 64, 65, 76, 80, 88, 90, 107], "none": [53, 58, 59], "nonlinear": [58, 59, 108], "noqa": 58, "norm": [58, 59], "norm_ep": [58, 59], "normal": [5, 7, 19, 55, 59, 60, 70, 73, 76, 77, 81, 101], "normalize_reward": 73, "normalized_shap": 58, "notabl": [12, 44, 52, 64, 103], "note": [40, 41, 45, 50, 54, 58, 67, 68, 69, 70, 83], "notebook": 41, "notin": [72, 80], "novel": [20, 27, 34, 39, 66, 72, 80, 83, 106, 107], "novelti": 18, "novemb": 52, "now": [50, 53, 64, 67, 68, 83, 91, 103], "np": [18, 67, 80], "nuanc": [60, 88], "nucleu": 58, "num": 104, "num_attention_head": 53, "num_base_token": 58, "num_channel": 58, "num_episod": 73, "num_experts_per_tok": 53, "num_featur": [58, 105], "num_head": 53, "num_reserved_special_token": 58, "num_sampl": [58, 80], "num_step": 5, "num_train_epoch": 111, "number": [1, 2, 6, 8, 9, 18, 24, 27, 36, 39, 44, 45, 46, 48, 51, 52, 53, 58, 65, 69, 70, 80, 90, 92, 101, 102, 103, 104, 105, 107, 108, 112], "numer": [18, 68, 75, 98], "numpi": [18, 19, 67, 80], "nvidia": [19, 90], "nvjitlink": 19, "nvrtc": 19, "nvtx": 19, "nw": 79, "o": [5, 15, 70, 78, 100, 104, 107], "o1": [28, 90], "o_": [53, 65, 70], "o_1": [70, 78], "o_2": [70, 78], "o_g": 70, "o_i": 65, "o_proj": 53, "obei": 104, "object": [2, 6, 9, 38, 43, 48, 49, 51, 54, 56, 60, 65, 67, 70, 71, 72, 75, 80, 84, 101], "observ": [2, 27, 35, 39, 44, 46, 50, 53, 57, 64, 65, 69, 77, 81, 88, 95, 98, 102, 104], "obstacl": [1, 43], "obtain": [5, 6, 14, 20, 27, 33, 34, 39, 49, 52, 53, 57, 60, 61, 64, 65, 69, 77, 80, 90, 91, 98], "obviou": 43, "occasion": [8, 57], "occur": 88, "occurr": 88, "od": 15, "off": [24, 36, 41, 42, 44, 65, 72, 77], "offer": [61, 69, 72, 77], "offici": 65, "offlin": [28, 29, 43, 48, 53, 61, 72], "offset": 5, "often": [6, 8, 24, 36, 43, 50, 65, 72, 77, 78, 81, 84, 105], "ofthought": 86, "ol": 100, "old": [65, 70, 75, 100], "older": [35, 100], "oliveira": [41, 93], "omit": [53, 101, 104, 108], "onc": [5, 14, 25, 27, 33, 39, 45, 104], "one": [2, 5, 8, 9, 12, 24, 25, 27, 35, 36, 39, 41, 43, 44, 46, 48, 49, 51, 54, 56, 57, 58, 59, 60, 61, 64, 66, 68, 69, 70, 72, 77, 78, 80, 82, 90, 92, 98, 101, 103, 105, 108], "ones": [5, 31, 53, 56, 57, 58, 59, 61, 84, 90, 101, 103, 105], "ones_lik": [58, 59, 103], "onli": [8, 9, 13, 14, 15, 17, 21, 24, 33, 35, 36, 46, 48, 49, 51, 54, 56, 57, 58, 59, 61, 64, 66, 67, 68, 70, 72, 76, 78, 82, 83, 84, 90, 91, 92, 95, 102, 103, 104, 106, 107], "onlin": [28, 29, 31, 49, 53, 65, 66, 72], "open": [12, 14, 25, 27, 29, 32, 33, 35, 39, 41, 54, 58, 60, 61, 65, 82, 88, 93], "openai": [7, 9, 19, 21, 41, 58, 71, 93], "opencod": 29, "openr1": 29, "openreview": [41, 93], "opensourc": 60, "openssl": 19, "oper": [6, 21, 27, 39, 69, 75, 103, 105], "opportun": 69, "oppos": [51, 108], "opposit": [72, 78], "optim": [6, 9, 14, 28, 33, 41, 43, 44, 45, 50, 52, 53, 54, 56, 60, 61, 65, 66, 69, 71, 75, 79, 80, 83, 88, 91, 93, 103], "optima": 91, "optimis": 68, "option": [14, 16, 33, 43, 53, 58, 59, 76, 77, 90, 103], "opu": 52, "oracl": [25, 43, 44, 91], "order": [5, 6, 7, 8, 9, 46, 48, 49, 50, 51, 53, 56, 67, 71, 76, 77, 81, 82, 83, 101, 102, 103, 104, 106, 107], "org": [20, 24, 27, 34, 36, 39, 41, 93], "organ": [61, 67], "origin": [7, 16, 17, 24, 27, 36, 39, 49, 50, 54, 55, 56, 57, 64, 65, 66, 72, 77, 81, 82, 83, 101, 102, 103, 107, 108], "orjson": 19, "orthogon": [20, 34], "oss": [41, 93], "other": [1, 5, 6, 9, 17, 27, 32, 39, 41, 42, 43, 46, 50, 51, 53, 54, 57, 58, 59, 61, 69, 72, 80, 82, 84, 91, 93, 98, 101, 105, 108, 110], "otherwis": [43, 54, 56, 65, 66, 72, 80, 92, 101, 102], "otim": [58, 59, 108], "our": [1, 2, 5, 6, 7, 9, 13, 14, 15, 17, 20, 21, 24, 25, 27, 33, 34, 35, 36, 39, 43, 46, 48, 49, 51, 52, 53, 54, 55, 56, 57, 60, 61, 65, 67, 68, 69, 70, 71, 76, 77, 79, 81, 82, 83, 88, 90, 91, 92, 98, 103, 106, 107, 112], "ourselv": 49, "out": [9, 25, 49, 56, 57, 58, 59, 65, 66, 69, 71, 90, 98, 102, 103, 104], "out_logprob": 58, "out_token": 58, "outbound": 7, "outcom": [65, 66, 88], "outdat": 35, "outer": [58, 59, 103], "outermost": 73, "outlier": 75, "outlin": [27, 39, 81, 88], "outperform": [44, 46, 51, 52, 53, 54, 61, 77, 92, 98], "output": [2, 5, 6, 9, 14, 15, 19, 20, 24, 33, 34, 36, 42, 43, 44, 45, 48, 49, 50, 51, 54, 55, 56, 57, 58, 59, 60, 61, 64, 65, 66, 70, 71, 72, 78, 81, 83, 84, 95, 101, 104, 105, 107], "output_dir": 111, "outsid": 81, "ouyang": [41, 93], "over": [5, 6, 7, 8, 9, 17, 19, 43, 45, 49, 51, 53, 56, 57, 58, 59, 60, 61, 65, 69, 70, 71, 72, 73, 77, 81, 82, 84, 91, 92, 104, 105, 107], "overal": [5, 6, 24, 35, 36, 45, 48, 54, 57, 64, 65, 70, 72, 77], "overcom": 1, "overconfid": 81, "overfit": [26, 71, 81], "overlap": [35, 61], "overload": 54, "oversight": 21, "overthink": 54, "overview": [41, 58, 59], "overwrite_cach": 111, "owj": [9, 41, 93], "own": [57, 72, 76, 82, 91, 92], "p": [6, 9, 38, 43, 44, 56, 58, 67, 69, 70, 72, 78, 80, 107, 109], "p_": [43, 66, 69, 81, 84, 91, 101], "p_1": 91, "p_i": [38, 66], "pa": 38, "pack": [5, 111], "packag": [19, 25, 35], "pad": 105, "pad_id": 58, "padding_idx": 53, "page": [41, 42], "paino": [41, 93], "pair": [5, 6, 7, 9, 14, 15, 25, 33, 35, 43, 46, 50, 51, 53, 55, 60, 61, 65, 69, 71, 72, 76, 77, 78, 79, 80, 81, 82, 84, 90, 91], "pairwis": [9, 56, 64, 66, 78, 79, 80, 82, 83], "palm": [58, 59], "pamela": [41, 93], "pan": [41, 93], "panda": 19, "panpan": [41, 93], "paper": [9, 18, 20, 21, 24, 27, 34, 36, 39, 40, 43, 49, 50, 51, 57, 98, 103], "par": 77, "paradigm": [8, 66, 72], "parallel": [5, 48, 58, 61, 90, 101, 105], "paralleliz": 5, "param": [18, 58, 59], "paramet": [5, 6, 7, 8, 9, 20, 34, 35, 44, 49, 51, 54, 55, 56, 57, 58, 59, 60, 64, 65, 67, 68, 69, 70, 71, 72, 75, 80, 81, 95, 98, 101, 102, 103, 104, 105, 106, 107, 108, 112], "parameter": [45, 66, 84], "parametr": 67, "parenthes": 78, "parmar": [41, 93], "pars": [57, 61], "parsed_arg": 16, "parser": 57, "part": [46, 51, 61, 69, 101, 107], "partial": [43, 50], "particip": [22, 48, 49], "particular": [5, 57, 58, 59, 65, 67, 69, 70, 86, 102, 108], "particularli": [19, 72, 88], "partit": [67, 81, 101], "pass": [6, 15, 18, 22, 25, 27, 35, 39, 45, 48, 50, 51, 52, 57, 58, 59, 60, 64, 102, 108, 112], "pass_at_k": 18, "past": [69, 75], "pat_str": 58, "patch": 25, "path": [42, 58, 98], "path_to_custom_output": 19, "pattern": [8, 54, 58, 65, 66], "paul": [41, 93], "pavlov": [41, 93], "pbar": 73, "pdf": [20, 24, 27, 34, 36, 39], "pe_": 5, "peak": [14, 33], "pearson": 64, "pebbl": 19, "pei": [41, 93], "peiyi": [41, 93], "penal": [46, 65, 81], "penalti": [9, 56, 64, 65, 70, 71, 91], "peng": [41, 93], "per": [9, 17, 18, 43, 45, 48, 49, 53, 58, 70, 71, 78, 79, 82, 92, 104], "per_device_train_batch_s": 111, "percent": 76, "percentag": [48, 77], "perceptu": 43, "perfect": [57, 82], "perform": [2, 5, 7, 8, 9, 12, 13, 14, 15, 17, 20, 21, 33, 34, 35, 44, 46, 48, 49, 50, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 65, 66, 69, 71, 72, 77, 80, 82, 84, 86, 90, 92, 95, 98, 101, 102, 104, 105], "period": [51, 53, 107], "permit": [14, 33], "permut": [58, 105], "perplex": 102, "person": [9, 76], "perspect": [8, 65, 107], "peter": [41, 93], "petroski": [41, 93], "petrov": [41, 93], "pexpect": 19, "pgr": 46, "phase": [24, 35, 36, 50, 51, 54, 56, 60, 67, 76, 88], "phd": 21, "phenomenon": [46, 64, 65, 88], "phi": [9, 64, 67, 70, 71, 77, 83, 108], "phi4": 28, "philipp": [41, 93], "philosophi": 17, "php": 57, "phrase": 6, "physic": [17, 21], "pi": [5, 9, 43, 56, 67, 69, 71, 75, 80, 81, 84, 91, 98, 102, 103], "pi_": [9, 43, 56, 65, 68, 70, 71, 75, 80, 84, 91, 98], "piao": [41, 93], "pick": [44, 48], "piec": [2, 55, 57, 76, 77, 90], "piecewis": 56, "pii": 9, "pile": 103, "pinto": [41, 93], "pioneer": 54, "pip": [16, 19], "pipelin": [12, 14, 25, 33, 35, 52, 54, 60, 67, 78, 88, 92], "pivot": [1, 44], "pkginfo": 19, "place": [58, 59, 66, 78, 108], "placehold": [14, 33], "plai": [44, 52, 53, 54], "plain": [9, 50], "plan": [1, 54, 103], "plane": 107, "plappert": [41, 93], "platform": [7, 19, 31, 48, 49], "platformdir": 19, "playground": 9, "pleas": [16, 76], "plethora": 57, "plot": [45, 67, 69], "plot_loss": 111, "plt": 67, "plu": 19, "plugin": 19, "pm": 84, "pmatrix": [51, 106, 107], "po": 5, "poetri": 19, "point": [5, 35, 44, 49, 50, 53, 56, 61, 65, 82, 88, 103], "pointwis": [66, 80], "polar": [58, 59, 102, 103], "polici": [9, 52, 53, 54, 56, 57, 60, 67, 69, 71, 75, 76, 77, 80, 84, 88, 91], "polit": 76, "polosukhin": [41, 93], "pond": [41, 93], "pool": [1, 24, 36, 48, 80, 83, 84, 90], "poor": [50, 54, 69, 88, 105], "poorli": 46, "pop": 80, "popular": [12, 25, 46], "portion": [51, 57, 61, 88], "posit": [6, 41, 46, 51, 54, 55, 58, 59, 60, 68, 70, 72, 75, 77, 78, 82, 83, 84, 90, 92, 93, 95, 105, 106, 108], "positionwis": 5, "possess": 82, "possibl": [7, 24, 25, 36, 46, 49, 50, 54, 55, 57, 65, 69, 76, 78, 79, 83, 91, 92], "possibli": [35, 61, 76], "post": 77, "post0": 19, "postpon": 50, "potenti": [1, 8, 9, 38, 50, 53, 56, 61, 65, 66, 72, 77, 88], "pow": [58, 59, 105], "power": [8, 20, 34, 41, 48, 49, 58, 59, 65, 88, 93], "ppo": [9, 28, 29, 52, 56, 65, 71, 78], "ppo_train": 73, "pq": 38, "pr": 25, "practic": [5, 8, 13, 35, 43, 46, 50, 61, 64, 67, 69, 81, 84, 101, 103], "practition": [20, 34], "prafulla": [41, 93], "pranav": [41, 93], "pre": [2, 5, 8, 13, 29, 31, 35, 41, 48, 50, 52, 58, 64, 66, 76, 90, 93, 100, 102, 103, 106, 107, 109], "preambl": 77, "preced": [6, 56], "precis": [13, 43, 51, 72], "precomput": 103, "precompute_freqs_ci": [58, 59, 103], "predecessor": [60, 61], "predefin": [20, 34, 75, 79, 88], "predicetd": 66, "predict": [1, 5, 6, 9, 15, 19, 46, 48, 49, 51, 58, 64, 66, 69, 71, 81, 84, 91, 92, 103], "predominantli": 51, "prefer": [9, 12, 28, 29, 41, 43, 44, 52, 53, 54, 60, 61, 64, 68, 69, 71, 76, 78, 79, 80, 82, 83, 88, 91, 92, 93, 103, 105], "prefix": [9, 19, 24, 36, 43, 51, 92], "preliminari": [53, 66, 88], "prepend": [58, 76], "prescrib": 69, "presenc": [42, 61], "present": [9, 50, 54, 57, 69, 71, 72, 75, 76, 77, 83, 92, 102, 103], "preserv": [61, 102, 103], "pressur": 102, "pretrain": [8, 9, 17, 24, 36, 46, 51, 53, 61, 66, 71, 73, 76, 82, 92, 95, 102, 103, 112], "pretrained_weight": 111, "prev_po": 58, "prevent": [5, 51, 54, 75], "preview": 90, "previou": [5, 8, 24, 27, 36, 39, 53, 57, 58, 59, 60, 69, 72, 91, 102, 103], "previous": 5, "primarili": [9, 21, 52, 57, 60, 71, 72], "princip": 101, "principl": [76, 78], "print": [13, 42, 58], "prior": [7, 8, 21, 35, 56, 67, 72, 76, 84], "priorit": [57, 60], "privaci": 76, "privat": 1, "prm800k": 92, "pro": [49, 52, 69], "prob": 58, "probabl": [5, 6, 9, 18, 43, 44, 58, 65, 67, 68, 69, 70, 72, 75, 76, 77, 80, 84, 92], "problem": [8, 15, 16, 17, 18, 19, 20, 21, 22, 25, 26, 31, 32, 34, 35, 37, 41, 46, 48, 49, 50, 51, 54, 57, 61, 65, 67, 80, 81, 88, 90, 92, 93], "probs_idx": 58, "probs_sort": 58, "probs_sum": 58, "proce": [27, 39], "procedur": [6, 9, 25, 46, 49, 68, 69, 71, 77, 82], "process": [7, 13, 24, 27, 35, 36, 39, 41, 43, 44, 46, 50, 52, 53, 54, 58, 60, 61, 64, 65, 66, 69, 72, 75, 90, 93, 103, 105, 107, 109], "prod": 18, "produc": [2, 5, 6, 9, 14, 20, 24, 25, 27, 33, 34, 36, 39, 43, 44, 48, 49, 50, 53, 54, 57, 58, 61, 64, 69, 71, 72, 77, 78, 81, 84, 91, 95, 104], "product": [18, 43, 58, 59, 106, 107, 108], "profession": [17, 51], "profici": 53, "program": [18, 22, 31, 32, 35, 48, 49, 50, 51, 52, 54, 57, 61], "programm": [15, 22, 32], "programmat": 18, "progress": [5, 32, 50, 54, 56, 65, 69, 70], "project": [5, 48, 104], "promin": [27, 39], "promis": [7, 8, 44, 61, 77, 90], "promot": [24, 36], "prompt": [2, 9, 12, 14, 16, 17, 18, 20, 24, 25, 33, 34, 35, 36, 48, 49, 50, 51, 52, 53, 54, 56, 57, 58, 60, 64, 65, 69, 71, 73, 75, 76, 78, 79, 80, 81, 82, 83, 84, 88, 95], "prompt_data": 73, "prompt_max_len": 73, "prompt_token": 58, "prompts_dataload": 73, "prone": [26, 81], "prop": 55, "proper": [13, 61, 64, 66], "properli": [41, 56], "properti": [103, 106], "proport": [51, 53, 61, 68, 101], "propos": [2, 5, 24, 36, 43, 46, 51, 58, 59, 61, 64, 65, 66, 67, 70, 72, 80, 83, 84, 90, 91, 98, 101, 104, 108, 112], "proprietari": [51, 60], "propto": 68, "proto": 19, "protobuf": 19, "protocol": [35, 57], "prove": [35, 107], "proven": [52, 70], "provid": [1, 2, 6, 9, 14, 16, 17, 18, 19, 22, 25, 26, 33, 37, 46, 51, 52, 54, 56, 58, 61, 67, 68, 70, 72, 81, 82, 86, 88, 90, 92, 103, 107], "proxim": 70, "prune": 22, "pseudo": 84, "pseudocod": 38, "pseudolabel": 84, "psi": [67, 70, 80, 81], "psi_": 81, "psm": 51, "psychologi": 17, "pth": 58, "ptimiz": 65, "ptx": [9, 71], "ptyprocess": 19, "public": [9, 48, 49, 50, 52, 61, 71], "publicli": [51, 55, 60], "pull": 25, "punish": 65, "punit": 65, "pure": 88, "puri": [41, 93], "purpos": [2, 25, 41, 46, 51, 95, 103], "pursu": 21, "push": [5, 20, 34, 72], "put": [14, 24, 33, 36, 43, 48, 88], "puzzl": [46, 54], "py": [16, 111], "py310h06a4308_0": 19, "pyarrow": 19, "pyasn1": 19, "pycpars": 19, "pydant": 19, "pydoc": 35, "pyext": 19, "pypars": 19, "pyplot": 67, "pyproject": 19, "python": [2, 15, 16, 19, 20, 22, 25, 34, 35, 48, 49, 51, 57, 61, 98], "pytorch": 90, "pytz": 19, "pyyaml": 19, "q": [5, 9, 38, 41, 43, 58, 59, 60, 61, 65, 70, 72, 93, 98, 102, 103, 104, 106, 107], "q_": [58, 59, 102, 103], "q_0": [58, 59], "q_1": [58, 59], "q_2": [58, 59], "q_3": [58, 59], "q_a_layernorm": 53, "q_a_proj": 53, "q_b_proj": 53, "q_head_dim": 53, "q_i": [38, 98], "q_lora_rank": 53, "qa": [35, 38, 55, 88], "qihao": [41, 93], "qime": [41, 93], "qin": [41, 93], "qinyu": [41, 93], "qiu": [41, 93], "qiufeng": [41, 93], "qiushi": [41, 93], "qk": 5, "qk_nope_head_dim": 53, "qk_rope_head_dim": 53, "qkv": 60, "qlora": 73, "qpa": 38, "qr": 104, "qu": [41, 93], "quad": [6, 45, 54, 56, 65, 66, 81, 84, 101, 102, 103], "qualiti": [5, 7, 13, 20, 21, 24, 25, 26, 28, 34, 35, 36, 37, 38, 43, 49, 53, 54, 56, 60, 61, 64, 65, 77, 78, 79, 82, 83, 84, 90, 102, 103], "quan": [41, 93], "quantifi": [81, 90], "quantil": 84, "quantiti": [28, 57], "quartil": 57, "queri": [1, 5, 9, 12, 35, 41, 51, 53, 58, 59, 60, 61, 66, 69, 70, 81, 84, 93, 102, 103, 106, 107], "query_1": 57, "query_2": 57, "question": [1, 6, 7, 14, 16, 17, 21, 22, 25, 27, 33, 35, 38, 39, 44, 50, 51, 54, 61, 65, 70, 77, 90, 92, 95, 98, 102, 103], "question_id": 19, "quickli": 65, "quit": [50, 52], "qw_": 5, "qwen": [28, 41, 65, 93], "qwen2": [28, 29, 41, 60, 65, 90, 93], "qy": [41, 60, 61, 93], "r": [5, 9, 15, 27, 39, 41, 43, 51, 53, 54, 56, 57, 58, 64, 65, 68, 69, 70, 75, 80, 81, 91, 93, 98, 101, 102, 104, 106, 107, 112], "r1": [29, 54, 65], "r_": [9, 43, 53, 56, 64, 65, 66, 67, 69, 70, 71, 75, 77, 80, 81, 82, 98, 106, 107], "r_1": [66, 70], "r_h": 56, "r_i": [65, 66, 70, 98], "r_l": 66, "racist": 76, "radford": [41, 93], "rafael": [41, 93], "rafailov": [41, 93], "rai": [41, 93], "rais": [17, 58, 103], "ramesh": [41, 93], "ramp": 102, "ran": 49, "rand_prompt": 73, "randn": [58, 105], "random": [7, 17, 20, 34, 48, 49, 58, 69, 80, 81, 83, 90], "randomli": [15, 17, 20, 27, 34, 39, 48, 51, 61, 76, 78], "rang": [6, 8, 9, 17, 19, 20, 34, 46, 51, 55, 56, 57, 58, 59, 65, 73, 75, 76, 77, 100, 103], "rangl": [58, 59, 102, 103, 107], "rank": [9, 44, 48, 50, 56, 57, 67, 71, 80, 81, 82, 112], "rapidfuzz": 19, "rapidli": 72, "rate": [2, 35, 46, 48, 49, 50, 53, 55, 56, 57, 64, 65, 69, 71, 75, 77, 92, 95], "rater": 69, "rather": [67, 78, 88], "ratio": [54, 61, 68, 75, 102], "rational": 77, "raul": [41, 93], "raw": [52, 109], "re": [2, 50, 58, 59, 67, 80, 102, 103, 107], "reach": [2, 8, 21, 24, 36, 49, 50, 56, 92, 101], "read": [7, 58], "readabl": [57, 88], "readlin": 19, "real": [2, 25, 35, 51, 73, 95, 103, 107], "realist": [13, 14, 21, 33], "realiti": [80, 103], "realiz": 6, "realli": [41, 93], "realm": [20, 27, 34, 39], "realworld": 35, "rearrang": [67, 80], "reason": [8, 17, 18, 27, 29, 39, 41, 43, 44, 50, 51, 53, 54, 57, 60, 65, 68, 70, 77, 83, 92, 93, 103, 105], "recal": [43, 57, 69, 72], "receiv": [2, 7, 43, 56, 69, 101], "recent": [1, 8, 48, 49, 55, 56, 57, 58, 64, 71], "recip": [51, 57, 83], "recommend": [12, 82], "recov": [46, 69], "rectifi": [58, 59, 108], "recurr": 5, "red": [27, 39, 104], "reddit": 7, "reduc": [7, 14, 17, 33, 35, 43, 44, 56, 68, 91, 101, 103, 104, 108, 112], "reduct": [58, 68], "redund": [49, 54, 101], "reevalu": 88, "ref": [67, 68, 70, 91], "refer": [9, 22, 25, 26, 41, 43, 51, 56, 57, 64, 65, 67, 68, 69, 70, 76, 82, 90, 92, 102], "refin": [13, 54, 57, 69], "reflect": [22, 35, 50, 72, 90], "regard": 65, "regardless": [64, 65], "regener": 35, "regex": 58, "regim": 92, "region": [5, 43, 75, 103], "reglu": [58, 59, 108], "regress": [5, 9, 51, 56, 57, 64, 71], "regul": 65, "regular": [41, 57, 60, 69, 70], "regularli": [54, 61], "rehears": 51, "reiichiro": [41, 93], "reinforc": [9, 46, 51, 67, 70, 78, 92, 95], "reject": [28, 29, 51, 54, 56, 57, 61, 64, 79, 81, 83, 84, 98], "rejected_1": 57, "rejected_2": 57, "rel": [5, 9, 44, 48, 52, 53, 54, 58, 59, 60, 65, 68, 76, 88, 95, 103, 106, 107], "relat": [14, 19, 33, 35, 44, 51, 52, 54, 60, 61, 70, 80, 98, 101, 104, 107], "relationship": [41, 93], "releas": [14, 16, 33, 35, 51], "relev": [25, 46, 50, 60, 61, 65, 83, 84, 92], "reli": [20, 34, 43, 46, 49, 50, 54, 84, 90, 105], "reliabl": [15, 17, 21, 46, 53, 54, 60, 88, 92], "relianc": 105, "relu": [5, 55, 58, 59, 67, 108], "remain": [1, 24, 25, 36, 45, 46, 48, 49, 68, 81, 84, 92], "remark": [88, 95], "remind": [14, 33], "remov": [9, 27, 35, 39, 49, 51, 55, 57, 61, 71, 76, 80, 83, 98], "ren": [41, 93], "render": 41, "renorm": 58, "reorder": 51, "repair": 19, "reparameter": 67, "repeat": [14, 24, 33, 36, 50, 65, 90], "repeatedli": 48, "repetit": [7, 65], "replac": [55, 58, 59, 60, 76, 77, 103, 108], "replai": [69, 70, 73], "replay_buff": 73, "repo": [25, 57, 61], "report": [18, 41, 61, 65, 81, 93, 98], "repositori": [19, 25, 35, 48, 51, 52, 61], "repres": [5, 24, 27, 36, 39, 43, 51, 57, 58, 59, 68, 75, 81, 101, 102, 103, 107, 108, 110], "represent": [5, 38, 46, 48, 54, 58, 59, 67, 77, 107, 108], "reproduc": [1, 35], "request": [1, 14, 19, 25, 33, 76, 82], "requir": [1, 2, 5, 6, 8, 14, 15, 25, 33, 46, 49, 50, 51, 54, 56, 57, 58, 69, 80, 88, 101, 102, 103, 104, 105, 106, 107], "rerank": 49, "resampl": [60, 69], "rescal": 103, "research": [1, 21, 35, 56, 72], "reserv": 53, "reserved_special_token_": 58, "reserved_special_token_0": 58, "reserved_special_token_1": 58, "reserved_special_token_2": 58, "reserved_special_token_3": 58, "reserved_special_token_4": 58, "reshap": [58, 59, 103], "reshape_for_broadcast": [58, 59, 103], "residu": [5, 45], "resolv": [25, 48, 64], "resort": 88, "resourc": [50, 56, 61, 90], "respect": [5, 9, 44, 65, 68, 70, 71, 72, 75, 77, 78, 81, 101, 104], "respons": [9, 12, 14, 16, 19, 27, 33, 35, 39, 44, 53, 54, 56, 57, 58, 60, 61, 64, 65, 66, 69, 70, 71, 72, 75, 76, 77, 79, 80, 81, 82, 84, 88, 91, 95, 101], "response_candid": 80, "response_reward": 80, "rest": 42, "restrict": [2, 43, 54, 65, 68, 75], "result": [2, 5, 7, 8, 17, 18, 20, 26, 34, 35, 43, 46, 49, 50, 51, 56, 61, 70, 72, 80, 81, 88, 92, 98, 101, 102, 103, 104, 106, 107], "retain": [35, 51, 54, 57, 61, 76, 84], "retriev": [1, 35], "return": [18, 43, 58, 59, 72, 75, 80, 103, 105], "reus": [60, 88, 103], "reveal": [54, 57], "revers": 77, "review": [60, 67], "revis": [57, 91], "reward": [9, 28, 29, 41, 44, 46, 50, 51, 52, 53, 54, 60, 67, 70, 72, 73, 77, 79, 80, 83, 84, 93], "reward_pretrain": 73, "rewon": [41, 93], "rewrit": 76, "rft": [28, 66, 98], "rho": 98, "rho_": 80, "rich": 35, "right": [5, 9, 18, 43, 45, 51, 58, 59, 64, 65, 67, 68, 70, 71, 72, 75, 77, 80, 91, 92, 102, 103, 104, 107, 109], "rigor": [13, 16, 17, 35, 41, 93], "risk": [25, 54, 61], "rl": [9, 28, 29, 48, 52, 53, 54, 56, 60, 65, 67, 75, 76, 77, 78, 84, 88, 92], "rlaif": [29, 76, 78, 84], "rlcd": [28, 29, 84], "rlhf": [46, 51, 64, 65, 67, 69, 71, 73, 76, 80], "rlhf1": 29, "rlhf2": 29, "rm": [9, 28, 46, 53, 54, 56, 57, 64, 73, 77, 79], "rm_": 53, "rmboost": 28, "rmsnorm": [41, 53, 55, 60, 93], "roberta": 57, "robust": [24, 25, 35, 36, 50, 52, 60, 76], "roform": [41, 93], "role": [2, 44, 52, 53, 54, 58], "rollout": [65, 91], "rollout_batch_s": 73, "room": 50, "rope": [28, 29, 51, 54, 55, 60, 104], "rope_theta": [58, 59], "rotari": [41, 51, 55, 60, 93, 106], "rotat": [51, 106, 107], "roug": [24, 36], "roughli": [45, 49, 55, 81, 92], "round": [27, 39, 49], "rout": [53, 54, 101], "row": [58, 59, 103], "rozi\u00e8r": [41, 93], "rsa": 19, "rsm": [41, 57, 60, 93], "rso": [28, 29], "rsqrt": [58, 59, 105], "rtol": [58, 105], "rtx4090": 73, "ruan": [41, 93], "rui": [41, 93], "ruiqi": [41, 93], "ruizh": [41, 93], "rule": [21, 50, 52, 53, 54, 57, 88], "rulebas": 54, "run": [15, 42, 50, 51, 57, 68, 76, 90, 91], "runji": [41, 93], "runnabl": 1, "runner": 19, "runtim": [19, 49], "runxin": [41, 93], "ruyi": [41, 93], "rwc": [7, 8, 41, 93], "rx": 69, "ryan": [41, 93], "ryder": [41, 93], "s3transfer": 19, "s_": [43, 54, 66, 68, 75, 101, 103], "s_1": [61, 66, 103], "s_2": 103, "s_i": 66, "s_j": 66, "s_n": 61, "s_t": 75, "safe": 71, "safeti": [46, 51, 53, 56], "sahil280114": [14, 33], "sai": 76, "salienc": 46, "salient": 46, "sam": [41, 93], "same": [5, 8, 27, 35, 39, 41, 48, 49, 50, 51, 52, 53, 61, 65, 67, 68, 76, 78, 80, 81, 82, 88, 98, 101, 102, 103, 104, 105], "sampl": [7, 15, 16, 17, 18, 20, 24, 25, 27, 28, 29, 34, 35, 36, 39, 43, 44, 51, 52, 54, 55, 56, 57, 58, 60, 61, 66, 67, 69, 70, 75, 76, 77, 78, 79, 82, 83, 84, 90, 92, 95, 98, 105], "sample_top_p": 58, "sandbox": [60, 61], "sandhini": [41, 93], "sanghai": [41, 93], "sanit": 16, "sastri": [41, 93], "satisfactori": [53, 72], "satisfi": 72, "saunder": [41, 93], "save": [50, 104], "save_path": 73, "save_step": [73, 111], "scail": [75, 90], "scalabl": [6, 21, 61, 66, 72, 75, 77], "scalar": [9, 56, 64, 66, 71, 72], "scale": [6, 8, 24, 25, 26, 35, 36, 37, 41, 46, 56, 58, 59, 60, 61, 65, 67, 72, 75, 88, 93, 95, 103, 104], "scan": 45, "scarciti": 61, "scenario": [1, 2, 19, 35, 61, 65], "schedul": [35, 55], "scheme": [56, 69, 110], "school": 21, "schulman": [41, 93], "scienc": [17, 35, 55], "scientif": 35, "scope": [25, 54], "score": [13, 17, 54, 56, 57, 58, 59, 60, 61, 64, 66, 69, 70, 71, 75, 76, 77, 78, 80, 81, 82, 84, 92, 101, 102, 103], "scorer": 61, "scott": [41, 93], "scrape": [7, 25, 31, 48, 65], "scratch": [28, 81, 102, 103], "script": [16, 111], "scy": [38, 41, 61, 93], "search": [1, 49, 50, 92], "seattl": 100, "second": [1, 5, 7, 14, 15, 33, 35, 43, 49, 53, 54, 61, 69, 77, 91, 108], "secret": [28, 29], "secretli": [41, 93], "secretstorag": 19, "section": [35, 52, 65, 76, 88, 103, 107], "secur": 61, "see": [12, 15, 17, 41, 42, 43, 45, 57, 59, 68, 74, 77, 103], "seed": [13, 14, 20, 24, 33, 34, 35, 36, 61, 82, 83], "seek": [56, 90], "seen": [57, 84], "segment": [21, 56, 109], "select": [12, 15, 25, 27, 35, 39, 44, 48, 49, 50, 51, 54, 56, 57, 58, 64, 65, 69, 71, 92, 95, 98, 101], "selector": 92, "self": [5, 6, 7, 9, 14, 16, 19, 20, 22, 25, 28, 29, 33, 34, 41, 51, 53, 57, 58, 59, 61, 70, 73, 93, 101, 102, 103, 105, 106, 107], "selfattent": 107, "selfinstruct": [24, 36], "semant": [22, 48, 49, 50, 57, 61, 83], "semi": [24, 36, 66], "sen": [41, 93], "send": [45, 69, 101], "sensit": [9, 17, 104, 105], "sent": [35, 54, 101], "sentenc": [5, 6, 7, 14, 33, 35, 55, 77, 109], "separ": [5, 12, 49, 51, 56, 92], "seq_len": [58, 105], "seqlen": [58, 59], "sequenc": [5, 6, 21, 41, 43, 51, 54, 56, 58, 59, 60, 65, 69, 72, 80, 82, 93, 101, 102, 104, 105, 106, 107, 108, 109], "sequenti": [43, 69, 90], "seri": [20, 34, 55, 60, 61, 82, 86, 92], "serv": [13, 35, 41, 44, 54, 66, 69, 81, 101], "servic": 60, "set": [1, 5, 8, 9, 12, 13, 14, 15, 17, 18, 21, 24, 25, 26, 27, 32, 33, 35, 36, 37, 39, 43, 44, 46, 48, 51, 52, 53, 54, 56, 57, 58, 60, 61, 65, 66, 69, 70, 71, 76, 80, 81, 82, 83, 90, 92, 95, 98, 101, 102, 103, 107], "setup": [2, 16, 46], "setuptool": 19, "seventh": [41, 93], "sever": [18, 44, 48, 49, 51, 52, 56, 57, 60, 61, 69, 88, 105], "sexist": 76, "sft": [9, 28, 29, 35, 53, 54, 60, 61, 67, 68, 70, 73, 75, 77, 79, 80, 81, 82, 88, 91, 98], "sh": 73, "sha": [41, 93], "shallow": 64, "shang": [41, 93], "shanghao": [41, 93], "shanghaoran": [41, 93], "shangyan": [41, 93], "shanhuang": [41, 93], "shantanu": [41, 93], "shao": [41, 93], "shaoq": [41, 93], "shape": [45, 58, 59, 103, 105], "share": [5, 9, 32, 53, 54, 88, 104], "sharegpt": 35, "sharma": [41, 93], "shazeer": [41, 93], "shelf": [44, 72, 77], "shellingham": 19, "shen": [41, 93], "shengfeng": [41, 93], "shengguang": [41, 93], "shift": [35, 66, 92], "shiji": [41, 93], "shirong": [41, 93], "shiyu": [41, 93], "short": [15, 22, 25, 54, 103], "shorter": [44, 65], "shot": [8, 9, 13, 17, 24, 36, 41, 76, 77, 82, 84, 92, 93, 98], "should": [1, 8, 14, 15, 21, 33, 42, 45, 46, 49, 68, 72, 76, 78, 80, 104], "show": [5, 8, 9, 13, 17, 41, 42, 43, 51, 55, 57, 67, 68, 72, 76, 80, 81, 82, 86, 88, 92, 103, 104, 107], "shown": [5, 9, 18, 20, 34, 35, 43, 46, 70, 71, 77, 81, 84], "shuai": [41, 93], "shuang": [41, 93], "shuffl": [27, 39], "shuip": [41, 93], "shukai": [41, 93], "shunfeng": [41, 93], "shusheng": [41, 93], "shyam": [41, 93], "sida": [41, 93], "siddhartha": [41, 93], "sidestep": 69, "sigler": [41, 93], "sigma": [9, 56, 58, 59, 67, 68, 71, 77, 80, 81, 108], "sigma_": 75, "sigmoid": [58, 59, 67, 80, 108], "signal": [24, 36, 43, 50, 52, 54, 57, 60, 65, 77, 78, 88], "signatur": [18, 22, 35, 49, 51], "signific": [7, 17, 35, 53, 56, 57, 60, 102, 103, 104, 105], "significantli": [5, 13, 14, 17, 20, 33, 34, 38, 44, 50, 51, 53, 56, 57, 60, 65, 66, 68, 76, 81, 86, 92, 104], "silu": [58, 59], "sim": [9, 43, 45, 56, 65, 66, 67, 68, 70, 71, 80, 81, 84, 91, 108], "simen": [41, 93], "similar": [8, 12, 17, 24, 36, 41, 48, 49, 57, 61, 76, 78, 92, 98, 102], "similarili": [102, 104], "similarli": [5, 49, 54, 68, 82, 84, 88], "simpl": [5, 8, 14, 18, 20, 27, 33, 34, 39, 41, 45, 46, 48, 54, 58, 59, 66, 67, 77, 84, 86, 107, 109], "simpler": [6, 43, 67], "simplest": 90, "simpli": [9, 14, 33, 46, 64, 90, 91, 102], "simplic": [6, 50, 75], "simplifi": [14, 18, 27, 33, 39, 58, 98], "simplist": 2, "simul": [48, 69, 78], "simultan": [5, 45, 66, 82], "sin": [5, 51, 58, 59, 102, 103, 106, 107], "sinan": [41, 93], "sinc": [5, 6, 43, 46, 49, 52, 58, 59, 61, 67, 68, 70, 76, 77, 80, 101, 103, 104, 105], "sine": 5, "singl": [1, 7, 9, 14, 18, 22, 33, 48, 49, 50, 51, 54, 71, 78, 83, 92, 101], "sinusoid": [5, 107], "site": 32, "situat": 101, "six": [19, 27, 39, 57], "size": [6, 7, 8, 26, 35, 37, 41, 51, 53, 55, 58, 59, 60, 65, 70, 72, 81, 93, 95, 101, 102, 103, 104, 105], "skill": [21, 26, 37, 51, 61, 82], "skywork": 28, "sl": 76, "slama": [41, 93], "slice": 104, "slight": 41, "slightli": [56, 57], "slope": 90, "slow": 105, "slowli": [102, 103], "slp": [41, 54, 60, 93, 102, 104, 106], "small": [5, 15, 24, 36, 41, 46, 48, 50, 51, 57, 61, 75, 82, 88, 90, 102, 103, 104, 105, 107], "smaller": [44, 55, 56, 91, 92, 95, 101, 102, 104, 106], "smallest": [7, 48, 58], "smallscal": 92, "smarter": 46, "smooth": [8, 27, 39], "snapshot": [48, 76], "sniffio": 19, "snippet": [13, 20, 34, 35, 51, 57, 60, 61], "so": [1, 9, 15, 42, 48, 50, 56, 57, 66, 68, 69, 70, 76, 82, 101, 102, 103, 104], "social": [7, 17], "soft": [50, 65, 77, 81], "softmax": [6, 58, 59, 77, 101, 102, 104], "softwar": [25, 35], "solar": [41, 93], "sole": [5, 43, 56, 65, 72], "solid": 104, "solut": [2, 13, 16, 20, 21, 22, 25, 26, 31, 32, 34, 37, 38, 48, 49, 50, 51, 57, 64, 67, 77, 83, 90, 92, 104, 105, 107], "solv": [2, 13, 16, 17, 18, 21, 22, 25, 41, 44, 48, 49, 50, 51, 57, 88, 91, 92, 93], "solvabl": 22, "some": [5, 6, 14, 33, 41, 48, 50, 52, 56, 57, 64, 69, 72, 80, 84, 101], "someon": 76, "someth": 8, "sometim": [8, 35], "song": [41, 93], "sonnet": 90, "sort": [57, 58, 69], "sound": 65, "sourc": [7, 12, 18, 22, 25, 27, 35, 39, 41, 51, 52, 54, 57, 58, 60, 61, 65, 80, 88, 93], "space": [2, 7, 27, 39, 49, 50, 106, 107], "span": [1, 17, 21, 41, 46, 51, 61, 90], "spars": [8, 25, 41, 50, 93], "speak": 80, "spearman": 64, "special": [1, 17, 41, 56, 58, 60, 84, 92, 101], "special_token": 58, "specif": [1, 8, 9, 14, 27, 33, 35, 39, 41, 43, 44, 50, 51, 54, 57, 58, 60, 61, 66, 67, 68, 76, 80, 81, 82, 90, 92, 101, 102, 103], "specifi": [22, 58, 88, 90], "speed": [9, 54, 71], "spent": 52, "sphinx": 41, "split": [9, 13, 31, 32, 49, 51, 58, 59], "split_experience_batch": 73, "spm": 51, "spot": 17, "spread": 102, "spuriou": 8, "sqlite": 19, "sqrt": [5, 58, 59, 81, 102, 103, 104, 105, 107], "squre": 58, "src": 111, "sse": 19, "stabil": [17, 55, 56, 58, 65, 70, 75, 105], "stabl": [16, 18, 44, 60, 75], "stack": [20, 34, 95, 101], "stackexchang": 55, "stage": [6, 25, 51, 53, 54, 56, 57, 60, 61, 70, 81, 88, 90, 95, 102, 103, 111], "stai": 43, "stale": 77, "stand": 41, "standalon": 51, "standard": [6, 21, 22, 24, 32, 36, 44, 45, 48, 51, 52, 57, 58, 60, 61, 68, 69, 70, 72, 75, 77, 78, 81, 92, 95, 101, 110], "star": [35, 91], "starcod": [20, 27, 34, 39, 51], "starcoderdata": [20, 34], "starkli": 17, "start": [9, 24, 25, 27, 36, 39, 41, 42, 44, 49, 50, 51, 58, 67, 69, 71, 80, 82], "start_header_id": 58, "start_po": [58, 59], "starter": 41, "state": [8, 9, 21, 43, 46, 55, 64, 65, 75, 76, 101, 102], "statement": [22, 48, 51], "static": [43, 57, 60], "staticmethod": 58, "statist": 105, "statu": [19, 25, 73], "std": [58, 65, 70, 81, 105], "steadi": 45, "steer": [46, 57, 69, 84], "stefano": [41, 93], "steinhardt": [41, 93], "stem": [17, 60], "step": [5, 9, 21, 24, 27, 28, 29, 35, 36, 39, 41, 43, 45, 49, 50, 51, 53, 54, 55, 56, 57, 65, 70, 71, 73, 76, 77, 83, 86, 88, 93, 95, 102, 103], "stepbi": 92, "steven": [41, 93], "still": [8, 46, 48, 52, 54, 67, 77, 103], "stochast": 6, "stoica": [41, 93], "stop": [45, 56], "stop_token": 58, "store": [41, 61, 80], "str": [58, 80], "straightforward": [6, 12, 21, 43, 57, 72, 80, 88, 98, 102, 103], "straightforwardli": 64, "strateg": [54, 57], "strategi": [35, 44, 51, 53, 54, 56, 60, 64, 65, 66, 73, 84, 91, 102, 104], "stream": 45, "streamlin": [27, 39], "strength": [9, 54, 57, 69, 71, 72], "strict": [24, 31, 36, 58], "strictli": 82, "string": [58, 77, 90, 91], "stringent": 57, "strip": [16, 58], "strong": [41, 53, 54, 64, 72, 81, 88, 90, 92, 93, 95, 102, 103], "stronger": [20, 34, 46, 54, 104], "strongest": 21, "strongli": [76, 95], "structur": [5, 6, 41, 50, 77, 101], "struggl": [8, 44, 81, 88], "stuck": 43, "student": [20, 21, 34, 46, 61], "studi": [8, 24, 36, 45, 46, 60, 69, 72], "style": [17, 51, 57], "su": [41, 93], "sub": [5, 7, 50, 52, 55, 58, 80, 105, 106, 107], "subbiah": [41, 93], "subject": [17, 51, 90], "sublay": 5, "submiss": [48, 49], "submit": [2, 9, 49, 69, 71], "suboptim": [35, 105], "subsequ": [5, 51, 56, 57, 58, 59, 69, 70, 82, 108], "subset": [7, 9, 21, 22, 25, 45], "subspac": 5, "substanti": [8, 14, 27, 33, 39, 46, 70, 72, 91], "substitut": [53, 67, 72, 101], "subtract": 70, "subword": [41, 93, 109], "succ": [9, 67, 80, 81, 84], "succeed": 50, "success": [2, 17, 18, 46, 50, 56, 61, 82, 83], "successfulli": [35, 65], "suchir": [41, 93], "sufeng": [41, 93], "suffer": [54, 65, 75, 88, 91], "suffici": [8, 9, 48, 56, 61, 68, 70, 86, 103], "suffix": 51, "suggest": [8, 44, 61, 67, 68, 72, 77, 95, 108], "suit": 6, "suitabl": [51, 64, 81], "sujoi": [41, 93], "sum": [5, 43, 54, 58, 64, 66, 69, 70, 75, 101, 107], "sum_": [6, 43, 58, 59, 65, 66, 67, 70, 72, 75, 77, 80, 81, 90, 91, 101, 102, 103, 104, 107, 109], "sumit": [41, 93], "summar": 103, "summari": [7, 77], "sun": [41, 93], "sup": 28, "super": [53, 58, 59, 105], "superalign": 46, "superhuman": 46, "superior": [5, 44, 52, 65, 82], "supervis": [7, 8, 9, 21, 24, 25, 36, 43, 46, 51, 61, 67, 78, 79, 80, 82, 84, 90, 95, 106, 107], "supervison": 46, "supervisor": 46, "supplement": 57, "suppli": 98, "support": [2, 42, 51, 53, 57, 61, 73, 110], "suppos": [64, 72, 101], "suppress": [65, 90], "sure": [14, 33, 51, 76], "surfac": [49, 78, 92], "surpass": [8, 21, 27, 39, 61, 79], "surpris": [9, 57, 70], "surprisingli": 68, "surrog": 70, "surround": [51, 53], "suspect": [5, 76], "sutskev": [41, 93], "swaroop": [41, 93], "swiglu": [55, 60], "swish": [28, 58, 59], "symbol": 5, "sympi": 19, "syncheck": 16, "synnaev": [41, 93], "syntact": [48, 57], "syntax": [41, 57, 61], "synthes": [13, 35, 60, 61], "synthesi": [35, 51, 61], "synthet": [20, 34, 57, 60, 61, 79, 83, 84], "system": [8, 21, 35, 41, 54, 57, 58, 65, 69, 88, 90, 93, 102], "systemat": 72, "t": [5, 6, 24, 27, 35, 36, 39, 41, 43, 48, 54, 58, 59, 65, 70, 72, 75, 82, 93, 100, 101, 102, 103, 104, 107], "t1": [58, 105], "t2": [58, 105], "t_": [68, 109], "t_1": 109, "t_2": 109, "t_n": 109, "tabl": [7, 78, 104], "tackl": 49, "taco": 29, "tag": [48, 49, 51, 61, 88], "tail": 61, "tailor": [49, 54, 69], "take": [9, 20, 21, 34, 43, 51, 58, 59, 64, 67, 68, 69, 71, 72, 75, 80, 82, 83, 84, 92, 106, 107, 108], "taken": 43, "talent": 21, "tan": [41, 93], "tang": [41, 93], "tao": [41, 93], "target": [6, 49, 57, 58, 72, 76, 80, 92, 98, 101, 103], "task": [2, 5, 7, 8, 9, 13, 14, 15, 17, 18, 20, 22, 25, 26, 33, 34, 35, 37, 43, 44, 46, 48, 50, 51, 53, 54, 57, 61, 65, 70, 72, 75, 76, 77, 81, 88, 92, 102, 103, 105, 112], "task_id": 16, "taskspecif": 7, "tau": [43, 69, 91], "taught": 29, "td": [70, 75], "teach": [46, 56, 95], "teacher": [20, 34, 35], "team": [9, 71], "technic": [41, 46, 93], "techniqu": [14, 27, 28, 29, 33, 39, 43, 44, 46, 50, 56, 57, 65, 75, 81, 102, 103], "teddi": [41, 93], "telecommun": 110, "tell": [46, 67], "temper": 48, "temperatur": [19, 35, 48, 49, 54, 58, 60, 65, 69, 98, 102], "templat": [20, 24, 27, 34, 35, 36, 39, 111], "tempor": 31, "ten": [8, 17, 51, 56, 61, 103], "tend": [65, 69, 77], "tensor": [58, 59, 103], "term": [6, 9, 18, 50, 52, 54, 56, 60, 65, 66, 67, 70, 72, 92, 103, 107, 108], "termin": [2, 43, 90], "terri": [9, 64, 67, 80, 81], "test": [2, 7, 8, 9, 16, 17, 18, 19, 21, 22, 25, 26, 31, 32, 35, 37, 45, 46, 48, 49, 50, 51, 52, 54, 57, 60, 61, 82, 88, 91, 92, 98], "tester": 57, "text": [2, 5, 6, 7, 8, 9, 14, 18, 22, 25, 33, 41, 42, 43, 45, 51, 52, 54, 56, 58, 59, 61, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 77, 80, 81, 82, 84, 90, 91, 92, 95, 98, 101, 102, 103, 104, 108, 109, 110], "text_complet": 58, "textbf": 43, "textbook": 17, "textto": 1, "textual": 6, "tezak": [41, 93], "th": [27, 39, 66, 68, 70, 82, 101, 102, 103, 104], "than": [5, 7, 8, 12, 13, 14, 16, 20, 24, 25, 33, 34, 35, 36, 44, 46, 48, 49, 50, 51, 53, 55, 57, 64, 65, 67, 68, 76, 77, 78, 80, 81, 92, 95, 98, 101, 102, 103, 104], "thank": [2, 72], "thei": [1, 7, 9, 20, 34, 41, 46, 48, 51, 56, 57, 81, 82, 83, 90, 92, 101, 104, 105, 108, 110], "them": [1, 6, 9, 24, 36, 46, 49, 50, 52, 53, 57, 60, 65, 78, 80, 83, 88, 101, 104, 105, 107], "themselv": [9, 46, 66, 71], "theoret": [35, 68], "therebi": [54, 65, 101], "therefor": [20, 24, 34, 36, 49, 52, 53, 54, 56, 65, 68, 70, 72, 81, 92, 103, 104], "theta": [6, 9, 43, 51, 56, 58, 59, 64, 65, 66, 67, 68, 69, 70, 71, 75, 84, 91, 102, 103, 106, 107], "theta_": [51, 58, 59, 65, 68, 70, 75, 102, 103, 106, 107], "theta_0": [58, 59, 103], "theta_1": [58, 59, 103], "theta_d": 102, "theta_j": [58, 59], "thi": [1, 2, 5, 6, 7, 8, 9, 14, 17, 18, 20, 21, 24, 25, 26, 27, 33, 34, 35, 36, 38, 39, 40, 41, 42, 43, 44, 46, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 60, 61, 64, 65, 66, 67, 68, 69, 70, 71, 72, 74, 75, 76, 80, 81, 82, 83, 84, 88, 90, 91, 92, 98, 101, 102, 103, 104, 105, 107], "thing": [42, 76], "think": [76, 88, 90, 101, 103], "third": [1, 5, 15], "thirti": [41, 93], "thompson": 69, "thorough": [53, 57], "thoroughli": 54, "thorp": [41, 93], "those": [5, 9, 12, 25, 41, 44, 48, 52, 54, 56, 57, 60, 65, 69, 83, 92, 101], "though": 18, "thought": [17, 54, 57, 76, 77, 98], "thousand": [8, 48, 49, 51, 56, 57, 103], "three": [1, 2, 8, 9, 13, 18, 19, 22, 35, 43, 46, 51, 57, 61, 66, 67, 71, 72, 77, 80, 81, 90, 92, 104, 108], "threshold": [57, 58, 79, 84], "through": [1, 6, 17, 20, 27, 34, 39, 57, 58, 59, 60, 61, 64, 65, 66, 72, 75, 77, 80, 84, 88, 90, 95, 102, 103, 104, 106, 107, 108], "throughout": [57, 88], "thu": [27, 39, 43, 45, 46, 56, 58, 59, 64, 65, 69, 75, 78, 80, 82, 91, 102], "tian": [41, 93], "tianhang": [41, 93], "tianhao": [41, 93], "tianjian": [41, 93], "tianjun": [41, 93], "tianyi": [41, 93], "tianyu": [41, 93], "tier": 35, "tild": [56, 69, 70], "tillet": [41, 93], "time": [1, 5, 12, 19, 27, 39, 43, 48, 51, 56, 57, 64, 65, 77, 84, 88, 91, 92, 101, 104, 105, 112], "timeout": [19, 61], "tingyu": [41, 93], "tini": 8, "tip": 60, "titl": [67, 110], "titlecas": 13, "tk": 19, "tl": 7, "to_remov": 80, "todai": 46, "togeth": [5, 24, 36, 48, 49, 57, 76, 77], "tok": 58, "tok_embed": [58, 59], "token": [5, 6, 7, 9, 19, 25, 28, 29, 43, 48, 51, 52, 53, 55, 56, 57, 59, 61, 64, 68, 70, 71, 72, 77, 90, 91, 92, 95, 100, 102, 103, 104, 105, 106, 107, 109], "token1": 101, "token2": 101, "token3": 101, "token_logprob": 58, "tokenization\u4e4b\u540e": 100, "tokenization\u7b97\u6cd5\u4e4b\u4e00": 100, "tokenize\u7684\u610f\u601d\u662f\u628a\u4e00\u4e2a\u53e5\u5b50\u6216\u957f\u8bed\u6599\u5206\u6210token": 100, "token\u53ef\u4ee5\u7406\u89e3\u4e3a\u4e00\u4e2a\u7b26\u53f7": 100, "token\u6570": 100, "token\u66ff\u6362\u5b83\u4eec": 100, "toler": 92, "tolist": 58, "tom": [41, 93], "tomli": 19, "tomlkit": 19, "tone": 57, "tong": [41, 93], "tongliang": [41, 93], "tongzheng": [41, 93], "too": [46, 61, 65, 75, 103], "took": 90, "tool": [1, 12, 21, 41, 48, 57, 109], "toolbelt": 19, "top": [7, 20, 34, 35, 42, 44, 48, 54, 56, 57, 58, 59, 64, 66, 81, 84, 95, 101], "top_p": 58, "topic": [26, 37, 53, 57], "topk": [54, 101], "topp": [19, 65], "torch": [19, 58, 59, 101, 103, 105], "toreproduc": 12, "total": [1, 18, 20, 24, 25, 32, 34, 36, 45, 49, 52, 53, 54, 56, 70, 90, 101, 104], "total_len": 58, "toutanova": [41, 93], "toward": [24, 36, 41, 44, 78, 84, 91, 93, 101], "toxic": [9, 76], "tqdm": 19, "trace": [90, 91], "traceback": 58, "track": 80, "tractabl": 46, "trade": 65, "tradeoff": 72, "tradit": [8, 17], "train": [1, 2, 5, 9, 14, 20, 21, 24, 25, 26, 31, 32, 33, 34, 36, 37, 41, 43, 44, 46, 48, 49, 51, 52, 56, 58, 64, 66, 67, 68, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 90, 92, 93, 101, 102, 103, 104, 105, 109], "train_bash": 111, "train_batch_s": 73, "train_ppo": 73, "train_ppo_llama": 73, "trainabl": 112, "trainer": 73, "trainin": 52, "training_step_actor": 73, "training_step_crit": 73, "trajectori": [43, 66, 75, 88], "transduct": 5, "transfer": [106, 107], "transform": [5, 7, 8, 41, 48, 51, 53, 54, 55, 56, 57, 60, 65, 93, 102, 103, 104, 105, 106, 107, 108, 112], "transformerblock": [6, 58, 59], "transit": [43, 91], "translat": [5, 7, 41, 57, 88, 93], "transmit": 69, "transpos": [58, 59], "trap": 91, "treat": [42, 58, 60, 81, 103], "tremend": 53, "trend": 45, "tri": [82, 90], "trick": [50, 73, 102], "trigonometr": [58, 59, 102, 103], "trigonometri": 57, "trillion": [52, 54, 60, 61], "trim": [49, 95], "triplet": [6, 51, 72, 80], "triton": 19, "triu": [58, 59], "trivial": 65, "trl": 73, "troubl": 76, "trough": 51, "trove": 19, "true": [13, 56, 58, 59, 73, 105, 111], "truncat": 65, "trust": 75, "truth": [2, 13, 21, 22, 32, 46, 52, 53, 54, 57, 60, 64, 65, 66, 67, 80, 83], "try": [12, 14, 33, 50, 58, 61, 68, 82, 98], "trylimit": 50, "tu": [41, 93], "tune": [8, 9, 12, 13, 14, 24, 27, 33, 36, 39, 41, 57, 61, 67, 68, 70, 72, 73, 78, 79, 80, 82, 91, 92, 93, 95, 98, 102, 103], "tupl": [58, 59, 103], "turbo": [12, 20, 34, 52, 58], "turn": [51, 57, 58, 92, 103], "tutori": 61, "two": [1, 5, 6, 13, 15, 24, 25, 27, 36, 39, 41, 42, 43, 44, 46, 48, 49, 50, 51, 53, 54, 56, 57, 58, 59, 60, 64, 65, 66, 67, 68, 69, 72, 76, 77, 78, 80, 81, 82, 83, 84, 88, 90, 91, 92, 95, 101, 102, 103, 104, 108], "tworek": [41, 93], "tx": 69, "txt": [14, 33], "type": [12, 14, 19, 24, 27, 33, 35, 36, 39, 46, 48, 51, 54, 55, 58, 59, 66, 72, 76, 77, 80, 81, 88, 103], "type_a": [58, 59, 103, 105], "typeddict": 58, "typescript": 57, "typic": [2, 7, 8, 25, 44, 46, 55, 57, 65, 68, 69, 70, 72, 76, 78, 84, 101, 102, 103, 106, 107], "tzdata": 19, "u": [6, 7, 8, 15, 25, 38, 48, 49, 54, 67, 72, 80, 81, 83, 92, 101, 104], "u_": [6, 101], "u_1": 6, "u_i": 6, "u_n": 6, "uation": 15, "uiuc": 16, "uk": 104, "ultim": [27, 39, 46, 57, 101], "ultrafeedback": 64, "unambigu": 22, "unansw": 1, "unawar": 1, "unbalanc": 101, "unbias": [18, 58, 70, 105], "uncertainti": 69, "unchang": 91, "unclear": [46, 91], "uncur": 83, "under": [17, 20, 34, 43, 44, 53, 67, 68, 75], "underli": 103, "underload": 54, "underset": [56, 64, 69, 70, 84, 91], "understand": [6, 17, 38, 41, 42, 46, 50, 51, 57, 61, 93, 98], "undesir": [43, 65], "unembed": [9, 53, 71], "uneth": 76, "unexpect": [65, 88], "unhealthi": 65, "unicod": [41, 61, 93], "unicode\u548cutf": 110, "unicode\u5f53\u524d\u9ed8\u8ba4\u7684\u7248\u672c\u662fuc": 110, "unicode\u662fascii": 110, "unicode\u7684\u5b57\u7b26\u96c6\u8303\u56f4": 110, "unicode\u76f4\u63a5\u517c\u5bb9\u4e86\u521d\u671fascii": 110, "unifi": [2, 27, 39], "uniform": [51, 80, 92, 102, 105], "uniformli": [7, 69, 90, 92], "union": 58, "uniqu": [50, 51, 53], "unit": [18, 21, 35, 51, 56, 57, 60, 61, 102, 103], "univers": [38, 41, 93, 103], "unk": 80, "unknown": [1, 67, 80], "unlabel": [6, 84], "unleash": [38, 61], "unlik": [21, 35, 65, 102], "unlikelihood": 72, "unlimit": 7, "unlock": [51, 53], "unmerg": 73, "unpack": [28, 29], "unsatisfactori": 72, "unsupervis": [7, 41, 46, 61, 93], "unsur": 56, "until": [24, 36, 49, 50, 57, 69, 80, 101], "untruth": 9, "unveil": [27, 39], "up": [6, 8, 9, 13, 14, 24, 26, 33, 35, 36, 37, 49, 56, 57, 65, 69, 71, 72, 76, 90, 92, 95, 102, 103], "up_proj": 53, "updat": [25, 45, 49, 54, 61, 65, 73, 102], "upgrad": [16, 27, 39], "uplift": 65, "upon": [44, 54, 60, 61], "upper": [44, 65, 103], "upweight": 43, "uq": 104, "uritempl": 19, "url": [41, 93], "urllib3": 19, "us": [1, 5, 6, 7, 8, 9, 13, 14, 15, 18, 19, 20, 21, 24, 25, 27, 33, 34, 35, 36, 39, 41, 42, 43, 44, 45, 46, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 64, 65, 66, 67, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 88, 90, 92, 95, 98, 102, 103, 104, 105, 106, 107, 108], "usag": [2, 13, 27, 35, 39, 57], "use_fast_token": 111, "user": [1, 2, 9, 12, 14, 33, 35, 46, 51, 56, 57, 58, 71, 81, 82, 83, 95], "usual": [2, 5, 24, 25, 36, 57, 65, 67, 70, 80, 83, 90, 101, 104, 106, 107], "uszkoreit": [41, 93], "ut": 72, "util": [1, 17, 27, 39, 50, 51, 54, 56, 57, 60, 61, 65, 67, 76, 81, 88], "uv": 104, "uw_": 6, "v": [5, 43, 58, 59, 64, 72, 75, 78, 104, 106, 107, 108], "v0": [12, 16, 73], "v1": [20, 34, 56], "v2": [28, 29, 41, 54, 56, 93, 101, 104], "v3": [29, 56, 88], "v5": [51, 56], "v_": [64, 70], "v_head_dim": 53, "valid": [8, 9, 21, 24, 26, 31, 35, 36, 48, 50, 54, 60, 61, 65, 67, 71, 81, 84, 90], "valu": [5, 9, 21, 35, 45, 48, 54, 58, 59, 61, 68, 70, 71, 72, 75, 77, 80, 84, 101, 102, 103, 107], "valuabl": [106, 107], "valueerror": 58, "vanish": 69, "var": [58, 105], "vare": 45, "vari": [7, 17, 27, 39, 45, 49, 53, 55, 101, 102], "variabl": [61, 105, 106, 107], "varianc": [18, 43, 56, 60, 69, 70, 75, 105], "variant": [6, 51, 58, 59, 75, 82], "variat": [17, 41, 48, 58, 59, 78, 108], "varieti": [45, 57], "variou": [13, 15, 20, 24, 27, 34, 36, 39, 45, 52, 55, 57, 60, 61, 76, 82, 102, 103], "vast": 61, "vaswani": [41, 93], "vdot": [106, 107], "ve": 58, "vector": [5, 6, 21, 51, 58, 59, 64, 68, 69, 102, 103, 104, 105, 107, 108], "vedant": [41, 93], "verb": [14, 33], "verbos": 64, "verdict": 83, "veri": [9, 18, 45, 46, 48, 61, 71, 76, 78, 102, 103], "verif": [13, 57, 61, 88], "verifi": [2, 12, 13, 21, 22, 41, 54, 65, 88, 91, 93], "versatil": [41, 93], "version": [5, 9, 17, 20, 34, 35, 49, 51, 53, 56, 57, 58, 59, 67, 83, 88, 98, 108, 111], "versu": [75, 98], "veryeasyhack": 76, "via": [9, 13, 35, 41, 44, 50, 51, 57, 64, 65, 67, 75, 79, 83, 86, 88, 93], "view": [58, 59, 103], "view_as_complex": [58, 59, 103], "view_as_r": [58, 59, 103], "vineet": [41, 93], "violat": 78, "virtualenv": 19, "visual": [14, 33], "vllm": 16, "vocab": 68, "vocab_s": [53, 58, 59], "vocabulari": [53, 60, 68], "voss": [41, 93], "vote": [66, 90, 92], "vsp": [5, 6, 41, 60, 93, 104], "vw_": 5, "w": [5, 9, 41, 58, 59, 64, 67, 68, 71, 80, 82, 83, 93, 100, 104, 106, 107, 108], "w1": [58, 59], "w2": [58, 59, 108], "w3": [58, 59], "w_": [5, 6, 43, 58, 59, 107, 108, 112], "w_1": 61, "w_1s_1": 61, "w_2": 5, "w_e": 6, "w_n": 61, "w_ns_n": 61, "w_p": 6, "w_y": 6, "wa": [6, 7, 15, 22, 27, 35, 39, 48, 49, 51, 52, 54, 56, 61, 82, 91], "wai": [18, 21, 24, 36, 43, 44, 46, 50, 61, 65, 76, 80, 98, 101, 102, 103], "wainwright": [41, 93], "wait": 90, "waitlist": 9, "wake": [14, 33], "wan": [41, 93], "wang": [41, 93], "wangd": [41, 93], "want": [9, 24, 36, 43, 71, 80, 98, 102, 103], "warm": 65, "warmup": [35, 55, 95], "wast": 48, "wastag": 101, "wavecod": [41, 61, 93], "wavelength": [5, 102], "wcg19": [41, 60, 93], "we": [1, 2, 5, 6, 7, 8, 9, 12, 13, 14, 15, 17, 18, 20, 21, 22, 24, 25, 27, 31, 33, 34, 35, 36, 38, 39, 43, 44, 45, 46, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 86, 88, 90, 91, 92, 95, 98, 101, 102, 103, 104, 106, 107, 108, 112], "weak": [28, 72, 103], "weaker": [20, 34], "weakli": [45, 46], "weather": 1, "web": [7, 52, 65, 80], "webpag": [7, 71], "websit": [35, 55, 60, 61, 65], "webtext": 7, "webtext2": 45, "wei": [41, 93], "weigh": 67, "weight": [5, 6, 35, 43, 48, 53, 55, 56, 58, 59, 61, 65, 67, 72, 77, 95, 101, 103, 105, 107, 108, 112], "welind": [41, 93], "well": [5, 6, 12, 43, 60, 68, 72, 76, 102, 103], "wellcalibr": 76, "wen": [41, 93], "wenbin": [41, 93], "wenfeng": [41, 93], "wenji": [41, 93], "wenjun": [41, 93], "wentao": [41, 93], "wenxiang": [41, 93], "were": [18, 54, 56, 92], "west": [28, 29], "what": [1, 14, 33, 46, 52, 55, 69, 103], "wheel": 19, "when": [1, 2, 5, 7, 9, 14, 21, 24, 33, 35, 36, 41, 42, 43, 45, 46, 49, 50, 57, 58, 59, 64, 65, 68, 69, 70, 72, 75, 78, 81, 83, 90, 91, 92, 98, 101, 105, 107, 108], "where": [5, 6, 9, 13, 18, 21, 24, 25, 27, 32, 36, 38, 39, 43, 44, 45, 50, 51, 53, 54, 56, 57, 58, 59, 60, 61, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 77, 80, 81, 82, 83, 86, 90, 98, 101, 102, 103, 104, 105, 106, 107, 108, 112], "wherea": [41, 43, 48], "wherebi": 56, "wherein": 72, "wherev": 54, "whether": [24, 36, 41, 46, 48, 52, 54, 58, 61, 72, 75, 77, 83, 88, 91, 92, 101], "which": [1, 2, 5, 6, 7, 8, 9, 14, 15, 17, 18, 20, 21, 22, 24, 27, 33, 34, 36, 39, 42, 43, 44, 45, 48, 49, 50, 51, 52, 53, 54, 56, 57, 58, 59, 60, 61, 64, 65, 66, 67, 68, 69, 70, 71, 75, 76, 77, 78, 80, 81, 82, 83, 84, 88, 92, 98, 101, 102, 103, 105, 106, 107, 108, 112], "while": [2, 5, 7, 8, 9, 20, 21, 25, 34, 43, 45, 49, 50, 51, 53, 54, 57, 60, 61, 68, 69, 70, 75, 76, 80, 81, 82, 91, 95, 98, 101, 102, 103, 107, 109], "white": 18, "whiten": 56, "who": 21, "whole": 72, "whose": [9, 58, 61], "why": [46, 50], "wide": [6, 9, 17, 25, 45, 46, 49, 57, 64, 70, 81], "widespread": [41, 93], "width": 58, "wifi": 76, "wiki": 55, "wikihow": 95, "wikipedia": [7, 55], "wildchat": 35, "william": [41, 93], "win": [69, 77, 80, 82, 83], "window": [6, 25, 54, 57], "winner": 83, "winter": [41, 93], "wise": [6, 53, 54, 58, 59, 76, 90, 103, 105, 108], "within": [2, 54, 57, 61, 65, 75, 76, 83, 84, 88, 102, 103, 105], "without": [2, 7, 13, 15, 25, 43, 46, 61, 64, 65, 67, 68, 69, 71, 76, 83, 88, 92, 95, 107], "wizard": [28, 29], "wk": [58, 59], "wo": [58, 59], "wojciech": [41, 93], "word": [6, 7, 9, 12, 14, 21, 25, 33, 41, 61, 65, 78, 93, 100, 106, 107, 109], "wordpiece\u6bcf\u6b21\u9009\u62e9\u5408\u5e76\u7684\u4e24\u4e2a\u5b50\u8bcd": 109, "wordpiece\u7b97\u6cd5\u4e5f\u662f\u6bcf\u6b21\u4ece\u8bcd\u8868\u4e2d\u9009\u51fa\u4e24\u4e2a\u5b50\u8bcd\u5408\u5e76\u6210\u65b0\u7684\u5b50\u8bcd": 109, "work": [2, 5, 7, 8, 13, 20, 21, 24, 34, 36, 38, 43, 46, 53, 55, 57, 58, 59, 66, 67, 68, 76, 83, 103, 108], "world": [2, 14, 17, 25, 33], "world_siz": 73, "wors": [80, 98], "worst": [44, 84], "would": [5, 43, 46, 56, 64, 76, 83, 92], "wq": [58, 59], "write": [9, 12, 14, 22, 33, 41, 42, 51, 53, 54, 56, 71, 88, 95, 102, 107], "writer": 21, "written": [9, 18, 21, 24, 32, 36, 41, 42, 76], "wrong": [50, 68, 92, 98], "wrote": [14, 33, 76], "wu": [41, 93], "wv": [58, 59], "wwl": [35, 41, 57, 61, 93], "wx": 112, "x": [5, 6, 9, 15, 41, 43, 51, 56, 58, 59, 64, 66, 67, 68, 69, 70, 71, 72, 75, 77, 79, 80, 81, 83, 84, 91, 93, 100, 102, 103, 105, 106, 107, 108, 109, 112], "x_": [24, 36, 58, 59, 83, 91, 102, 103, 107], "x_0": [58, 59, 102, 103], "x_1": [5, 58, 59, 72, 79, 91, 102, 103], "x_2": [58, 59, 91], "x_i": [82, 83, 91], "x_m": 72, "x_n": [5, 79], "xdxac": 100, "xia": [41, 93], "xiangyu": [41, 93], "xianji": [41, 93], "xianzu": [41, 93], "xiao": [41, 93], "xiaodong": [41, 93], "xiaohan": [41, 93], "xiaohuan": [41, 93], "xiaojin": [41, 93], "xiaokang": [41, 93], "xiaosha": [41, 93], "xiaotao": [41, 93], "xiaowen": [41, 93], "xiaoxiang": [41, 93], "xie": [41, 93], "xin": [41, 93], "xingkai": [41, 93], "xingxuan": [41, 93], "xingzhang": [41, 93], "xinnan": [41, 93], "xinyi": [41, 93], "xinyu": [41, 93], "xiong": [41, 93], "xk": [58, 59, 103], "xk_": [58, 59, 103], "xk_out": [58, 59, 103], "xp": 108, "xq": [58, 59, 103], "xq_": [58, 59, 103], "xq_out": [58, 59, 103], "xu": [41, 93], "xuan": [41, 93], "xuancheng": [41, 93], "xue": [41, 93], "xuecheng": [41, 93], "xv": [58, 59, 108], "xw": [58, 59, 108], "xw_": [58, 59, 108], "xw_1": 5, "xx": [16, 29], "xxhash": 19, "xxx": 19, "xz": 19, "x\u4e3a\u5b57\u7b26\u7684unicode\u4e8c\u8fdb\u5236": 110, "y": [6, 9, 41, 43, 56, 58, 59, 64, 65, 67, 68, 69, 71, 72, 75, 77, 79, 80, 81, 83, 91, 93, 100, 102, 105, 109], "y1": 67, "y2": 67, "y3": 67, "y_": [9, 24, 36, 43, 56, 64, 66, 67, 68, 71, 72, 79, 80, 81, 82, 83, 84, 91], "y_1": [5, 9, 67, 72, 77, 81, 84, 91], "y_2": [9, 67, 77, 81, 84, 91], "y_c": 81, "y_i": [66, 91], "y_l": 68, "y_m": 5, "y_n": 72, "y_r": 81, "y_t": 72, "y_w": 68, "yaml": [19, 50], "yan": [41, 93], "yang": [41, 93], "yangyu": [41, 93], "yanhong": [41, 93], "yann": [41, 93], "yanp": [41, 93], "yao": [41, 93], "yaofeng": [41, 93], "yaohui": [41, 93], "yarl": 19, "yarn": 54, "ye": [41, 93], "yellow": 18, "yet": 13, "yi": [41, 93], "yibo": [41, 93], "yichang": [41, 93], "yichao": [41, 93], "yield": [5, 9, 22, 57, 69, 70, 81, 112], "yifeng": [41, 93], "yiliang": [41, 93], "yilong": [41, 93], "yin": [41, 93], "ying": [41, 93], "yishi": [41, 93], "yishuji": [41, 93], "yixin": [41, 93], "yixuan": [41, 93], "yiyuan": [41, 93], "yml": 19, "yongji": [41, 93], "yongqiang": [41, 93], "you": [12, 14, 19, 33, 41, 42, 51, 56, 75, 76, 81, 93, 98], "young": 21, "your": [12, 19, 41, 42, 51, 76, 93], "yu": [41, 93], "yuan": [41, 93], "yuchen": [41, 93], "yuduan": [41, 93], "yuheng": [41, 93], "yukun": [41, 93], "yuliang": 29, "yunfei": [41, 93], "yunfeng": [41, 93], "yunlong": [41, 93], "yunxian": [41, 93], "yuqiong": [41, 93], "yura": [41, 93], "yuri": [41, 93], "yute": [41, 93], "yuwei": [41, 93], "yuxiang": [41, 93], "yuxuan": [41, 93], "yuyao": [41, 93], "yyl": [41, 93, 98], "yz": [41, 61, 93], "yzh": [41, 61, 93], "z": [5, 41, 67, 69, 75, 80, 93, 100, 109], "z_": 80, "z_1": 5, "z_n": 5, "zabdzabac": 100, "zaremba": [41, 93], "zehui": [41, 93], "zekun": [41, 93], "zemlyanskii": [41, 93], "zeng": [41, 93], "zero": [8, 17, 43, 56, 58, 59, 98, 107], "zero_stag": 73, "zeros_lik": 58, "zeyu": [41, 93], "zh": 52, "zha": [41, 93], "zhang": [41, 93], "zhangli": [41, 93], "zhao": [41, 93], "zhaojian": [41, 93], "zhe": [41, 93], "zhen": [41, 93], "zhenda": [41, 93], "zheng": [41, 93], "zhenru": [41, 93], "zhewen": [41, 93], "zhihong": [41, 93], "zhiniu": [41, 93], "zhipeng": [41, 93], "zhongyu": [41, 93], "zhou": [41, 93], "zhoujun": [41, 93], "zhu": [41, 93], "zhuoshu": [41, 93], "ziegler": [41, 93], "zihan": [41, 93], "zihui": [41, 93], "zilin": [41, 93], "zip": [58, 80], "zipp": 19, "ziwei": [41, 93], "zixuan": [41, 93], "zlib": 19, "zlm": [12, 41, 93], "zou": [41, 93], "zy": 100, "zydzyac": 100, "\u4e00": 110, "\u4e00\u4e2a\u5728\u5f00\u5934": 100, "\u4e00\u822c\u4e0d\u76f4\u63a5\u4f7f\u7528unicod": 110, "\u4e00\u90e8\u5206\u6b63\u8d1f\u62b5\u6d88\u4e00\u90e8\u5206\u632f\u8361": 102, "\u4e00\u95e8\u8bed\u8a00\u4e2d": 100, "\u4e0a\u9762\u4ecb\u7ecd\u4e86\u6587\u5b57\u5b57\u7b26\u6620\u5c04\u4e3aunicode\u5b57\u7b26\u96c6": 110, "\u4e0b\u4e00\u4e2a\u5e38\u89c1\u7684\u5b57\u8282\u5bf9\u662f": 100, "\u4e0b\u8f7d": 16, "\u4e0b\u8f7d\u8bc4\u4f30\u6587\u4ef6": 19, "\u4e0b\u9762\u4e3e\u4e2a\u4f8b\u5b50": 100, "\u4e0b\u9762\u7684\u793a\u4f8b\u5c06\u89e3\u91ca": 100, "\u4e0d\u4f1a\u51fa\u73b0\u53ea\u5b66\u90a3\u4e9b\u7b80\u5355\u4e14": 70, "\u4e0d\u4f1a\u5355\u72ec\u51fa\u73b0": 100, "\u4e0d\u8868\u793a\u5176\u4ed6\u8bed\u8a00": 110, "\u4e0d\u8db3\u7684\u5728\u9ad8\u4f4d\u75280\u8865\u5145": 110, "\u4e0ebpe\u7684\u6700\u5927\u533a\u522b\u5728\u4e8e": 109, "\u4e0ebpe\u7b97\u6cd5\u7c7b\u4f3c": 109, "\u4e14\u5047\u8bbe\u5404\u4e2a\u5b50\u8bcd\u4e4b\u95f4\u662f\u72ec\u7acb\u5b58\u5728\u7684": 109, "\u4e24\u4e2a\u5b57\u6bb5": 16, "\u4e2a": 100, "\u4e2a\u4e0d\u540c\u7684token": 100, "\u4e2a\u5355\u8bcd": 100, "\u4e2a\u5b50\u8bcd\u7ec4\u6210": 109, "\u4e2d": 16, "\u4e2d\u4f7f\u7528\u4e86\u4e0a\u8ff0\u7b97\u6cd5\u7684\u4e00\u4e2a\u53d8\u4f53": 100, "\u4e2d\u51cf\u5c11\u8ba1\u6570": 100, "\u4e2d\u5b58\u5728": 100, "\u4e2d\u62bd\u53d6\u51fa\u6765": 19, "\u4e2d\u6587": 110, "\u4e2d\u7684": 16, "\u4e2d\u76f8\u5bf9\u597d\u7684": 70, "\u4e2d\u88ab\u9996\u6b21\u63d0\u51fa": 100, "\u4e3a\u4e86\u4ee5\u6700\u6709\u6548\u7684\u65b9\u5f0f\u6784\u5efa\u8bed\u6599\u5e93": 100, "\u4e3a\u4e86\u5408\u5e76": 100, "\u4e3a\u4e86\u66f4\u6e05\u6670\u7684\u7406\u89e3": 100, "\u4e3a\u4e86\u672c\u6587\u7684\u7b80\u5355\u8d77\u89c1": 100, "\u4e3a\u4e86\u8282\u7701\u7a7a\u95f4": 110, "\u4e3a\u4e86\u89e3\u51b3": 110, "\u4e3a\u4ec0\u4e48\u4e0d\u76f4\u63a5\u8c03\u7528": 16, "\u4e3a\u53e5\u5b50\u7684\u4e00\u4e2a\u5206\u8bcd\u7ed3\u679c": 109, "\u4e3a\u8865\u5145": 110, "\u4e3e\u4f8b1": 110, "\u4e3e\u4f8b2": 110, "\u4e4b\u524d": 110, "\u4e5f\u53eb\u5b57\u7b26\u96c6": 110, "\u4e5f\u5c31\u662f\u4e24\u5b50\u8bcd\u5728\u8bed\u8a00\u6a21\u578b\u4e0a\u5177\u6709\u8f83\u5f3a\u7684\u5173\u8054\u6027": 109, "\u4e5f\u5c31\u662f\u628a\u6bcf\u4e00\u4e2a\u5355\u8bcd\u770b\u6210\u4e00\u4e2atoken": 100, "\u4e5f\u5c31\u662f\u7528\u5b9a\u957f2\u4e2a\u5b57\u8282\u6765\u8868\u793a\u5b57\u7b26": 110, "\u4e5f\u662fnlp\u4e2d\u6700\u91cd\u8981\u7684\u7f16\u7801\u65b9\u5f0f\u4e4b\u4e00": 100, "\u4e5f\u80fd\u88ab\u7b97\u4f5c\u5b57\u7b26\u5bf9\u7684\u4e00\u90e8\u5206": 100, "\u4e8c\u8fdb\u5236\u5c31\u662f01000001": 110, "\u4e8c\u8fdb\u5236\u5c31\u662f110000": 110, "\u4eca\u5929\u5c31\u6765\u4e86\u89e3\u4e0b\u5b57\u7b26\u5728\u8ba1\u7b97\u673a\u4e2d\u7684\u7f16\u7801\u65b9\u5f0f": 110, "\u4ece": 19, "\u4ece\u6700\u957f\u5230\u6700\u77ed": 100, "\u4ece\u800c\u6211\u4eec\u77e5\u9053\u5269\u4f59\u7684": 100, "\u4ed6\u4eec\u5177\u6709\u6700\u5927\u7684\u4e92\u4fe1\u606f\u503c": 109, "\u4ee5\u4e2d\u6587": 110, "\u4f1a\u4e0d\u4f1a\u4f7f\u5f97\u8bad\u7ec3\u53d8\u5f97\u5f88\u6162": 70, "\u4f1a\u4e0d\u4f1a\u66f4\u597d": 70, "\u4f20\u5165\u89e3\u6790\u540e\u7684\u53c2\u6570": 16, "\u4f3c\u7136\u503c\u7684\u53d8\u5316\u53ef\u8868\u793a\u4e3a": 109, "\u4f46\u4e00\u4e2a\u8bcd\u5728\u7ed3\u5c3e\u6709\u4e00\u4e2a": 100, "\u4f46\u4ecd\u6709\u53ef\u80fd\u51fa\u73b0\u4e0d\u5728\u91cc\u9762\u7684\u5355\u8bcd": 100, "\u4f46\u5b83\u5177\u6709\u826f\u597d\u7684\u6027\u80fd": 100, "\u4f46\u6211\u4eec\u53ea\u6709\u4e00\u4e2a\u5355\u8bcd\u5305\u542b\u8fd9\u4e9b\u5b57\u7b26": 100, "\u4f46\u662funicode\u8fd8\u662f\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6269\u5c55\u673a\u5236": 110, "\u4f46\u662f\u5176\u4ed6\u6b27\u6d32\u56fd\u5bb6\u7684\u8bed\u8a00\u6bd4\u5982\u6cd5\u8bed": 110, "\u4f46\u662f\u5b83\u548c\u6c49\u5b57\u7684\u6620\u5c04\u4e0eunicode\u548c\u6c49\u5b57\u7684\u6620\u5c04\u6ca1\u6709\u4efb\u4f55\u5173\u7cfb": 110, "\u4f46\u8fd9\u5e76\u4e0d\u4e00\u5b9a\u662f\u6700\u5408\u7406\u7684\u7f16\u7801\u65b9\u5f0f": 100, "\u4f4e\u7ef4": 102, "\u4f4e\u9891\u5185\u63d2\u7684\u65b9\u5f0f": 102, "\u4f60\u53ef\u80fd\u4e0d\u6e05\u695awordpiece\u662f\u5982\u4f55\u9009\u53d6\u5b50\u8bcd\u7684": 109, "\u4f7f\u7528\u4e0b\u8ff0\u547d\u4ee4\u8fdb\u884c\u8bc4\u4f30": 19, "\u4f8b\u5982\u5b57\u7b26\u4e32": 16, "\u4f8b\u5982\u7f16\u7801\u5e8f\u5217": 100, "\u4fee\u6539\u540e\u663e\u793a\u7ed3\u679c\u5982\u4e0b": 110, "\u5047\u8bbe\u5355\u8bcd\u7684\u5e8f\u5217\u662f": 100, "\u5047\u8bbe\u53e5\u5b50": 109, "\u5047\u8bbe\u6211\u4eec\u6709\u4e00\u4e2a\u8bed\u6599\u5e93": 100, "\u5047\u8bbe\u6211\u4eec\u6709\u9700\u8981\u7f16\u7801": 100, "\u5047\u8bbe\u628a\u76f8\u90bb\u4f4d\u7f6e\u7684x\u548cy\u4e24\u4e2a\u5b50\u8bcd\u8fdb\u884c\u5408\u5e76": 109, "\u5047\u8bbe\u8fd9\u4e9b\u8bcd\u51fa\u73b0\u7684\u9891\u7387\u5982\u4e0b": 100, "\u50cf": 100, "\u5141\u8bb8\u8868\u793a\u4e00\u767e\u591a\u4e07\u4e2a\u5b57\u7b26": 110, "\u5168\u7403\u7edd\u5927\u90e8\u5206\u5b57\u7b26\u7684\u5b57\u7b26\u96c6": 110, "\u5176\u4e2d": [19, 100], "\u5176\u4e2d\u4e0d\u6b62utf": 110, "\u5176\u4e2d\u4f1a\u6d89\u53ca\u5230ascii": 110, "\u5176\u4e2d\u5305\u542b\u5355\u8bcd": 100, "\u5176\u4ed6\u5b57\u8282": 110, "\u5176\u4ed6\u8bed\u8a00": 110, "\u5176\u4ed6\u8bed\u8a00\u53ef\u80fd\u6709\u6240\u4e0d\u540c": 100, "\u51fa\u73b0\u4e86": 100, "\u51fd\u6570": 16, "\u51fd\u6570\u5462": 16, "\u51fd\u6570\u5feb\u901f\u8f6c\u6362\u4e3a\u547d\u4ee4\u884c\u63a5\u53e3": 16, "\u51fd\u6570\u7684\u53c2\u6570\u5217\u8868": 16, "\u5206\u522b\u6765\u81ea": 16, "\u5219\u53e5\u5b50": 109, "\u521d\u59cbtoken\u5c06\u662f\u6240\u6709\u5b57\u7b26\u548c": 100, "\u524d\u9762\u5168\u90e8\u586b\u51450": 110, "\u5269\u4e0b\u7684\u552f\u4e00\u5b57\u8282\u5bf9\u662f": 100, "\u52a0\u4e0a\u63a7\u5236\u7801\u540e\u5bf9\u5e94\u7684utf": 110, "\u5339\u914d": 16, "\u5341\u516d\u8fdb\u5236": 110, "\u5355\u5b57\u8282256\u4e2a\u5b57\u7b26\u80af\u5b9a\u6ca1\u6cd5\u5b8c\u5168\u8868\u793a": 110, "\u5373": [16, 100], "\u538b\u7f29": 100, "\u539f\u672c\u5728\u4f4e\u7ef4\u5ea6\u4e0a": 102, "\u53c2\u6570\u4e3a": 19, "\u53cd\u5411\u6267\u884c\u4ee5\u4e0a\u8fc7\u7a0b\u5c31\u884c\u4e86": 100, "\u53cd\u590d\u8fed\u4ee3\u76f4\u5230\u6ee1\u8db3\u505c\u6b62\u6761\u4ef6": 100, "\u53ea\u9700\u8981\u4e00\u4e2a\u5b57\u8282\u8868\u793a": 110, "\u53ea\u9700\u8981\u4f7f\u7528\u540e\u97627\u4f4d\u5c31\u8db3\u591f\u4e86": 110, "\u53ef\u4ee5\u4f7f\u75281": 110, "\u5404\u4e2a\u56fd\u5bb6\u4fbf\u7740\u624b\u81ea\u5df1\u56fd\u5bb6\u8bed\u8a00\u7684\u7f16\u7801": 110, "\u5408\u5e76\u505c\u6b62token": 100, "\u5408\u5e76\u540e\u4ea7\u751f\u7684\u5b50\u8bcd\u8bb0\u4e3az": 109, "\u5408\u5e76\u5b57\u7b26\u53ef\u4ee5\u8ba9\u4f60": 100, "\u5408\u5e76\u5b83\u4eec": 100, "\u540c\u7406\u8fd8\u6709\u5176\u4ed6\u56fd\u5bb6\u7684\u7279\u6709\u5b57\u7b26\u96c6": 110, "\u548c": [16, 100, 102], "\u548cascii\u7684\u76ee\u7684\u4e00\u6837": 110, "\u548cascii\u7801\u4e00\u81f4": 110, "\u56de\u8f6613\u7b49\u7b49\u4e00\u4e9b\u952e\u76d8\u4e0a\u6240\u89c1\u7684\u7b26\u53f7": 110, "\u56e0\u4e3a": 100, "\u56e0\u4e3a\u4eba\u7c7b\u8bed\u8a00\u4e5f\u7ecf\u5e38\u4ee5\u5355\u8bcd\u4e3a\u5355\u4f4d\u8fdb\u884c\u4ea4\u6d41": 100, "\u56e0\u4e3a\u5b83\u4eec\u5728\u6211\u4eec\u7684\u8bed\u6599\u5e93\u4e2d\u51fa\u73b0\u4e86": 100, "\u56e0\u4e3a\u6211\u4eec\u7edf\u8ba1\u76f8\u90bb\u5b57\u7b26\u5bf9\u65f6\u4e0d\u80fd\u628a\u5206\u522b\u4f4d\u4e8e\u4e24\u4e2a\u5355\u8bcd\u4e2d\u7684\u5b57\u7b26\u5bf9\u7b97\u8fdb\u53bb": 100, "\u56e0\u4e3a\u6ca1\u6709\u51fa\u73b0\u591a\u6b21\u7684\u5b57\u8282\u5bf9": 100, "\u56e0\u6b64": 100, "\u56e0\u6b64bpe\u4e5f\u662f\u5178\u578b\u7684\u57fa\u4e8esubword\u7684tokenization\u7b97\u6cd5": 100, "\u56e0\u6b64\u6211\u4eec\u53ef\u91c7\u7528\u9ad8\u9891\u5916\u63a8": 102, "\u56e0\u6b64\u6211\u4eec\u5c06\u7528\u4e00\u4e2a\u65b0\u5b57\u8282": 100, "\u56e0\u6b64\u6211\u4eec\u6ca1\u6709\u5c06\u5b83\u4eec\u5408\u5e76": 100, "\u5728": 16, "\u5728finest\u548clowest\u4e24\u4e2a\u8bcd\u4e2d": 100, "\u5728unicode\u8bde\u751f": 110, "\u5728\u5b9e\u8df5\u4e2d": 100, "\u5728\u6211\u4eec\u7684\u8bed\u6599\u5e93\u4e2d": 100, "\u5728\u6211\u4eec\u7684\u8bed\u6599\u5e93\u4e2d\u51fa\u73b0\u4e86": 100, "\u5728\u6bcf\u4e2a\u5355\u8bcd\u7684\u672b\u5c3e\u6dfb\u52a0": 100, "\u5728\u8ba1\u7b97\u673a\u4e2d\u6240\u6709\u4fe1\u606f\u90fd\u662f\u4ee5\u4e8c\u8fdb\u5236\u4f4d0\u548c1\u6765\u5b58\u50a8\u7684": 110, "\u5728\u8bed\u8a00\u6a21\u578b\u9884\u6d4b\u7684\u65f6\u5019\u9700\u8981\u5728\u8fd9\u4e2a\u62e5\u6709\u51e0\u4e07\u4e2a\u5355\u8bcd\u7684\u5217\u8868\u4e0a\u8ba1\u7b97\u4e00\u4e2a\u6982\u7387\u5206\u5e03": 100, "\u5728\u8fd9\u91cc": 100, "\u5728\u8fed\u4ee3\u7684\u65f6\u5019\u901a\u8fc7\u6bd4\u8f83token\u7684\u9891\u7387\u5927\u5c0f\u6765\u7a77\u5c3d\u6bcf\u4e00\u79cd\u53ef\u80fd": 100, "\u5927\u5199\u5b57\u6bcda\u5bf9\u5e94\u7684ascii\u7801\u662f65": 110, "\u5927\u5bb6\u90fd\u77e5\u9053\u7535\u8111\u6240\u6709\u4fe1\u606f\u90fd\u662f\u4ee5\u4e8c\u8fdb\u5236\u7684\u65b9\u5f0f\u6765\u8ba1\u7b97\u548c\u5b58\u50a8\u7684": 110, "\u5927\u6982\u5c31\u662f\u8bf4ascii\u662f\u7f8e\u56fd\u4fe1\u606f\u4ea4\u6362\u6807\u51c6\u4ee3\u7801": 110, "\u5927\u90e8\u5206\u662f2\u5b57\u8282": 110, "\u5982\u4f55\u6765\u8868\u793aunicod": 110, "\u5982\u4f55\u77e5\u9053\u5f53\u524d\u5b57\u8282\u5c31\u662f\u8981\u8868\u793a\u4e00\u4e2a\u5b57\u7b26\u8fd8\u662f\u8fde\u7eed\u7684\u4e24\u4e2a\u5b57\u8282\u8868\u793a\u4e00\u4e2a\u5b57\u7b26": 110, "\u5982\u4f55\u8ba9\u8ba1\u7b97\u673a\u8bfb\u61c2unicod": 110, "\u5982\u4f55\u8bfb\u53d6unicode\u5b57\u7b26\u96c6": 110, "\u5982\u4f55\u9009\u62e9\u4e24\u4e2a\u5b50\u8bcd\u8fdb\u884c\u5408\u5e76": 109, "\u5982\u679c\u4f1a\u7559\u4e0b\u51e0\u4e2a\u5b50\u4e32": 100, "\u5982\u679c\u5728\u4f4e\u7ef4\u5ea6\u8fdb\u884c\u5185\u63d2": 102, "\u5982\u679c\u6309\u7167unicode\u7684\u65b9\u5f0f\u7edf\u4e00\u7528\u4e24\u4e2a\u5b57\u8282\u8868\u793a\u4e00\u4e2a\u5b57\u7b26": 110, "\u5982\u679c\u7b97\u6cd5\u770b\u5230token": 100, "\u5b57\u6bcd": 110, "\u5b57\u7b26\u7801\u5c31\u662f\u5bf9\u5e94\u7684unicod": 110, "\u5b57\u7b26\u7801\u7ec4\u6210": 110, "\u5b57\u7b26\u7b49": 100, "\u5b57\u7b26\u96c6\u5373\u662f\u6587\u5b57\u7b26\u53f7\u548c\u4e8c\u8fdb\u5236\u7684\u4e00\u79cd\u6620\u5c04\u5173\u7cfb": 110, "\u5b57\u8282\u957f\u5ea6": 110, "\u5b66\u540c\u4e00\u4e2a": 70, "\u5b83\u4e0d\u80fd\u88ab\u8fdb\u4e00\u6b65\u538b\u7f29": 100, "\u5b83\u4eec\u51fa\u73b0\u4e86": 100, "\u5b83\u4eec\u7ecf\u5e38\u5728\u8bed\u6599\u4e2d\u4ee5\u76f8\u90bb\u65b9\u5f0f\u540c\u65f6\u51fa\u73b0": 109, "\u5b83\u53ea\u6709\u4e00\u4e2a": 100, "\u5b83\u5728": 100, "\u5b83\u5c31\u4f1a\u77e5\u9053\u5b83\u662f": 100, "\u5b83\u7684\u4f5c\u7528\u662f\u628a\u5404\u4e2a\u8bed\u8a00\u7684\u5b57\u7b26\u6620\u5c04\u4e3a\u4e8c\u8fdb\u5236": 110, "\u5b83\u7684\u7279\u70b9\u662f\u53d8\u957f\u7f16\u7801": 110, "\u5b83\u9075\u5faa\u4e00\u79cd\u8d2a\u5a6a\u7684\u7b56\u7565\u6765\u5c3d\u53ef\u80fd\u53d6\u5f97\u6700\u4f18\u7684\u89e3\u51b3\u65b9\u6848": 100, "\u5b8c\u5168\u517c\u5bb9ascii": 110, "\u5bf9\u4e0eascii\u7a7a\u95f4\u6d6a\u8d39": 110, "\u5bf9\u4e8e\u5355\u5b57\u8282\u7684\u5b57\u7b26": 110, "\u5bf9\u4e8e\u53e5\u5b50": 109, "\u5bf9\u4e8e\u5b58\u50a8\u7a7a\u95f4\u662f\u6781\u5927\u7684\u6d6a\u8d39": 110, "\u5bf9\u4e8e\u672a\u77e5": 100, "\u5bf9\u4e8e\u82f1\u6587\u56fd\u5bb6\u6765\u8bf4\u80fd\u6ee1\u8db3\u65e5\u5e38\u4f7f\u7528": 110, "\u5bf9\u4e8e\u9700\u8981n\u4e2a\u5b57\u8282\u7684\u5b57\u7b26": 110, "\u5bf9\u5e94\u7684unicode\u5b57\u7b26\u96c6\u662f4e00": 110, "\u5bf9\u5e94\u7684unicode\u662fu": 110, "\u5bf9\u5e94\u7684\u4e8c\u8fdb\u5236\u4e3a01000001": 110, "\u5bf9\u65b0\u6570\u636e\u8fdb\u884c\u7f16\u7801\u7684\u8fc7\u7a0b\u8fd8\u662f\u6bd4\u8f83\u7b80\u5355\u7684": 100, "\u5bf9\u7528\u4f4e\u7ef4\u533a\u5206\u4e0d\u540c\u4f4d\u7f6e\u95f4\u7684\u80fd\u529b\u5f71\u54cd\u66f4\u5927": 102, "\u5bfb\u627e\u6700\u5e38\u51fa\u73b0\u7684\u5b57\u8282\u5bf9": 100, "\u5c06": 16, "\u5c06\u53c2\u6570\u503c\u8f6c\u6362\u4e3a\u6b63\u786e\u7684\u7c7b\u578b": 16, "\u5c31\u4ee3\u8868\u4e00\u4e2a\u8bed\u8a00\u5355\u4f4d": 100, "\u5c31\u50cf\u5355\u8bcd": 100, "\u5c31\u662f\u4f7f\u7528\u63a7\u5236\u7801": 110, "\u5c31\u80fd\u4ee3\u8868\u4e86\u6240\u6709\u952e\u76d8\u4e0a\u7684\u666e\u901a\u7b26\u53f7": 110, "\u5c31\u8bde\u751f\u4e86utf": 110, "\u5c3d\u7ba1\u8d2a\u5a6a": 100, "\u5e03\u5c14\u503c\u7b49": 16, "\u5e74\u53d1\u8868\u7684\u6587\u7ae0": 100, "\u5e76\u4e00\u6b21\u53c8\u4e00\u6b21\u5730\u6267\u884c\u76f8\u540c\u7684\u8fed\u4ee3": 100, "\u5e76\u4e14\u6211\u4eec\u7684\u5b50\u5b57\u7b26\u4e32\u5c06\u88ab\u66ff\u6362\u4e3a\u6211\u4eectoken\u5217\u8868\u4e2d\u5df2\u7ecf\u5b58\u5728\u7684token\u7ec4\u5408": 100, "\u5e76\u4e14\u7531utf": 110, "\u5e76\u5c06\u5176\u91cd\u547d\u540d\u4e3a": 16, "\u5e76\u5c06\u5176\u9891\u7387\u8bb0\u4e3a": 100, "\u5e76\u5c06\u5b83\u4eec\u6dfb\u52a0\u5230token\u5217\u8868\u4e2d": 100, "\u5e76\u5c06\u65b0\u8bcd\u7684token\u6dfb\u52a0\u5230\u6211\u4eec\u7684token\u5b57\u5178\u4e2d\u4ee5\u5907\u5c06\u6765\u7528\u5230": 100, "\u5e76\u5c1d\u8bd5\u4f7f\u7528\u8fd9\u4e9btoken\u66ff\u6362\u7ed9\u5b9a\u5355\u8bcd\u5e8f\u5217\u4e2d\u7684\u5b50\u5b57\u7b26\u4e32": 100, "\u5e76\u88ab\u4f5c\u4e3a\u673a\u5668\u7ffb\u8bd1\u7b49\u4e3b\u6d41nlp\u4efb\u52a1\u7684\u9996\u9009tokenize\u65b9\u6cd5\u4e4b\u4e00": 100, "\u5e76\u91cd\u65b0\u8ba1\u7b97\u6bcf\u4e2atoken\u51fa\u73b0\u7684\u9891\u7387": 100, "\u5e93": 16, "\u5f00\u5934": 110, "\u5f00\u59cb": 100, "\u5f53\u524d\u5206\u8bcd\u4e0b\u53e5\u5b50s\u7684\u4f3c\u7136\u503c\u53ef\u4ee5\u8868\u793a\u4e3a": 109, "\u5f53\u7136": 100, "\u5f53\u8fd0\u884c\u811a\u672c\u65f6": 16, "\u610f\u5473\u7740\u8fd9\u4e9b\u7ef4\u5ea6\u4e0a\u7684\u4fe1\u53f7\u53d8\u5316\u975e\u5e38\u8fc5\u901f": 102, "\u6211\u4eec\u4f1a\u5c06": 100, "\u6211\u4eec\u5148\u7528\u4e00\u53e5\u8bdd\u6982\u62ec\u5b83\u7684\u6838\u5fc3\u601d\u60f3": 100, "\u6211\u4eec\u53ef\u4ee5\u770b\u5230": 100, "\u6211\u4eec\u53ef\u4ee5\u9012\u5f52\u5730\u4f7f\u7528\u5b57\u8282\u5bf9\u7f16\u7801\u5c06": 100, "\u6211\u4eec\u5c06tokenized\u597d\u7684\u5355\u8bcd\u4fdd\u5b58\u5728\u5b57\u5178\u4e2d": 100, "\u6211\u4eec\u5c06\u4ece\u7b2c\u4e8c\u5e38\u89c1\u7684\u6807\u8bb0": 100, "\u6211\u4eec\u5c06\u5b57\u7b26\u89c6\u4e3a\u4e0e\u5b57\u8282\u7b49\u4ef7": 100, "\u6211\u4eec\u5c06\u5b83\u4eec\u5408\u5e76\u4ee5\u5f62\u6210\u4e00\u4e2a\u65b0\u7684token": 100, "\u6211\u4eec\u5c06\u6bcf\u4e2a\u5355\u8bcd\u62c6\u5206\u4e3a\u5b57\u7b26\u5e76\u8ba1\u7b97\u5b83\u4eec\u7684\u51fa\u73b0\u6b21\u6570": 100, "\u6211\u4eec\u5c06\u7528unknown": 100, "\u6211\u4eec\u5c06\u7ee7\u7eed\u6267\u884c\u6b64\u5408\u5e76\u6b65\u9aa4": 100, "\u6211\u4eec\u5c06\u88ab\u89e3\u7801\u4e3a": 100, "\u6211\u4eec\u5c06\u904d\u5386\u6211\u4eec\u5728\u8bed\u6599\u5e93\u4e2d\u627e\u5230\u7684\u6240\u6709token": 100, "\u6211\u4eec\u5c06\u904d\u5386\u6240\u6709token": 100, "\u6211\u4eec\u5e38\u7528\u7684\u8bed\u8a00\u6a21\u578b\u8bcd\u6c47\u5217\u8868\u662f\u5f88\u5927\u7684": 100, "\u6211\u4eec\u5e94\u7528\u4e0a\u8ff0\u7f16\u7801\u65b9\u6cd5\u5bf9\u65b0\u8bcd\u8fdb\u884ctoken": 100, "\u6211\u4eec\u5fc5\u987b\u7b80\u5355\u5730\u5c06\u6240\u6709token\u8fde\u63a5\u5728\u4e00\u8d77\u4ee5\u83b7\u5f97\u6574\u4e2a\u5355\u8bcd": 100, "\u6211\u4eec\u603b\u5171\u6709": 100, "\u6211\u4eec\u6709\u4e00\u4e2a\u9891\u7387\u4e3a": 100, "\u6211\u4eec\u6765\u67e5\u51fa\u8fd9\u51e0\u4e2a\u5b57\u5bf9\u5e94\u7684utf": 110, "\u6211\u4eec\u73b0\u5728\u5c06\u5408\u5e76token": 100, "\u6211\u4eec\u73b0\u5728\u6709": 100, "\u6211\u4eec\u73b0\u5728\u6709\u4e86": 100, "\u6211\u4eec\u73b0\u5728\u770b\u5230\u5b57\u8282\u5bf9": 100, "\u6211\u4eec\u7684\u6570\u636e\u73b0\u5728\u5df2\u8f6c\u6362\u4e3a": 100, "\u6211\u4eec\u7684\u6a21\u578b\u5728\u8bad\u7ec3\u4e2d\u6ca1\u6709\u770b\u5230\u7684\u8bcd": 100, "\u6211\u4eec\u770b\u5230\u5b57\u8282\u5bf9": 100, "\u6211\u4eec\u77e5\u9053": 100, "\u6211\u4eec\u8ba1\u7b97\u8fd9\u4e9b\u8bcd\u5728\u8bed\u6599\u5e93\u4e2d\u7684\u51fa\u73b0\u9891\u7387": 100, "\u6211\u4eec\u8fd8\u5c06\u4ece\u5355\u4e2atoken": 100, "\u6211\u4eec\u90fd\u4e86\u89e3\u4e00\u79cd\u6700\u57fa\u672c\u7684token": 100, "\u6216": 100, "\u6216\u8005\u53eb": 110, "\u6240\u4ee5": [100, 109], "\u6240\u4ee5ascii\u7684\u5b57\u7b26": 110, "\u6240\u4ee5\u4ed6\u4eec\u5c31\u628a\u7b2c\u4e00\u4e2a\u7a7a\u95f20\u4f7f\u7528\u4e86\u8d77\u6765": 110, "\u6240\u4ee5\u4ed6\u4eec\u628a\u6bcf\u4e2a\u5b57\u8282\u7684\u7b2c\u4e00\u4f4d\u7f6e\u7f6e0": 110, "\u6240\u4ee5\u4f7f\u7528\u4e24\u4e2a\u5b57\u8282\u5df2\u7ecf\u8db3\u591f": 110, "\u6240\u4ee5\u5165\u53e3\u662f": 16, "\u6240\u4ee5\u5bf9\u5e94\u7684utf": 110, "\u6240\u4ee5\u5c31\u51fa\u73b0\u4e86\u4e00\u4e2a\u5168\u7403\u7edf\u4e00\u7684\u7f16\u7801\u89c4\u5219\u53ebunicod": 110, "\u6240\u4ee5\u6211\u4eec\u4e0d\u5bf9\u5b83\u8fdb\u884c\u7f16\u7801": 100, "\u6240\u4ee5\u6211\u4eec\u6709": 100, "\u6240\u4ee5\u8fd9\u5c31\u51fa\u73b0\u4e86\u4e00\u4e2a\u95ee\u9898": 110, "\u628a": 16, "\u628a\u5b83\u653e\u5728\u672c\u5730": 19, "\u63a5\u4e0b\u6765": 100, "\u63a7\u5236\u7801\u5c31\u662f\u544a\u8bc9\u8ba1\u7b97\u673a\u5f53\u524d\u662f\u5355\u5b57\u8282\u8fd8\u662f\u591a\u5b57\u8282": 110, "\u653e\u5165\u7528\u6237\u4e3b\u76ee\u5f55\u4e0b\u7684": 16, "\u6563\u5ea6\u76f4\u63a5\u653e\u5728": 70, "\u6570\u5b57\u548c\u5b57\u6bcd\u90fd\u672a\u4e71\u7801": 110, "\u6570\u636e\u7684\u538b\u7f29": 100, "\u6587\u4ef6": 16, "\u6587\u4ef6\u4e2d\u6709\u5982\u4e0b\u914d\u7f6e": 16, "\u6587\u4ef6\u653e\u5165\u6b64\u76ee\u5f55\u5e76\u91cd\u547d\u540d\u4e3a": 16, "\u659c\u4f53": 110, "\u65b0": 100, "\u65b0\u5efa\u4e00\u4e2ahtml\u8f93\u5165": 110, "\u65cb\u8f6c\u5f97\u8d8a\u591a": 102, "\u65cb\u8f6c\u5f97\u8d8a\u5c11": 102, "\u65cb\u8f6c\u89d2\u5ea6\u8f83\u5927": 102, "\u65e0\u8bba\u5982\u4f55": 100, "\u65e0\u8bba\u662fascii\u7f16\u7801\u8fd8\u662futf": 110, "\u65e5\u6587": 110, "\u65f6": 102, "\u662f\u4e00\u79cd\u7b80\u5355\u7684\u6570\u636e\u538b\u7f29\u7b97\u6cd5": 100, "\u662f\u4ee3\u7801\u7247\u6bb5": 19, "\u662f\u4f7f\u7528": 16, "\u662f\u4f7f\u7528\u6700\u5e7f\u6cdb\u7684sub": 100, "\u662f\u7684": 100, "\u666e\u901a\u7b26\u53f7\u7684\u5b57\u7b26\u96c6": 110, "\u66ff\u6362\u5b83": 100, "\u6700\u5e38\u51fa\u73b0": 100, "\u6700\u5e38\u89c1\u7684\u5e26\u6709": 100, "\u6700\u7ec8": 100, "\u6700\u7ec8\u5bfc\u81f4": 102, "\u6709\u4e00\u79cd\u7f16\u7801\u65b9\u5f0f\u80fd\u5927\u5927\u51cf\u5c0ftoken": 100, "\u6709\u4ec0\u4e48\u7528": 16, "\u6709\u5f88\u591a\u9ad8\u9891\u7ef4\u5ea6\u8f6c\u4e86\u5f88\u591a\u5708": 102, "\u672a\u63d0\u53ca\u7684\u4f4d\u4f7f\u7528\u5bf9\u5e94\u7684unicode\u8865\u5145": 110, "\u6765\u505a\u4e00\u4e2a\u5c0f\u5b9e\u9a8c\u5e76\u4e14\u9a8c\u8bc1\u4e0b": 110, "\u6765\u8bf4": 110, "\u67e5\u770b\u5176\u4ed6token": 100, "\u6807\u8bb0\u4ee5\u6807\u8bc6\u5355\u8bcd\u8fb9\u754c\u80fd\u591f\u8ba9\u7b97\u6cd5\u77e5\u9053\u6bcf\u4e2a\u5355\u8bcd\u7684\u7ed3\u675f\u4f4d\u7f6e": 100, "\u6807\u8bb0\u7684\u96c6\u5408": 100, "\u6a21\u4eff\u663e\u8457\u6027": 46, "\u6b21": 100, "\u6b27\u6d32\u90e8\u5206\u8bed\u8a00\u5b57\u6bcd\u7684\u5b57\u7b26\u96c6": 110, "\u6b64\u65f6\u53e5\u5b50": 109, "\u6bd4\u5982utf": 110, "\u6bd4\u5982\u4e2d\u56fd1980\u5e74\u53d1\u884c\u4e86gb2312\u4ee5\u53ca\u540e\u9762\u51fa\u73b0\u5b83\u7684\u6269\u5c55\u7248\u672cgbk": 110, "\u6bd4\u5982\u5728ascii\u4e2d": 110, "\u6bd4\u5982\u5927\u5199\u5b57\u6bcda": 110, "\u6bd4\u5982\u6c49\u5b57": 110, "\u6bd4\u5982\u6c49\u5b57\u4e00\u5728unicode\u4e2d\u5bf9\u5e94\u7684\u5b57\u7b26\u96c6\u662f4e00": 110, "\u6bd4\u5982\u82f1\u6587\u5b57\u6bcda\u5bf9\u5e94\u7684unicode\u662fu": 110, "\u6bd4\u5982\u8bf4\u4e2d\u6587": 110, "\u6c49\u5b57": 110, "\u6c49\u5b57\u7684\u5b57\u7b26\u96c6": 110, "\u6ca1\u6709": 70, "\u6d4f\u89c8\u5668\u6253\u5f00\u540e\u663e\u793a\u6b63\u5e38": 110, "\u7136\u540e\u5bf9\u5176\u8fdb\u884c\u7f16\u53f7": 100, "\u7136\u540e\u6211\u4eec\u4f7f\u7528chrome\u7684charset\u63d2\u4ef6\u6765\u4fee\u6539\u5f53\u524d\u9875\u9762\u89e3\u7801\u7684\u5b57\u7b26\u96c6": 110, "\u7136\u800c": 100, "\u73b0\u5728\u6211\u4eec\u53d1\u73b0": 100, "\u73b0\u5728\u6211\u4eec\u5c06\u6700\u5e38\u89c1\u7684\u5b57\u8282\u5bf9\u5408\u5e76\u6210\u4e00\u4e2atoken": 100, "\u73b0\u5728\u8ba9\u6211\u4eec\u770b\u770b\u5982\u4f55\u89e3\u7801\u6211\u4eec\u7684\u793a\u4f8b": 100, "\u751f\u6210\u5f85\u8bc4\u4f30\u7684\u6587\u4ef6": 19, "\u7528\u6700\u5c11\u7684token\u6765\u8868\u793a\u8bed\u6599\u5e93": 100, "\u7528\u6765\u8868\u793a\u7535\u8111\u548c\u5176\u4ed6\u7535\u4fe1\u8bbe\u5907\u7684\u6587\u672c": 110, "\u7531": 109, "\u7531m\u4e2a\u5b50\u8bcd\u7ec4\u6210": 109, "\u7531\u4e8eascii\u8868\u793a\u7684\u8303\u56f4\u6709\u9650": 110, "\u7531\u4e8e\u6211\u4eec\u603b\u5171\u6709": 100, "\u7684": [16, 100], "\u7684\u4f18\u52bf": 70, "\u7684\u5b57\u7b26\u91cf\u5df2\u7ecf\u8db3\u4ee5\u7528\u4e8e\u5168\u7403\u4e3b\u8981\u8bed\u8a00\u7684\u5927\u591a\u6570\u5b57\u7b26": 110, "\u7684\u5b57\u8282\u5bf9\u662f": 100, "\u7684\u60c5\u51b5": 70, "\u7684\u6548\u679c": 70, "\u7684\u6570\u636e": 100, "\u7684\u65b0token": 100, "\u7684\u6838\u5fc3\u673a\u5236": 16, "\u7684\u7b2c\u4e00\u5b57\u8282\u5168\u662f0": 110, "\u7684\u8bed\u8a00\u6a21\u578b\u4f3c\u7136\u503c\u7b49\u4ef7\u4e8e\u6240\u6709\u5b50\u8bcd\u6982\u7387\u7684\u4e58\u79ef": 109, "\u7684\u9891\u7387\u4e3a": 100, "\u7684\u9891\u7387\u51cf\u5c11": 100, "\u76ee\u5f55\u4e0b": 19, "\u76f4\u5230\u8fbe\u5230\u6211\u4eec\u9884\u5148\u8bbe\u7f6e\u7684token\u6570\u9650\u5236\u6216\u8fed\u4ee3\u9650\u5236": 100, "\u76f8\u6bd4": 70, "\u76f8\u90bb\u5b57\u8282\u5bf9": 100, "\u76f8\u90bb\u6570\u636e\u5355\u4f4d\u5728bpe\u4e2d\u770b\u4f5c\u76f8\u90bb\u5b57\u8282\u5bf9": 100, "\u7701\u8d44\u6e90": 70, "\u770b\u5230\u8fd9\u91cc": 109, "\u770b\u5b8c\u4e0b\u9762\u5b8c\u6574\u7684\u8fed\u4ee3\u8fc7\u7a0b": 100, "\u770b\u770b\u6211\u4eec\u6700\u7ec8\u7684token\u5217\u8868": 100, "\u771f\u5b9e\u7684": 72, "\u77e5\u9053\u4e86\u7f16\u7801\u7684\u539f\u7406\u540e\u81ea\u7136\u80fd\u60f3\u5230\u7684\u5c31\u662f\u6587\u672c\u7f16\u7801\u548c\u89e3\u7801\u6240\u4f7f\u7528\u7684\u5b57\u7b26\u96c6\u4e0d\u540c": 110, "\u786e\u4fdd\u6700\u5e38\u89c1\u7684\u8bcd\u5728token\u5217\u8868\u4e2d\u8868\u793a\u4e3a\u5355\u4e2atoken": 100, "\u7a0d\u540e\u6211\u4eec\u5c06\u770b\u5230": 100, "\u7a76\u7adf\u8c03\u7528\u7684\u662f\u4ec0\u4e48\u51fd\u6570": 16, "\u7a7a\u95f4\u6781\u5927\u6d6a\u8d39": 110, "\u7b2cn": 110, "\u7b2c\u4e00\u4e2a\u5b57\u8282": 110, "\u7b2c\u4e00\u4e2a\u5b57\u8282\u7684\u7b2c\u4e8c\u4e2a0": 110, "\u7b2c\u4e8c\u9ad8\u9891\u7387\u6807\u8bb0\u662f": 100, "\u7b49\u8bcd\u4e4b\u95f4\u7684\u533a\u522b": 100, "\u7b97\u6cd5\u7684\u4e0b\u4e00\u6b65\u662f\u5bfb\u627e\u6700\u9891\u7e41\u7684\u5b57\u7b26\u5bf9": 100, "\u7b97\u6cd5\u7684\u4e3b\u8981\u76ee\u6807": 100, "\u7c7b\u4f3c\u5730": 16, "\u7f16\u7801\u4e3a": 100, "\u7f16\u7801\u672c\u8eab\u8ba1\u7b97\u590d\u6742\u5ea6\u6bd4\u8f83\u9ad8": 100, "\u7f16\u7801\u7c7b\u578b": 110, "\u8001\u89c4\u77e9": 100, "\u800cwordpiece\u9009\u62e9\u80fd\u591f\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u6982\u7387\u6700\u5927\u7684\u76f8\u90bb\u5b50\u8bcd\u52a0\u5165\u8bcd\u8868": 109, "\u800c\u4e0d\u662f": 100, "\u800c\u4e14\u8fc7\u5927\u7684token\u5217\u8868\u5341\u5206\u5f71\u54cd\u6a21\u578b\u7684\u9884\u6d4b\u51c6\u786e\u5ea6": 100, "\u800c\u5728gb2312\u4e2d\u5bf9\u5e94\u7684\u5b57\u7b26\u96c6\u662fd2bb": 110, "\u800c\u662f\u5c06\u5b83\u4ee5utf": 110, "\u800c\u6c49\u5b57\u4e00\u4e8c\u4e09\u4e71\u7801\u4e86": 110, "\u800c\u7f55\u89c1\u7684\u8bcd\u88ab\u5206\u89e3\u4e3a\u4e24\u4e2a\u6216\u591a\u4e2asubword": 100, "\u80fd\u591f\u6e05\u695a\u5730\u7406\u89e3wordpiece\u5728\u5408\u5e76\u8fd9\u4e00\u6b65\u662f\u5982\u4f55\u4f5c\u51fa\u9009\u62e9\u7684": 109, "\u80fd\u591f\u7cbe\u7ec6\u5730\u533a\u5206\u76f8\u90bb\u4f4d\u7f6e": 102, "\u82e5\u4f7f\u7528\u8fd9\u79cd\u7f16\u7801\u65b9\u5f0f": 100, "\u82f1\u6587\u5b57\u6bcd": 110, "\u83b7\u53d6\u6a21\u578b\u7684": 16, "\u8461\u8404\u7259\u8bed\u7b49\u65e0\u6cd5\u7528127\u4e2a\u5b57\u7b26\u8868\u793a\u5b8c\u5168": 110, "\u8868\u793a\u5b50\u8bcd": 109, "\u8981\u89e3\u7801": 100, "\u89c4\u52191": 110, "\u89c4\u52192": 110, "\u89e3\u6790\u547d\u4ee4\u884c\u53c2\u6570": 16, "\u8ba1\u7b97": 102, "\u8ba1\u7b97\u673a\u5bf9\u6570\u636e\u7684\u8bfb\u53d6\u662f\u6309\u7167\u4e00\u4e2a\u5b57\u8282\u7684\u5927\u5c0f\u6765\u8bfb\u53d6\u8bc6\u522b\u7684": 110, "\u8ba1\u7b97\u673a\u6309\u7167\u5b57\u8282\u6765\u8bfb\u53d6\u6570\u636e\u65f6": 110, "\u8ba1\u7b97\u673a\u662f\u5982\u4f55\u77e5\u9053\u5f53\u524d\u5b57\u8282\u8868\u793a\u7684\u662f\u4ec0\u4e48\u610f\u601d\u5462": 110, "\u8ba9\u6211\u4eec\u5728\u6bcf\u4e2a\u5355\u8bcd\u7684\u672b\u5c3e\u6dfb\u52a0\u4e00\u4e2a\u7279\u6b8a\u7684\u7ed3\u675f\u6807\u8bb0": 100, "\u8ba9\u6211\u4eec\u73b0\u5728\u505c\u6b62\u8fed\u4ee3\u5e76": 100, "\u8ba9\u6211\u4eec\u73b0\u5728\u8003\u8651": 100, "\u8ba9\u6211\u4eec\u7528": 100, "\u8ba9\u6211\u4eec\u901a\u8fc7\u4e00\u4e2a\u5b9e\u9645\u7684\u4f8b\u5b50\u6765\u4e86\u89e3\u4e00\u4e0b\u5b83\u7684nlp\u7248\u672c": 100, "\u8bcd": 100, "\u8bf4\u660e": 110, "\u8bf4\u660eunicode\u548cgbk\u90fd\u5411\u4e0b\u517c\u5bb9\u4e86ascii": 110, "\u8c03\u7528": 16, "\u8d8a\u8fd1": 102, "\u8d8a\u8fdc": 102, "\u8f6c\u6362\u4e3a\u4e8c\u8fdb\u5236\u662f0100": 110, "\u8f6c\u6362\u4e3a\u4e8c\u8fdb\u5236\u662f100": 110, "\u8f6c\u6362\u4e3a\u5341\u8fdb\u5236\u662f19968": 110, "\u8f6c\u6362\u4e3a\u5341\u8fdb\u5236\u662f65": 110, "\u8f93\u51fa\u6587\u4ef6": 16, "\u8fd8\u6709\u5176\u4ed6\u7f16\u7801\u65b9\u5f0f": 110, "\u8fd8\u6709\u7a7a\u683c32": 110, "\u8fd9\u4e00\u95ee\u9898": 110, "\u8fd9\u4e24\u4e2a\u8bcd\u90fd\u6709\u4e00\u4e2a\u5171\u540c\u7684": 100, "\u8fd9\u4e2a\u4e8c\u8fdb\u5236\u670915\u4f4d": 110, "\u8fd9\u4e2a\u8bcd\u7684token": 100, "\u8fd9\u4e5f\u662f": 100, "\u8fd9\u53ea\u662f\u82f1\u8bed\u7684\u7528\u6cd5": 100, "\u8fd9\u5b8c\u5168\u7b49\u540c\u4e8e127\u4f4d\u6700\u521d\u7684ascii\u7801": 110, "\u8fd9\u610f\u5473\u7740\u6211\u4eec\u7684\u9891\u7387\u8ba1\u6570\u5c06\u5728\u6bcf\u4e2a\u5408\u5e76\u6b65\u9aa4\u540e\u53d1\u751f\u53d8\u5316": 100, "\u8fd9\u662f\u66f4\u65b0\u540e\u7684\u8868\u683c": 100, "\u8fd9\u6709\u52a9\u4e8e\u7b97\u6cd5\u67e5\u770b\u6bcf\u4e2a\u5b57\u7b26\u5e76\u627e\u5230\u9891\u7387\u6700\u9ad8\u7684\u5b57\u7b26\u914d\u5bf9": 100, "\u8fd9\u6709\u52a9\u4e8e\u7b97\u6cd5\u7406\u89e3": 100, "\u8fd9\u6837\u7684token\u5c06\u88ab\u4e0d\u540c\u5730\u5904\u7406": 100, "\u8fd9\u79cd\u73b0\u8c61\u79f0\u4e4b\u4e3a": 102, "\u8fd9\u79cd\u7f16\u7801\u65b9\u5f0f\u5341\u5206\u7b26\u5408\u4eba\u7c7b\u8bed\u8a00\u4e60\u60ef": 100, "\u8fd9\u91cc": [16, 109], "\u8fd9\u91cc\u4fee\u6539\u4e3agbk": 110, "\u8fd9\u91cc\u7b80\u5355\u4ecb\u7ecd\u4e0b\u8fd9\u51e0\u79cd\u7f16\u7801\u65b9\u5f0f\u7684\u533a\u522b": 110, "\u8fdc\u7a0b\u8870\u51cf\u66f2\u7ebf\u5982\u4e0b": 102, "\u8fed\u4ee3": 100, "\u901a\u5e38\u6709\u51e0\u4e07\u5230\u51e0\u5341\u4e07\u91cf\u7ea7\u7684\u5355\u8bcd\u6570": 100, "\u901a\u8fc7\u5f62\u5f0f\u5316\u65b9\u6cd5": 109, "\u90a3\u4e48\u4eba\u4eec\u80fd\u8bc6\u522b\u7684\u6587\u5b57\u548c\u8ba1\u7b97\u673a\u4e2d\u7684\u4e8c\u8fdb\u5236\u5fc5\u7136\u5b58\u5728\u4e00\u79cd\u6620\u5c04\u5173\u7cfb": 110, "\u90a3\u4e48\u9762\u5bf9\u5168\u4e16\u754c\u8fd9\u4e48\u591a\u8bed\u8a00": 110, "\u90a3\u5982\u4f55\u628a\u538b\u7f29\u7684\u7f16\u7801\u590d\u539f\u5462": 100, "\u90a3\u5c31\u662f\u672c\u6587\u5373\u5c06\u4ecb\u7ecd\u7684byte": 100, "\u90a3\u6837\u7684\u8ba1\u7b97\u91cf\u662f\u975e\u5e38\u6050\u6016\u7684": 100, "\u90a3\u82f1\u6587\u5b57\u7b26": 110, "\u90e8\u5206\u9891\u7387\u4f4e": 102, "\u90e8\u5206\u9891\u7387\u9ad8": 102, "\u90fd\u4e00\u6837": 110, "\u91cc\u548c\u653e\u5728": 70, "\u91cc\u7684\u533a\u522b": 70, "\u95f4\u76f8\u4e92\u9694\u5f00": 70, "\u963f\u62c9\u4f2f\u6570\u5b570\u5bf9\u5e94\u7684ascii\u7801\u662f48": 110, "\u963f\u62c9\u4f2f\u6570\u5b57\u548c\u82f1\u6587\u5b57\u6bcd": 110, "\u968f\u673a\u6027\u5f88\u5927": 102, "\u968f\u7740\u8ba1\u7b97\u673a\u5728\u5168\u7403\u7684\u53d1\u5c55\u548c\u666e\u53ca": 110, "\u9700": 16, "\u9700\u52a0\u4e0a": 16, "\u9700\u8981\u4ece": 19, "\u9700\u8981\u81f3\u5c112\u4e2a\u5b57\u8282\u8868\u793a": 110, "\u975e\u5e38\u91cd\u8981": 100, "\u9996\u5148\u6765\u660e\u786e\u4e00\u4e0b\u57fa\u7840\u6982\u5ff5": 100, "\u9996\u5148\u770b\u4e0bwiki\u5bf9\u5b83\u7684\u5b9a\u4e49": 110, "\u9ad8\u4f4d\u4ee5": 110, "\u9ad8\u4f4d\u524dn\u4f4d\u4e3a": 110, "\u9ad8\u7684": 70, "\u9ad8\u7ef4": 102, "\u9ad8\u9891\u4fe1\u606f\u7684\u635f\u5931": 102, "\u9ad8\u9891\u7ef4\u5ea6\u5c11\u4f4e\u9891\u7ef4\u5ea6\u591a": 102, "\ud835\udc41": 69}, "titles": ["Agent", "API-Bank", "CodeAct", "&lt;no title&gt;", "Base", "Attention Is All You Need", "GPT", "GPT2", "GPT3", "InstructGPT", "Benchmarks", "Aider Polyglot", "Alignment Benchmarks", "BigCodeBench", "Code Alpaca", "CRUXEval", "EvalPlus", "General Benchmarks", "HumanEval", "LiveCodeBench", "Magicoder", "Math &amp; Science Benchmarks", "MBPP", "RewardBench", "SELF-INSTRUCT", "SWE-bench", "TACO", "WizardCoder", "Contents", "Contents", "Data", "AlphaCode", "APPS", "Code Alpaca", "Magicoder", "OpenCoder", "SELF-INSTRUCT", "TACO", "UNICODER", "WizardCoder", "Introduction", "Markdown Files", "Notebooks with MyST Markdown", "GOLD", "Evaluation of LLMs Should Not Ignore Non-Determinism", "Scaling Laws for Neural Language Models", "Weak to Strong Generalization", "Models", "AlphaCode", "AlphaCode 2", "AlphaCodium", "Code Llama", "DeepSeek-Coder-V2", "DeepSeek-V2", "DeepSeek V3", "Llama", "Llama 2", "Llama 3", "Llama3", "Llama 3 Source Code", "Qwen 2.5", "Qwen2.5-Coder", "Qwen3", "Preference Optimization", "ArmoRM-MoE", "DAPO", "DeepSeek-GRM", "DPO", "DPOP", "Efficient Exploration for LLMs", "Group Relative Policy Optimization", "Instruct GPT", "Aligning Language Models with Judgments", "OpenRLHF", "PPO", "REINFORCE++", "Constitutional AI: Harmlessness from AI Feedback", "RLAIF vs. RLHF", "RLCD", "RS-DPO", "RSO", "Secrets of RLHF in Large Language Models: Reward Modeling", "Self-Rewarding Language Models", "Self-Taught Evaluators", "West-of-N", "Reasoning", "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models", "DeepCoder", "DeepSeek-R1", "Logic-RL", "s1: Simple test-time scaling", "Training Language Models to Self-Correct via Reinforcement Learning", "Let\u2019s Verify Step by Step", "References", "SFT", "LIMA: Less Is More for Alignment", "RETHINKING DATA SELECTION AT SCALE: RANDOM SELECTION IS ALMOST ALL YOU NEED", "RL on Incorrect Synthetic Data Scales the Efficiency of LLM Math Reasoning by Eight-Fold", "Scaling Relationship on Learning Mathematical Reasoning with Large Language Models", "Techniques", "Byte Pair Encoding (BPE)", "DeepSeekMoE", "Extending context window of LLMs", "Extending context window of large language models via position interpolation", "Multi-Head Latent Attention", "Normalization", "RoPE", "Rotary Positional Embeddings (RoPE)", "SwiGLU", "WordPiece, ULM and SentencePiece", "ASCII,UNICODE,UTF8", "Llama Factory", "LORA", "OpenRLHF"], "titleterms": {"": 92, "1": 64, "16": 110, "2": [49, 51, 56, 60, 64], "2d": [106, 107], "3": [57, 59], "32\u7b49": 110, "5": [60, 61], "500": 21, "8": 110, "A": 61, "AT": 96, "Not": 44, "The": [43, 50, 64, 75, 98], "abil": [82, 98], "ablat": [95, 104], "absolut": 107, "accuraci": 98, "activ": [58, 59, 69, 92], "adapt": 81, "add": 42, "addit": 50, "advantag": 75, "agent": [0, 2], "aggreg": 64, "aha": 88, "ai": [76, 77], "aider": 11, "algorithm": [43, 69, 75, 80, 82, 88], "align": [12, 44, 52, 53, 72, 82, 95], "all": [5, 88, 96], "almost": 96, "alpaca": [14, 33], "alphacod": [31, 48, 49], "alphacodium": 50, "an": [42, 69], "analyz": 81, "annot": 83, "api": 1, "app": 32, "appendix": 67, "approach": [7, 8, 25, 48, 55, 66, 80, 107], "architectur": [5, 8, 53, 54, 55, 60, 69], "arena": 12, "armorm": 64, "ascii": 110, "assess": 69, "attent": [5, 58, 59, 104], "augment": 98, "automat": 13, "averag": 57, "awar": 102, "background": [75, 102, 103, 107], "balanc": 101, "bank": 1, "base": [4, 25, 60, 61, 65, 66, 88, 92], "basic": [45, 54], "batch": [75, 105], "batchnorm": 58, "bench": 25, "benchmark": [10, 12, 13, 17, 21, 25, 66], "benefit": 2, "better": [2, 81], "between": 104, "bigcodebench": 13, "boost": 66, "bpe": 100, "byte": 100, "cach": 104, "capabl": 57, "case": [13, 106, 107], "cell": 42, "chain": 86, "chart": 45, "chat": 57, "chatformat": 58, "citat": 41, "classif": [24, 36], "clip": [65, 75], "cluster": [48, 49], "code": [13, 14, 16, 27, 33, 39, 50, 51, 57, 59], "codeact": 2, "codecontest": 31, "codellama": 25, "coder": [52, 61], "cold": [66, 88], "collaps": 91, "collect": [52, 56, 92], "commun": 101, "comparison": [66, 104], "composit": [35, 61], "compress": 104, "comput": 45, "concept": 50, "consider": 101, "constitut": 76, "construct": [13, 25, 53, 54, 83], "content": [28, 29], "context": [51, 54, 102, 103], "contextwindow": 102, "contrast": 72, "control": 57, "correct": [16, 18, 91], "count": [45, 98], "creat": [42, 90], "creation": 82, "critiqu": [66, 76], "cruxev": 15, "curat": [13, 90], "dapo": 65, "data": [13, 14, 24, 30, 33, 35, 36, 45, 52, 53, 54, 55, 56, 57, 61, 81, 90, 92, 95, 96, 97, 98], "dataset": [7, 8, 9, 31, 45, 48, 51, 65, 71], "decod": [5, 6], "decontamin": [35, 61], "decoupl": 104, "deepcod": 87, "deepseek": [52, 53, 54, 66, 88], "deepseekmo": 101, "deriv": 67, "design": [1, 50], "detail": [9, 20, 34, 35, 65, 73], "determin": 44, "devic": 101, "dialog": 57, "differ": [66, 81], "direct": [41, 57, 67, 103], "discuss": 53, "distanc": 102, "diverg": 65, "divers": [81, 95], "done": 2, "dot": 5, "dpo": [67, 68, 79], "dpop": 68, "drop": 101, "dynam": [65, 102], "effect": 44, "effici": [69, 97, 102], "eight": 97, "elicit": 86, "embed": [5, 45, 102, 103, 104, 107], "empir": [45, 69], "encod": [5, 100], "enhanc": 75, "enn": 69, "epistem": 69, "estim": 69, "evalplu": 16, "evalu": [1, 8, 13, 16, 19, 20, 27, 34, 39, 44, 48, 49, 53, 54, 60, 61, 77, 83, 95], "evol": [27, 39], "evolut": 88, "exampl": 42, "exist": 2, "experi": [7, 65, 82, 103], "experiment": [9, 25, 44, 52, 69, 75, 82], "expert": [64, 101], "explor": 69, "extend": [102, 103], "extens": [54, 102], "extrapol": 103, "factor": [44, 98], "factori": 111, "failur": 68, "featur": 25, "feed": 5, "feedback": [76, 77], "feedforward": [58, 59], "fewer": 2, "ffn": 108, "file": 41, "filter": [24, 36, 48, 49], "fine": [6, 25, 31, 48, 49, 51, 52, 53, 54, 56, 60, 66, 71, 83, 88, 101], "finetun": [24, 36, 57], "flip": 81, "flow": 50, "fold": 97, "follow": [24, 36, 82], "form": [106, 107], "format": [25, 57], "formul": [106, 107], "forward": 5, "framework": [2, 6, 43], "frequenc": 102, "from": [2, 20, 34, 43, 66, 72, 76, 77], "full": 44, "fullest": 81, "function": [18, 58, 59], "gate": 108, "gb2312": 110, "gbk\u7b49\u5176\u4ed6\u7f16\u7801": 110, "gener": [13, 17, 24, 36, 44, 46, 58, 66, 92, 106, 107], "get": 2, "glu": 108, "gold": 43, "gpqa": 21, "gpt": [6, 71], "gpt2": 7, "gpt3": 8, "gqa": 104, "gradient": [43, 65, 67], "grain": 101, "grm": 66, "group": 70, "grpo": 70, "gsm8k": 21, "hard": 12, "harmless": 76, "head": [5, 104], "high": [9, 102], "higher": 65, "how": [44, 81], "human": [13, 56, 81, 95], "humanev": [16, 18], "hyper": 53, "i": [2, 5, 41, 44, 91, 95, 96], "identif": [24, 36], "ifev": 12, "ignor": 44, "ii": 91, "impact": 81, "implement": [20, 34, 73, 103], "incorpor": 72, "incorrect": 97, "infer": 66, "infil": 51, "infinit": 45, "influenc": 44, "inform": 102, "initi": [82, 83, 91], "input": [6, 7, 25], "insight": 50, "instal": 19, "instanc": [24, 36], "instruct": [13, 20, 24, 27, 34, 35, 36, 38, 39, 51, 60, 61, 71, 82, 83], "instructgpt": 9, "integr": 75, "interact": 2, "interpol": [102, 103], "interpret": 64, "introduct": [20, 34, 40, 46, 51, 55, 72, 78, 103], "isol": 101, "iter": [56, 57, 70, 83], "its": 81, "joint": 104, "judgment": [72, 83], "kei": 104, "kl": [65, 75], "kv": 104, "label": [77, 81], "languag": [45, 57, 72, 81, 82, 86, 91, 98, 102, 103, 109], "larg": [48, 81, 86, 92, 98, 102, 103], "latent": 104, "law": 45, "layer": 105, "layernorm": 58, "learn": [41, 52, 53, 54, 60, 69, 71, 72, 76, 77, 88, 91, 92, 98], "less": 95, "let": 92, "level": [9, 65, 75, 101], "leverag": 81, "lima": 95, "limit": 45, "linear": 108, "lite": 25, "livecodebench": 19, "llama": [25, 51, 55, 56, 57, 59, 103, 111], "llama3": 58, "llm": [2, 44, 69, 77, 97, 98, 102], "lm": [24, 36], "load": 101, "local": 102, "logic": 89, "long": [51, 54], "lora": 112, "loss": [65, 67, 98, 101, 102], "low": 104, "magicod": [20, 34], "main": [65, 76], "make": 2, "margin": 81, "markdown": [41, 42], "math": [21, 97, 98], "mathemat": 98, "mbpp": [16, 22], "measur": 81, "mechan": 104, "metadata": 42, "method": [9, 76, 83, 92], "methodologi": [9, 46, 71, 77], "mha": 104, "mini": 75, "mixtur": [61, 64, 101], "mla": 104, "mle": 43, "mmlu": 17, "mode": 68, "model": [5, 7, 8, 9, 45, 47, 49, 56, 57, 58, 59, 60, 61, 64, 65, 66, 69, 71, 72, 81, 82, 83, 84, 86, 88, 91, 92, 98, 102, 103, 109], "moe": 64, "moment": 88, "more": [2, 41, 95], "mqa": 104, "multi": [2, 5, 54, 64, 91, 104], "myst": [41, 42], "n": [45, 84], "need": [5, 64, 96], "network": [5, 69], "neural": [45, 69], "nl": 13, "nlp\u5b9e\u4f8b": 100, "non": [44, 45], "normal": [58, 75, 105], "notebook": 42, "ntk": 102, "object": 64, "off": 43, "offlin": 60, "onli": 6, "onlin": 60, "ood": 92, "open": [20, 34], "opencod": 35, "openrlhf": [73, 113], "optim": [55, 57, 63, 67, 70], "orient": [13, 50, 88], "orm": 92, "oss": [20, 34], "outcom": [70, 92], "overal": [49, 82], "overfit": 45, "overlong": 65, "overview": 50, "packag": 2, "pair": [83, 100], "paramet": [45, 53], "part": 102, "passiv": 69, "pattern": 44, "penalti": 75, "perform": [45, 81, 88], "pi_": 67, "pipelin": 69, "point": 69, "polici": [43, 49, 61, 65, 70], "polyglot": 11, "posit": [5, 102, 103, 104, 107], "post": [35, 54, 57, 60, 61], "postprocess": [24, 36], "potenti": [44, 81], "power": 45, "ppo": [70, 73, 74, 75], "pre": [6, 53, 54, 55, 57, 60, 61, 98], "predict": 54, "prefer": [56, 57, 63, 67, 77, 81, 84], "preliminari": [65, 67, 80, 81, 91, 101, 104, 107], "pretrain": [35, 56], "prevent": 91, "principl": [1, 66], "prm": 92, "pro": 17, "problem": [72, 91], "process": [57, 70, 88, 92], "product": 5, "program": 13, "promis": 2, "prompt": [27, 39, 77, 86], "properti": 107, "propos": [50, 107], "qualiti": [57, 66, 95], "quantiti": 95, "queri": 104, "quick": 16, "quickli": 42, "qwen": 60, "qwen2": 61, "qwen3": 62, "r": [67, 79], "r1": 88, "random": 96, "rank": 104, "reason": [85, 86, 88, 90, 97, 98], "recip": 61, "redux": 17, "refactor": 13, "refer": 93, "reinforc": [52, 53, 54, 60, 71, 75, 76, 77, 88, 91], "reject": [66, 80, 88], "rel": [70, 102], "relat": 84, "relationship": 98, "remov": 65, "represent": 7, "respons": 83, "result": [9, 25, 44, 45, 52, 53, 54, 57, 65, 66, 69, 76, 77, 82, 90], "rethink": 96, "retriev": 25, "review": 70, "revis": 76, "reward": [43, 56, 57, 64, 65, 66, 69, 71, 75, 81, 82, 88, 91, 92], "rewardbench": 23, "rl": [43, 66, 70, 71, 89, 91, 97], "rlaif": 77, "rlcd": 78, "rlhf": [56, 77, 81], "rm": [66, 71, 81], "rmsnorm": [58, 59, 105], "role": 41, "rope": [58, 59, 102, 103, 106, 107], "rotari": [102, 103, 104, 107], "round": 57, "rso": 80, "rule": [65, 66], "s1": 90, "s1k": 90, "sampl": [41, 48, 49, 65, 80, 88], "scale": [5, 44, 45, 48, 66, 90, 92, 96, 97, 98, 102], "scenario": 88, "scienc": 21, "score": [49, 91], "secret": 81, "segment": 101, "select": [83, 96], "self": [24, 36, 66, 82, 83, 84, 88, 91], "semi": 13, "sentencepiec": 109, "set": 72, "setup": [25, 75, 82, 91], "sft": [56, 57, 71, 94, 111, 113], "shape": [65, 91], "share": 101, "should": 44, "show": 2, "simpl": 90, "size": 45, "small": 92, "smooth": 81, "softmax": 5, "softwar": 2, "sourc": [20, 34, 59], "spct": 66, "special": 51, "specif": 6, "stack": 5, "stage": [35, 50, 64, 91], "standard": 104, "stanford": [14, 33], "start": [16, 66, 88], "statist": [13, 80], "step": 92, "strategi": 101, "strength": 81, "strong": [2, 46], "summar": 7, "supervis": [6, 52, 53, 54, 57, 60, 70, 71, 76, 88, 92, 98], "surfac": 44, "swe": 25, "swiglu": [58, 59, 108], "swish": 108, "synthesi": 13, "synthet": [92, 97], "system": [1, 49], "taco": [26, 37], "takeawai": [52, 53, 54, 57, 60, 61, 81, 92, 106], "task": [6, 24, 36], "taught": 83, "techniqu": [77, 99], "temperatur": 44, "templat": 88, "test": [13, 90], "thought": 86, "tiktoken": 58, "time": [45, 66, 90], "token": [54, 58, 60, 65, 75, 101], "tool": 2, "train": [6, 7, 8, 27, 35, 39, 45, 53, 54, 55, 57, 60, 61, 65, 69, 82, 83, 84, 88, 91, 95, 98], "transform": [6, 45, 58, 59, 101], "tune": [6, 20, 25, 31, 34, 35, 48, 49, 51, 52, 53, 54, 56, 60, 66, 71, 83, 88], "turn": [2, 91], "two": 35, "ulm": 109, "understand": 66, "unicod": [38, 110], "unigram": 109, "unit": 108, "unpin": 66, "unsupervis": 6, "updat": 75, "us": 2, "utf": 110, "utf8": 110, "v": [77, 92, 98], "v2": [52, 53], "v3": 54, "valu": 104, "variant": 108, "variou": 44, "verifi": 92, "via": [91, 103], "weak": 46, "west": 84, "what": [2, 41, 44], "why": [6, 104, 105], "window": [102, 103], "wise": 5, "wizardcod": [27, 39], "wizardlm": [27, 39], "wordpiec": 109, "work": 84, "yaml": 42, "yarn": 102, "you": [5, 96], "zero": 88, "\u4e3a\u4ec0\u4e48\u4f1a\u4e71\u7801": 110, "\u521d\u8bc6bpe": 100, "\u603b\u7ed3": 110, "\u662f\u4e0d\u662f\u8d2a\u5fc3\u7b97\u6cd5": 100, "\u672c\u5730": [16, 19], "\u7684\u8fdc\u7a0b\u8870\u51cf": 102, "\u7f16\u7801\u548c\u89e3\u7801": 100, "\u9ad8\u9891\u5916\u63a8\u4f4e\u9891\u5185\u63d2": 102}})