{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b33df04-9c9f-4f9f-bf47-4eef57f5ff34",
   "metadata": {},
   "source": [
    "# Rotary Positional Embeddings (RoPE)\n",
    "\n",
    "```{note}\n",
    "Position encoding enables valuable supervision for dependency modeling between elements at different positions of the sequence.\n",
    "Rotary Position Embedding(RoPE) is a novel method to effectively leverage the positional information.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3e7343-3971-406a-b896-07be2209ae6f",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "### Preliminary\n",
    "\n",
    "Let $\\{w_{i}\\}_{i=1}^{N}$ be a sequence of $N$ input tokens, the corresponding word embedding is denoted as $\\{\\mathbf{x}_{i}\\}_{i=1}^{N}$, where $\\mathbf{x}_{i}\\in\\mathbb{R}^{d}$ is the $d$-dimensional word embedding vector of token $w_{i}$ without position information. The self-attention first incorporates position information to the word embeddings and transforms them into queries, keys and value representation.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{q}_{m} &= f_{q}(\\mathbf{x}_{m}, m)\\\\\n",
    "\\mathbf{k}_{n} &= f_{k}(\\mathbf{x}_{n}, n)\\\\\n",
    "\\mathbf{v}_{n} &= f_{v}(\\mathbf{x}_{n}, n)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The query and key values are then used to compute the attention weights, while the output is computed as the weighted sum over the value representation.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "a_{m,n} &= \\frac{\\exp\\left(\\frac{\\mathbf{q}_{m}^{T}\\mathbf{k}_{n}}{\\sqrt{d}}\\right)}{\\sum_{j=1}^{N}\\exp\\left(\\frac{\\mathbf{q}_{m}^{T}\\mathbf{k}_{j}}{\\sqrt{d}}\\right)}\\\\\n",
    "\\mathbf{o}_{m} &= \\sum_{n=1}^{N}a_{m,n}\\mathbf{v}_{n}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb361c8b-2927-4406-bbfe-d6bf64a23d5b",
   "metadata": {},
   "source": [
    "### Absolute position embedding\n",
    "\n",
    "A typical choice of $f_{t:t\\in\\{q,k,v\\}}$ is\n",
    "\n",
    "$$f_{t:t\\in\\{q,k,v\\}}(\\mathbf{x}_{i}, i) := \\mathbf{W}_{t:t\\in\\{q,k,v\\}}(\\mathbf{x}_{i} + \\mathbf{p}_{i})$$\n",
    "\n",
    "where $\\mathbf{p}_{i}\\in\\mathbb{R}^{d}$ is a d-dimensional vector depending on $i$. The original transformer architecture proposed to generate $\\mathbf{p}_{i}$ using the sinusoidal function.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{p}_{i, 2t} &= \\sin(i/10000^{2t/d}) \\\\\n",
    "\\mathbf{p}_{i, 2t+1} &= \\cos(i/10000^{2t/d})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $i$ is the position and $2t,2t+1$ are dimensions.\n",
    "\n",
    "* low dimension: $t$ small $\\to$ $10000^{2t/d}$ small $\\to$ high frequency.\n",
    "* high dimension: $t$ large $\\to$ $10000^{2t/d}$ large $\\to$ low frequency.\n",
    "* 10000 is the base period, increase base period allows for processing much larger sequences.\n",
    "\n",
    "In the next section, we show that our proposed RoPE\n",
    "is related to this intuition from the sinusoidal function perspective. However, instead of directly adding the position\n",
    "to the context representation, RoPE proposes to incorporate the relative position information by multiplying with the\n",
    "sinusoidal functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68c351c-b0ea-4b02-9d3a-39cf1e712a77",
   "metadata": {},
   "source": [
    "## Proposed approach\n",
    "\n",
    "In this section, we discuss the proposed rotary position embedding (RoPE).\n",
    "\n",
    "### Formulation\n",
    "\n",
    "Transformer-based language modeling usually leverages the position information of individual tokens through a selfattention\n",
    "mechanism. $\\mathbf{q}_{m}^{T}\\mathbf{k}_{n}$ typically enables knowledge transfer between\n",
    "tokens at different positions. In order to incorporate relative position information, we require the inner product of query\n",
    "$\\mathbf{q}_{m}$ and key $\\mathbf{k}_{n}$ to be formulated by a function $g$, which takes only the word embeddings $\\mathbf{x}_{m}$, $\\mathbf{x}_{n}$, and their relative position $m-n$ as input variables.\n",
    "\n",
    "$$\\langle f_{q}(\\mathbf{x}_{m}, m),f_{k}(\\mathbf{x}_{n}, n) \\rangle = g(\\mathbf{x}_{m}, \\mathbf{x}_{n}, m-n)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87791ef-1c8c-4efa-a503-353885fb038d",
   "metadata": {},
   "source": [
    "### 2D case\n",
    "\n",
    "We begin with a simple case with a dimension $d=2$, we make use of the geometric property of vectors on a 2D plane and its complex form to prove that a solution to our formulation Equation is:\n",
    "\n",
    "$$\n",
    "\\mathbf{R}_{m,\\theta} = \\begin{pmatrix}\n",
    "  \\cos m\\theta & -\\sin m\\theta \\\\\n",
    "  \\sin m\\theta & \\cos m\\theta\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{R}_{m,\\theta}^{T} = \\mathbf{R}_{-m,\\theta}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{R}_{m,\\theta}\\mathbf{R}_{n,\\theta} = \\mathbf{R}_{m+n,\\theta}\n",
    "$$\n",
    "\n",
    "then:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "f_{q}(\\mathbf{x}_{m}, m) &= \\mathbf{R}_{m,\\theta}\\mathbf{W}_{q}\\mathbf{x}_{m} \\\\\n",
    "f_{k}(\\mathbf{x}_{n}, n) &= \\mathbf{R}_{n,\\theta}\\mathbf{W}_{k}\\mathbf{x}_{n} \\\\\n",
    "g(\\mathbf{x}_{m}, \\mathbf{x}_{n}, m-n) &= (\\mathbf{W}_{q}\\mathbf{x}_{m})^{T}\\mathbf{R}_{n-m,\\theta}(\\mathbf{W}_{k}\\mathbf{x}_{n})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $\\mbox{Re}[\\cdot]$ is the real part of a complex number and $(\\mathbf{W}_{k}\\mathbf{x}_{n})^{\\ast}$ represents the conjugate complex number of $(\\mathbf{W}_{k}\\mathbf{x}_{n})$, $\\theta\\in\\mathbb{R}$ is a non-zero constant. We can further write $f_{q}$ in a multiplication matrix:\n",
    "\n",
    "$$f_{q}(\\mathbf{x}_{m}, m) = \n",
    "\\begin{pmatrix}\n",
    "  \\cos m\\theta & -\\sin m\\theta \\\\\n",
    "  \\sin m\\theta & \\cos m\\theta\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "  W_{q}^{(11)} & W_{q}^{(12)} \\\\\n",
    "  W_{q}^{(21)} & W_{q}^{(22)}\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "  x_{m}^{(1)} \\\\\n",
    "  x_{m}^{(2)}\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99983d5-3396-433c-9b17-f171fb404946",
   "metadata": {},
   "source": [
    "## General form\n",
    "\n",
    "In order to generalize our results in 2D to any $\\mathbf{x}_{i}\\in\\mathbb{R}^{d}$ where $d$ is even, we divide the d-dimensional space into $d/2$ sub-spaces:\n",
    "\n",
    "$$\n",
    "R_{m,\\Theta}^{d} = \n",
    "\\begin{pmatrix}\n",
    "  \\cos m\\theta_{1} & -\\sin m\\theta_{1} & 0 & 0 & \\dots & 0 & 0 \\\\\n",
    "  \\sin m\\theta_{1} &  \\cos m\\theta_{1} & 0 & 0 & \\dots & 0 & 0\\\\\n",
    "  0 & 0 &  \\cos m\\theta_{2} & -\\sin m\\theta_{2}  & \\dots & 0 & 0\\\\\n",
    "  0 & 0 &  \\sin m\\theta_{2} &  \\cos m\\theta_{2}  & \\dots & 0 & 0\\\\\n",
    "  \\vdots&  \\vdots&  \\vdots&  \\vdots& \\ddots &\\vdots  &\\vdots \\\\\n",
    "  0&  0&  0&  0&  \\dots&  \\cos m\\theta_{d/2} & -\\sin m\\theta_{d/2} \\\\\n",
    "  0&  0&  0&  0&  \\dots&  \\sin m\\theta_{d/2} &  \\cos m\\theta_{d/2}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "is the rotary matrix with pre-defined parameters $\\Theta = \\{\\theta_{i}=10000^{-2(i-1)/d},i\\in[1,2,\\dots,d/2]\\}$. RoPE encodes the absolute position with a rotation matrix and meanwhile incorporates the explicit relative relative position dependency in self-attention formulation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ea4d3d-bad9-466f-b77f-a09bbf6dce02",
   "metadata": {},
   "source": [
    "## Properties of RoPE\n",
    "\n",
    "**Long-term decay**: we set $\\theta_{i}=10000^{-2i/d}$. One can prove that this setting provides a long-term decay property, which means the inner-product will decay when the relative position increase.\n",
    "\n",
    "**RoPE with linear attention**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13379cc2-b9c1-4f31-9120-1675f05a5724",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
